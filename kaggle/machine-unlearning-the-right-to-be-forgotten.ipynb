{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/tamlhp/machine-unlearning-the-right-to-be-forgotten?scriptVersionId=135861266\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"31121804","metadata":{"papermill":{"duration":0.007489,"end_time":"2023-07-06T00:16:39.506743","exception":false,"start_time":"2023-07-06T00:16:39.499254","status":"completed"},"tags":[]},"source":["# Machine Unlearning: The Right to be Forgotten\n","\n","## Abstract\n","\n","Today, computer systems hold large amounts of personal data. Yet while\n","such an abundance of data allows breakthroughs in artificial\n","intelligence, and especially machine learning, its existence can be a\n","threat to user privacy, and it can weaken the bonds of trust between\n","humans and AI. Recent regulations now require that, on request, private\n","information about a user must be removed from both computer systems and\n","from machine learning models -- this legislation is more colloquially\n","called \"the right to be forgotten\"). While removing data from back-end\n","databases should be straightforward, it is not sufficient in the AI\n","context as machine learning models often 'remember' the old data.\n","Contemporary adversarial attacks on trained models have proven that we\n","can learn whether an instance or an attribute belonged to the training\n","data. This phenomenon calls for a new paradigm, namely *machine\n","unlearning*, to make machine learning models forget about particular\n","data. It turns out that recent works on machine unlearning have not been\n","able to completely solve the problem due to the lack of common\n","frameworks and resources. Therefore, this paper aspires to present a\n","comprehensive examination of machine unlearning's concepts, scenarios,\n","methods, and applications. Specifically, as a category collection of\n","cutting-edge studies, the intention behind this article is to serve as a\n","comprehensive resource for researchers and practitioners seeking an\n","introduction to machine unlearning and its formulations, design\n","criteria, removal requests, algorithms, and applications. In addition,\n","we aim to highlight the key findings, current trends, and new research\n","areas that have not yet featured the use of machine unlearning but could\n","benefit greatly from it. We hope this survey serves as a valuable\n","resource for machine learning researchers and those seeking to innovate\n","privacy technologies. Our resources are publicly available at\n","<https://github.com/tamlhp/awesome-machine-unlearning>.\n","\n","<div class=\"figure*\">\n","<figure>\n","<img src=\"https://raw.githubusercontent.com/tamlhp/awesome-machine-unlearning/main/framework.png\" alt=\"image\" style=\"max-width: 100%;\"/>\n","<figcaption aria-hidden=\"true\">A Typical Machine Unlearning Process</figcaption>\n","</figure>\n","</div>"]},{"cell_type":"markdown","id":"8d4c35dc","metadata":{"papermill":{"duration":0.006525,"end_time":"2023-07-06T00:16:39.520343","exception":false,"start_time":"2023-07-06T00:16:39.513818","status":"completed"},"tags":[]},"source":["<h1 id=\"sec:intro\">1. Introduction</h1>\n","<p>Computer systems today hold large amounts of personal data. Due to\n","the great advancement in data storage and data transfer technologies,\n","the amount of data being produced, recorded, and processed has exploded.\n","For example, four billion YouTube videos are watched every day&#xA0;<a href=\"#ref-sari2020learning\">(Sari et al.\n","2020)</a>. These online personal data, including digital footprints\n","made by (or about) netizens, reflects their behaviors, interactions, and\n","communication patterns in real-world&#xA0;<a href=\"#ref-nguyen2019debunking\">(Thanh Tam Nguyen 2019)</a>. Other\n","sources of personal data include the digital content that online users\n","create to express their ideas and opinions, such as product reviews,\n","blog posts (e.g.&#xA0;Medium), status seeking (e.g.&#xA0;Instagram), and knowledge\n","sharing (e.g. Wikipedia)&#xA0;<a href=\"#ref-nguyen2021judo\">(Thanh Toan Nguyen et al. 2021)</a>. More\n","recently, personal data has also expanded to include data from wearable\n","devices&#xA0;<a href=\"#ref-ren2022prototype\">(Z. Ren,\n","Nguyen, and Nejdl 2022)</a>. On the one hand, such an abundance of\n","data has helped to advance artificial intelligence (AI). However, on the\n","other hand, it threatens the privacy of users and has led to many data\n","breaches&#xA0;<a href=\"#ref-cao2015towards\">(Y. Cao and\n","Yang 2015)</a>. For this reason, some users may choose to have their\n","data completely removed from a system, especially sensitive systems such\n","as those do with finance or healthcare&#xA0;<a href=\"#ref-ren2022prototype\">(Z. Ren, Nguyen, and Nejdl 2022)</a>.\n","Recent regulations now compel organisations to give users &#x201C;the right to\n","be forgotten&#x201D;, i.e., the right to have all or part of their data deleted\n","from a system on request&#xA0;<a href=\"#ref-dang2021right\">(Dang 2021)</a>.</p>\n","<p>While removing data from back-end databases satisfies the\n","regulations, doing so is not sufficient in the AI context as machine\n","learning models often &#x2018;remember&#x2019; the old data. Indeed, in machine\n","learning systems, often millions, if not billions, of users&#x2019; data have\n","been processed during the model&#x2019;s training phase. However, unlike humans\n","who learn general patterns, machine learning models behave more like a\n","lossy data compression mechanism&#xA0;<a href=\"#ref-schelter2020amnesia\">(Schelter 2020)</a>, and some are\n","overfit against their training data. The success of deep learning models\n","in particular has been recently been attributed to the compression of\n","training data&#xA0;<a href=\"#ref-tishby2000information tishby2015deep\">(Tishby et al. 2000;\n","Tishby and Zaslavsky 2015)</a>. This memorization behaviour can be\n","further proven by existing works on adversarial attacks&#xA0;<a href=\"#ref-ren2020generating chang2022example ren2020enhancing\">(Z.\n","Ren, Baird, et al. 2020; Chang et al. 2022; Z. Ren, Han, et al.\n","2020)</a>, which have shown that it is possible to extract the\n","private information within some target data from a trained model.\n","However, we also know that the parameters of a trained model do not tend\n","to show any clear connection to the data that was used for\n","training&#xA0;<a href=\"#ref-shwartz2017opening\">(Shwartz-Ziv and Tishby 2017)</a>. As\n","a result, it can be challenging to remove information corresponding to a\n","particular data item from a machine learning model. In other words, it\n","can be difficult to make a machine learning model forget a user&#x2019;s\n","data.</p>\n","<div class=\"table*\">\n","\n","</div>\n","<p>This challenge of allowing users the possibility and flexibility to\n","completely delete their data from a machine learning model calls for a\n","new paradigm, namely <em>machine unlearning</em>&#xA0;<a href=\"#ref-nguyen2022markov baumhauer2020machine tahiliani2021machine\">(Q.\n","P. Nguyen et al. 2022; Baumhauer, Sch&#xF6;ttle, and Zeppelzauer 2020;\n","Tahiliani et al. 2021)</a>. Ideally, a machine unlearning mechanism\n","would remove data from the model without needing to retrain it from\n","scratch&#xA0;<a href=\"#ref-nguyen2022markov\">(Q. P.\n","Nguyen et al. 2022)</a>. To this end, a users&#x2019; right to be forgotten\n","would be observed and the model owner would be shielded from constant\n","and expensive retraining exercises.</p>\n","<p>Researchers have already begun to study aspects of machine\n","unlearning, such as removing part of the training data and analysing the\n","subsequent model predictions&#xA0;<a href=\"#ref-nguyen2022markov thudi2021necessity\">(Q. P. Nguyen et al.\n","2022; Thudi, Jia, et al. 2022)</a>. However, it turns out that this\n","problem cannot be completely solved due to a lack of common frameworks\n","and resources&#xA0;<a href=\"#ref-villaronga2018humans veale2018algorithms shintre2019making schelter2020amnesia\">(Villaronga,\n","Kieseberg, and Li 2018; Veale, Binns, and Edwards 2018; Shintre et al.\n","2019; Schelter 2020)</a>. Hence, to begin building a foundation of\n","works in this nascent area, we undertook a comprehensive survey of\n","machine unlearning: its definitions, scenarios, mechanisms, and\n","applications. Our resources are publicly available at&#xA0;<a href=\"#fn1\"\n","class=\"footnote-ref\" id=\"fnref1\"\n","role=\"doc-noteref\"><sup>1</sup></a>.</p>\n","\n","<h2 id=\"reasons-for-machine-unlearning\">1.1. Reasons for Machine\n","Unlearning</h2>\n","<p>There are many reasons for why a users may want to delete their data\n","from a system. We have categorized these into four major groups:\n","security, privacy, usability, and fidelity. Each reason is discussed in\n","more detail next.</p>\n","<p><strong>Security.</strong> Recently, deep learning models have been\n","shown to be vulnerable to external attacks, especially adversarial\n","attacks&#xA0;<a href=\"#ref-ren2020adversarial\">(K. Ren\n","et al. 2020)</a>. In an adversarial attack, the attacker generates\n","adversarial data that are very similar to the original data to the\n","extent that a human cannot distinguish between the real and fake data.\n","This adversarial data is designed to force the deep learning models into\n","outputting wrong predictions, which frequently results in serious\n","problems. For example, in healthcare, a wrong prediction could lead to a\n","wrong diagnosis, a non-suitable treatment, even a death. Hence,\n","detecting and removing adversarial data is essential for ensuring the\n","model&#x2019;s security and, once an attack is detected, the model needs to be\n","able delete the adversarial data through a machine unlearning\n","mechanism&#xA0;<a href=\"#ref-cao2015towards marchant2022hard\">(Y. Cao and Yang 2015;\n","Marchant, Rubinstein, and Alfeld 2022)</a>.</p>\n","<p><strong>Privacy.</strong> Many privacy-preserving regulations have\n","been enacted recently that involve the right to be forgotten&#x201D;&#xA0;<a href=\"#ref-bourtoule2021machine dang2021right\">(Bourtoule et al. 2021;\n","Dang 2021)</a>, such as the European Union&#x2019;s General Data Protection\n","Regulation (GDPR)&#xA0;<a href=\"#ref-magdziarczyk2019right\">(Mantelero 2013)</a> and the\n","California Consumer Privacy Act&#xA0;<a href=\"#ref-pardau2018california\">(Pardau 2018)</a>. In this\n","particular regulation, users must be given the right to have their data\n","and related information deleted to protect their privacy. In part, this\n","legislation has sprung up as a result of privacy leaks. For example,\n","cloud systems can leak user data due to multiple copies of data hold by\n","different parties, backup policies, and replication strategies&#xA0;<a href=\"#ref-singh2017data\">(A. Singh and Anand\n","2017)</a>. In another case, machine learning approaches for genetic\n","data processing were found to leak patients&#x2019; genetic markers&#xA0;<a href=\"#ref-fredrikson2014privacy wang2009learning\">(Fredrikson et al.\n","2014; R. Wang et al. 2009)</a>. It is therefore not surprising that\n","users would want to remove their data to avoid the risks of a data\n","leak&#xA0;<a href=\"#ref-cao2015towards\">(Y. Cao and Yang\n","2015)</a>.</p>\n","<p><strong>Usability.</strong> People have difference preferences in\n","online applications and/or services, especially recommender systems. An\n","application will produce inconvenient recommendations if it cannot\n","completely delete the incorrect data (e.&#x2006;g.,&#xA0;noise, malicious data,\n","out-of-distribution data) related to a user. For example, one can\n","accidentally search for an illegal product on his laptop, and find that\n","he keeps getting this product recommendation on this phone, even after\n","he cleared his web browser history&#xA0;<a href=\"#ref-cao2015towards\">(Y. Cao and Yang 2015)</a>. Such\n","undesired usability by not forgetting data will not only produce wrong\n","predictions, but also result in less users.</p>\n","<p><strong>Fidelity.</strong> Unlearning requests might come from biased\n","machine learning models. Despite recent advances, machine learning\n","models are still sensitive to bias that means their output can unfairly\n","discriminate against a group of people&#xA0;<a href=\"#ref-mehrabi2021survey\">(Mehrabi et al. 2021)</a>. For\n","example, COMPAS, the software used by courts to decide parole cases, is\n","more likely to consider African-American offenders to have higher risk\n","scores than Caucasians, even though ethnicity information is not part of\n","the input&#xA0;<a href=\"#ref-zou2018ai\">(Zou and\n","Schiebinger 2018)</a>. Similar situations have been observed in\n","beauty contest judged by AI, which was biased against contestants with\n","darker skin tones, or facial recognition AI that wrongly recognized\n","Asian facial features&#xA0;<a href=\"#ref-feuerriegel2020fair\">(Feuerriegel, Dolata, and Schwabe\n","2020)</a>.</p>\n","<p>The source of these biases often originate from data. For example, AI\n","systems that have been trained on public datasets that contain mostly\n","white persons, such as ImageNet, are likely to make errors when\n","processing images of black persons. Similarly, in an application\n","screening system, inappropriate features, such as the gender or race of\n","applicants, might be unintentionally learned by the machine learning\n","model&#xA0;<a href=\"#ref-dinsdale2021deep dinsdale2020unlearning\">(Dinsdale,\n","Jenkinson, and Namburete 2021; Dinsdale, Jenkinson, et al. 2020)</a>.\n","As a result, there is a need to unlearn these data, including the\n","features and affected data items.</p>\n","\n","<h2 id=\"challenges-in-machine-unlearning\">1.2. Challenges in Machine\n","Unlearning</h2>\n","<p>Before we can truly achieve machine unlearning, several challenges to\n","removing specific parts of the training data need to be overcome. The\n","challenges are summarized as follows.</p>\n","<p><strong>Stochasticity of training.</strong> We do not know the impact\n","of each data point seen during training on the machine learning model\n","due to the stochastic nature of the training procedure&#xA0;<a href=\"#ref-bourtoule2021machine\">(Bourtoule et al.\n","2021)</a>. Neural networks, for example, are usually trained on\n","random mini-batches containing a certain number of data samples.\n","Further, the order of the training batches is also random&#xA0;<a href=\"#ref-bourtoule2021machine\">(Bourtoule et al.\n","2021)</a>. This stochasticity raises difficulties for machine\n","unlearning as the specific data sample to be removed would need to be\n","removed from all batches.</p>\n","<p><strong>Incrementality of training.</strong> A model&#x2019;s training\n","procedure is an incremental process&#xA0;<a href=\"#ref-bourtoule2021machine\">(Bourtoule et al. 2021)</a>. In\n","other words, the model update on a given data sample will affect the\n","model performance on data samples fed into the model after this data. A\n","model&#x2019;s performance on this given data sample is also affected by prior\n","data samples. Determining a way to erase the effect of the to-be-removed\n","training sample on further model performance is a challenge for machine\n","unlearning.</p>\n","<p><strong>Catastrophic unlearning.</strong> In general, an unlearned\n","model usually performs worse than the model retrained on the remaining\n","data&#xA0;<a href=\"#ref-nguyen2020variational nguyen2022markov\">(Q. P. Nguyen, Low,\n","and Jaillet 2020; Q. P. Nguyen et al. 2022)</a>. However, the\n","degradation can be exponential when more data is unlearned. Such sudden\n","degradation is often referred as catastrophic unlearning&#xA0;<a href=\"#ref-nguyen2020variational\">(Q. P. Nguyen, Low,\n","and Jaillet 2020)</a>. While several studies&#xA0;<a href=\"#ref-du2019lifelong golatkar2020eternal\">(Du, Chen, et al. 2019;\n","Golatkar, Achille, and Soatto 2020a)</a> have explored ways to\n","mitigate catastrophic unlearning by designing special loss functions,\n","how to naturally prevent catastrophic unlearning is still an open\n","question.</p>\n","\n","\n"]},{"cell_type":"markdown","id":"1a9c41a0","metadata":{"papermill":{"duration":0.006853,"end_time":"2023-07-06T00:16:39.534155","exception":false,"start_time":"2023-07-06T00:16:39.527302","status":"completed"},"tags":[]},"source":["<h1 id=\"sec:framework\">2. Unlearning Framework</h1>\n","<h2 id=\"unlearning-workflow\">2.1. Unlearning Workflow</h2>\n","<p>The unlearning framework in <a href=\"#fig:unlearning_workflow\"\n","data-reference-type=\"autoref\"\n","data-reference=\"fig:unlearning_workflow\">[fig:unlearning_workflow]</a>\n","presents the typical workflow of a machine learning model in the\n","presence of a data removal request. In general, a model is trained on\n","some data and is then used for inference. Upon a removal request, the\n","data-to-be-forgotten is unlearned from the model. The unlearned model is\n","then verified against privacy criteria, and, if these criteria are not\n","met, the model is retrained, i.e., if the model still leaks some\n","information about the forgotten data. There are two main components to\n","this process: the <em>learning component</em> (left) and the\n","<em>unlearning component</em> (right). The learning component involves\n","the current data, a learning algorithm, and the current model. In the\n","beginning, the initial model is trained from the whole dataset using the\n","learning algorithm. The unlearning component involves an unlearning\n","algorithm, the unlearned model, optimization requirements, evaluation\n","metrics, and a verification mechanism. Upon a data removal request, the\n","current model will be processed by an unlearning algorithm to forget the\n","corresponding information of that data inside the model. The unlearning\n","algorithm might take several requirements into account such as\n","completeness, timeliness, and privacy guarantees. The outcome is an\n","unlearned model, which will be evaluated against different performance\n","metrics (e.g., accuracy, ZRF score, anamnesis index). However, to\n","provide a privacy certificate for the unlearned model, a verification\n","(or audit) is needed to prove that the model actually forgot the\n","requested data and that there are no information leaks. This audit might\n","include a feature injection test, a membership inference attack,\n","forgetting measurements, etc.</p>\n","<div class=\"figure*\">\n","<figure>\n","<img src=\"https://raw.githubusercontent.com/tamlhp/awesome-machine-unlearning/main/framework.png\" alt=\"image\" style=\"max-width: 80%;\"/>\n","<figcaption aria-hidden=\"true\">A Typical Machine Unlearning Process</figcaption>\n","</figure>\n","</div>\n","<p>If the unlearned model passes the verification, it becomes the new\n","model for downstream tasks (e.g., inference, prediction, classification,\n","recommendation). If the model does not pass verification, the remaining\n","data, i.e., the original data excluding the data to be forgotten, needs\n","to be used to retrain the model. Either way, the unlearning component\n","will be called repeatedly upon a new removal request.</p>\n","\n","<h2 id=\"unlearning-requests\">2.2. Unlearning Requests</h2>\n","<p><strong>Item Removal.</strong> Requests to remove certain\n","items/samples from the training data are the most common requests in\n","machine unlearning&#xA0;<a href=\"#ref-bourtoule2021machine\">(Bourtoule et al. 2021)</a>. The\n","techniques used to unlearn these data are described in detail in <a\n","href=\"#sec:algorithms\" data-reference-type=\"autoref\"\n","data-reference=\"sec:algorithms\">[sec:algorithms]</a>.</p>\n","<p><strong>Feature Removal.</strong> In many scenarios, privacy leaks\n","might not only originate from a single data item but also in a group of\n","data with the similar features or labels&#xA0;<a href=\"#ref-warnecke2021machine\">(Warnecke et al. 2021)</a>. For\n","example, a poisoned spam filter might misclassify malicious addresses\n","that are present in thousands of emails. Thus, unlearning suspicious\n","emails might not enough. Similarly, in an application screening system,\n","inappropriate features, such as the gender or race of applicants, might\n","need to be unlearned for thousands of affected applications.</p>\n","<p>In such cases, naively unlearning the affected data items\n","sequentially is imprudent as repeated retraining is computationally\n","expensive. Moreover, unlearning too many data items can inherently\n","reduce the performance of the model, regardless of the unlearning\n","mechanism used. Thus, there is a need for unlearning data at the feature\n","or label level with an arbitrary number of data items.</p>\n","<p>Warnecke et al.&#xA0;<a href=\"#ref-warnecke2021machine\">(Warnecke et al. 2021)</a> proposed\n","a technique for unlearning a group of training data based on influence\n","functions. More precisely, the effect of training data on model\n","parameter updates is estimated and formularized in closed-form. As a\n","result of this formulation, influences of the learning sets act as a\n","compact update instead of solving an optimisation problem iteratively\n","(e.g., loss minimization). First-order and second-order derivatives are\n","the keys to computing this update effectively&#xA0;<a href=\"#ref-warnecke2021machine\">(Warnecke et al. 2021)</a>.</p>\n","<p>Guo et al.&#xA0;<a href=\"#ref-guo2022efficient\">(T.\n","Guo et al. 2022)</a> proposed another technique to unlearn a feature\n","in the data based on disentangled representation. The core idea is to\n","learn the correlation between features from the latent space as well as\n","the effects of each feature on the output space. Using this information,\n","certain features can be progressively detached from the learnt model\n","upon request, while the remaining features are still preserved to\n","maintain good accuracy. However, this method is mostly applicable to\n","deep neural networks in the image domain, in which the deeper\n","convolutional layers become smaller and can therefore identify abstract\n","features that match real-world data attributes.</p>\n","<p><strong>Class Removal.</strong> There are many scenarios where the\n","forgetting data belongs to single or multiple classes from a trained\n","model. For example, in face recognition applications, each class is a\n","person&#x2019;s face so there could potentially be thousands or millions of\n","classes. However, when a user opts out of the system, their face\n","information must be removed without using a sample of their face.</p>\n","<p>Similar to feature removal, class removal is more challenging than\n","item removal because retraining solutions can incur many unlearning\n","passes. Even though each pass might only come at a small computational\n","cost due to data partitioning, the expense mounts up. However,\n","partitioning data by class itself does not help the model&#x2019;s training in\n","the first place, as learning the differences between classes is the core\n","of many learning algorithms&#xA0;<a href=\"#ref-tanha2020boosting\">(Tanha et al. 2020)</a>. Although some\n","of the above techniques for feature removal can be applied to class\n","removal&#xA0;<a href=\"#ref-warnecke2021machine\">(Warnecke et al. 2021)</a>, it is\n","not always the case as class information might be implicit in many\n","scenarios.</p>\n","<p>Tarun et al.&#xA0;<a href=\"#ref-tarun2021fast\">(Tarun\n","et al. 2021)</a> proposed an unlearning method for class removal\n","based on data augmentation. The basic concept is to introduce noise into\n","the model such that the classification error is maximized for the target\n","class(es). The model is updated by training on this noise without the\n","need to access any samples of the target class(es). Since such impair\n","step may disturb the model weights and degrade the classification\n","performance for the remaining classes, a repair step is needed to train\n","the model for one or a few more epochs on the remaining data. Their\n","experiments show that the method can be efficient for large-scale\n","multi-class problems (100 classes). Further, the method worked\n","especially well with face recognition tasks because the deep neural\n","networks were originally trained on triplet loss and negative samples so\n","the difference between the classes was quite significant&#xA0;<a href=\"#ref-masi2018deep\">(Masi et al.\n","2018)</a>.</p>\n","<p>Baumhauer et al.&#xA0;<a href=\"#ref-baumhauer2020machine\">(Baumhauer, Sch&#xF6;ttle, and Zeppelzauer\n","2020)</a> proposed an unlearning method for class removal based on a\n","linear filtration operator that proportionally shifts the classification\n","of the samples of the class to be forgotten to other classes. However,\n","the approach is only applicable to class removal due to the\n","characteristics of this operator.</p>\n","<p><strong>Task Removal.</strong> Today, machine learning models are not\n","only trained for a single task but also for multiple tasks. This\n","paradigm, aka continual learning or lifelong learning&#xA0;<a href=\"#ref-parisi2019continual\">(Parisi et al.\n","2019)</a>, is motivated by the human brain, in which learning\n","multiple tasks can benefit each other due to their correlations. This\n","technique is also used overcome data sparsity or cold-start problems\n","where there is not enough data to train a single task effectively.</p>\n","<p>However, in these settings too, there can be a need to remove private\n","data related to a specific task. For example, consider a robot that is\n","trained to assist a patient at home during their medical treatment. This\n","robot may be asked to forget this assistance behaviour after the patient\n","has recovered&#xA0;<a href=\"#ref-liu2022continual\">(B.\n","Liu, Liu, et al. 2022)</a>. To this end, temporarily learning a task\n","and forgetting it in the future has become a need for lifelong learning\n","models.</p>\n","<p>In general, unlearning a task is uniquely challenging as continual\n","learning might depend on the order of the learned tasks. Therefore,\n","removing a task might create a catastrophic unlearning effect, where the\n","overall performance of multiple tasks is degraded in a\n","domino-effect&#xA0;<a href=\"#ref-liu2022continual\">(B.\n","Liu, Liu, et al. 2022)</a>. Mitigating this problem requires the\n","model to be aware of that the task may potentially be removed in future.\n","Liu et al.&#xA0;<a href=\"#ref-liu2022continual\">(B. Liu,\n","Liu, et al. 2022)</a> explains that this requires users to explicitly\n","define which tasks will be learned permanently and which tasks will be\n","learned only temporarily.</p>\n","<p><strong>Stream Removal.</strong> Handling data streams where a huge\n","amount of data arrives online requires some mechanisms to retain or\n","ignore certain data while maintaining limited storage&#xA0;<a href=\"#ref-nguyen2017retaining\">(Tam et al.\n","2017)</a>. In the context of machine unlearning, however, handling\n","data streams is more about dealing with a stream of removal\n","requests.</p>\n","<p>Gupta et el.&#xA0;<a href=\"#ref-gupta2021adaptive\">(Gupta et al. 2021)</a> proposed a\n","streaming unlearning setting involving a sequence of data removal\n","requests. This is motivated by the fact that many users can be involved\n","in a machine learning system and decide to delete their data\n","sequentially. Such is also the case when the training data has been\n","poisoned in an adversarial attack and the data needs to be deleted\n","gradually to recover the model&#x2019;s performance. These streaming requests\n","can be either non-adaptive or adaptive. A non-adaptive request means\n","that the removal sequence does not depend on the intermediate results of\n","each unlearning request, whereas and adaptive request means that the\n","data to be removed depends on the current unlearned model. In other\n","words, after the poisonous data is detected, the model is unlearned\n","gradually so as to decide which data item is most beneficial to unlearn\n","next.</p>\n","\n","<h2 id=\"design-requirements\">2.3. Design Requirements</h2>\n","<p><strong>Completeness (Consistency).</strong> A good unlearning\n","algorithm should be complete&#xA0;<a href=\"#ref-cao2015towards\">(Y. Cao and Yang 2015)</a>, i.e.&#xA0;the\n","unlearned model and the retrained model make the same predictions about\n","any possible data sample (whether right or wrong). One way to measure\n","this consistency is to compute the percentage of the same prediction\n","results on a test data. This requirement can be designed as an\n","optimization objective in an unlearning definition (<a\n","href=\"#sec:exact_unlearning\" data-reference-type=\"autoref\"\n","data-reference=\"sec:exact_unlearning\">[sec:exact_unlearning]</a>) by\n","formulating the difference between the output space of the two models.\n","Many works on adversarial attacks can help with this formulation&#xA0;<a href=\"#ref-sommer2022athena chen2021machine\">(Sommer\n","et al. 2022; M. Chen et al. 2021b)</a>.</p>\n","<p><strong>Timeliness.</strong> In general, retraining can fully solve\n","any unlearning problem. However, retraining is time-consuming,\n","especially when the distribution of the data to be forgotten is\n","unknown&#xA0;<a href=\"#ref-cao2015towards bourtoule2021machine\">(Y. Cao and Yang 2015;\n","Bourtoule et al. 2021)</a>. As a result, there needs to be a\n","trade-off between completeness and timeliness. Unlearning techniques\n","that do not use retraining might be inherently not complete, i.e., they\n","may lead to some privacy leaks, even though some provable guarantees are\n","provided for special cases&#xA0;<a href=\"#ref-GuoGHM20 marchant2022hard neel2021descent\">(C. Guo et al.\n","2020; Marchant, Rubinstein, and Alfeld 2022; Neel, Roth, and\n","Sharifi-Malvajerdi 2021)</a>. To measure timeliness, we can measure\n","the speed up of unlearning over retraining after an unlearning request\n","is invoked.</p>\n","<p>It is also worth recognizing the cause of this trade-off between\n","retraining and unlearning. When there is not much data to be forgotten,\n","unlearning is generally more beneficial as the effects on model accuracy\n","are small. However, when there is much forgetting data, retraining might\n","be better as unlearning many times, even bounded, may catastrophically\n","degrade the model&#x2019;s accuracy&#xA0;<a href=\"#ref-cao2015towards\">(Y. Cao and Yang 2015)</a>.</p>\n","<p><strong>Accuracy.</strong> An unlearned model should be able to\n","predict test samples correctly. Or at least its accuracy should be\n","comparable to the retrained model. However, as retraining is\n","computationally costly, retrained models are not always available for\n","comparison. To address this issue, the accuracy of the unlearned model\n","is often measured on a new test set, or it is compared with that of the\n","original model before unlearning&#xA0;<a href=\"#ref-he2021deepobliviate\">(He et al. 2021)</a>.</p>\n","<p><strong>Light-weight.</strong> To prepare for unlearning process,\n","many techniques need to store model checkpoints, historical model\n","updates, training data, and other temporary data&#xA0;<a href=\"#ref-he2021deepobliviate bourtoule2021machine liu2020federated\">(He\n","et al. 2021; Bourtoule et al. 2021; G. Liu et al. 2020)</a>. A good\n","unlearning algorithm should be light-weight and scale with big data. Any\n","other computational overhead beside unlearning time and storage cost\n","should be reduced as well&#xA0;<a href=\"#ref-bourtoule2021machine\">(Bourtoule et al. 2021)</a>.</p>\n","<p><strong>Provable guarantees.</strong> With the exception of\n","retraining, any unlearning process might be inherently approximate. It\n","is practical for an unlearning method to provide a provable guarantee on\n","the unlearned model. To this end, many works have designed unlearning\n","techniques with bounded approximations on retraining&#xA0;<a href=\"#ref-GuoGHM20 marchant2022hard neel2021descent\">(C. Guo et al.\n","2020; Marchant, Rubinstein, and Alfeld 2022; Neel, Roth, and\n","Sharifi-Malvajerdi 2021)</a>. Nonetheless, these approaches are\n","founded on the premise that models with comparable parameters will have\n","comparable accuracy.</p>\n","<p><strong>Model-agnostic.</strong> An unlearning process should be\n","generic for different learning algorithms and machine learning\n","models&#xA0;<a href=\"#ref-bourtoule2021machine\">(Bourtoule et al. 2021)</a>,\n","especially with provable guarantees as well. However, as machine\n","learning models are different and have different learning algorithms as\n","well, designing a model-agnostic unlearning framework could be\n","challenging.</p>\n","<p><strong>Verifiability.</strong> Beyond unlearning requests, another\n","demand by users is to verify that the unlearned model now protects their\n","privacy. To this end, a good unlearning framework should provide\n","end-users with a verification mechanism. For example, backdoor attacks\n","can be used to verify unlearning by injecting backdoor samples into the\n","training data&#xA0;<a href=\"#ref-sommer2020towards\">(Sommer et al. 2020)</a>. If the\n","backdoor can be detected in the original model while not detected in the\n","unlearned model, then verification is considered to be a success.\n","However, such verification might be too intrusive for a trustworthy\n","machine learning system and the verification might still introduce false\n","positive due to the inherent uncertainty in backdoor detection.</p>\n","\n","<h2 id=\"unlearning-verification\">2.4. Unlearning Verification</h2>\n","<p>The goal of unlearning verification methods is to certify that one\n","cannot easily distinguish between the unlearned models and their\n","retrained counterparts&#xA0;<a href=\"#ref-thudi2021necessity\">(Thudi, Jia, et al. 2022)</a>. While\n","the evaluation metrics (<a href=\"#sec:metrics\"\n","data-reference-type=\"autoref\"\n","data-reference=\"sec:metrics\">[sec:metrics]</a>) are theoretical criteria\n","for machine unlearning, unlearning verification can act as a certificate\n","for an unlearned model. They also include best practices for validating\n","the unlearned models efficiently.</p>\n","<p>It is noteworthy that while unlearning metrics (in <a\n","href=\"#sec:formulation\" data-reference-type=\"autoref\"\n","data-reference=\"sec:formulation\">[sec:formulation]</a>) and verification\n","metrics share some overlaps, the big difference is that the former can\n","be used for optimization or to provide a bounded guarantee, while the\n","latter is used for evaluation only.</p>\n","<p><strong>Feature Injection Test.</strong> The goal of this test is to\n","verify whether the unlearned model has adjusted the weights\n","corresponding to the removed data samples based on data\n","features/attributes&#xA0;<a href=\"#ref-izzo2021approximate\">(Izzo et al. 2021)</a>. The idea is\n","that if the set of data to be forgotten has a very distinct feature\n","distinguishing it from the remaining set, it gives a strong signal for\n","the model weights. However, this feature needs to be correlated with the\n","labels of the set to be forgotten, otherwise the model might not learn\n","anything from this feature.</p>\n","<p>More precisely, an extra feature is added for each data item such\n","that it is equal to zero for the remaining set and is perfectly\n","correlated with the labels of the set to forget. Izzo et al.&#xA0;<a href=\"#ref-izzo2021approximate\">(Izzo et al.\n","2021)</a> applied this idea with linear classifiers, where the weight\n","associated with this extra feature is expected to be significantly\n","different from zero after training. After the model is unlearned, this\n","weight is expected to become zero. As a result, the difference of this\n","weight can be plotted before and after unlearning as a measure of\n","effectiveness of the unlearning process.</p>\n","<p>One limitation of this verification method is that the current\n","solution is only applicable for linear and logistic models&#xA0;<a href=\"#ref-izzo2021approximate\">(Izzo et al.\n","2021)</a>. This is because these models have explicit weights\n","associated with the injected feature, whereas, for other models such as\n","deep learning, injecting such a feature as a strong signal is\n","non-trivial, even though the set to be forgotten is small. Another\n","limitation to these types of methods is that an injected version of the\n","data needs to be created so that the model can be learned (either from\n","scratch or incrementally depending on the type of the model).</p>\n","<p><strong>Forgetting Measuring.</strong> Even after the data to be\n","forgotten has been unlearned from the model, it is still possible for\n","the model to carry detectable traces of those samples&#xA0;<a href=\"#ref-jagielski2022measuring\">(Jagielski et al.\n","2022)</a>. Jagielski et al.&#xA0;<a href=\"#ref-jagielski2022measuring\">(Jagielski et al. 2022)</a>\n","proposed a formal way to measure the forgetfulness of a model via\n","privacy attacks. More precisely, a model is said to <span\n","class=\"math inline\"><em>&#x3B1;</em></span>-forget a training sample if a\n","privacy attack (e.g., a membership inference) on that sample achieves no\n","greater than success rate <span class=\"math inline\"><em>&#x3B1;</em></span>.\n","This definition is more flexible than differential privacy because a\n","training algorithm is differentially private only if it immediately\n","forgets every sample it learns. As a result, this definition allows a\n","sample to be temporarily learned, and measures how long until it is\n","forgotten by the model.</p>\n","<p><strong>Information Leakage.</strong> Many machine learning models\n","inherently leak information during the model updating process&#xA0;<a href=\"#ref-chen2021machine\">(M. Chen et al.\n","2021b)</a>. Recent works have exploited this phenomenon by comparing\n","the model before and after unlearning to measure the information\n","leakage. More precisely, Salem et al.&#xA0;<a href=\"#ref-salem2020updates\">(Salem et al. 2020)</a> proposed an\n","adversary attack in the image domain that could reconstruct a removed\n","sample when a classifier is unlearned on a data sample. Brockschmidt et\n","al.&#xA0;<a href=\"#ref-zanella2020analyzing\">(Zanella-B&#xE9;guelin et al. 2020)</a>\n","suggested a similar approach for the text domain. Chen et al.&#xA0;<a href=\"#ref-chen2021machine\">(M. Chen et al.\n","2021b)</a> introduced a membership inference attack to detect whether\n","a removed sample belongs to the learning set. Compared to previous\n","works&#xA0;<a href=\"#ref-Salem0HBF019 shokri2017membership\">(Salem et al. 2019;\n","Shokri et al. 2017)</a>, their approach additionally makes use of the\n","posterior output distribution of the original model, besides that of the\n","unlearned model. Chen et al.&#xA0;<a href=\"#ref-chen2021machine\">(M. Chen et al. 2021b)</a> also proposed\n","two leakage metrics, namely the degradation count and the degradation\n","rate.</p>\n","<div class=\"compactitem\">\n","<p>The <em>degradation count:</em> is defined as the ratio between the\n","number of target samples whose membership can be inferred by the\n","proposed attack with higher confidence compared to traditional attacks\n","and the total number of samples.</p>\n","<p>The <em>degradation rate:</em> is defined the average improvement\n","rate of the confidence of the proposed attack compared to traditional\n","attacks.</p>\n","</div>\n","<p><strong>Membership Inference Attacks.</strong> This kind of attack is\n","designed to detect whether a target model leaks data&#xA0;<a href=\"#ref-shokri2017membership thudi2022bounding chen2021machine\">(Shokri\n","et al. 2017; Thudi, Shumailov, et al. 2022; M. Chen et al.\n","2021b)</a>. Specifically, an inference model is trained to recognise\n","new data samples from the training data used to optimize the target\n","model. In&#xA0;<a href=\"#ref-shokri2017membership\">(Shokri et al. 2017)</a>, a set of\n","shallow models were trained on a new set of data items different from\n","the one that the target model was trained on. The attack model was then\n","trained to predict whether a data item belonged to the training data\n","based on the predictions made by shallow models for training as well as\n","testing data. The training set for the shallow and attack models share\n","similar data distribution to the target model. Membership inference\n","attacks are helpful for detecting data leaks. Hence, they are useful for\n","verifying the effectiveness of the machine unlearning&#xA0;<a href=\"#ref-chen2021machine\">(M. Chen et al.\n","2021b)</a>.</p>\n","<p><strong>Backdoor attacks.</strong> Backdoor attacks were proposed to\n","inject backdoors to the data for deceiving a machine learning\n","model&#xA0;<a href=\"#ref-wang2019neural\">(B. Wang et al.\n","2019)</a>. The deceived model makes correct predictions with clean\n","data, but with poison data in a target class as a backdoor trigger, it\n","makes incorrect predictions. Backdoor attacks were used to verify the\n","effectiveness of machine unlearning in&#xA0;<a href=\"#ref-sommer2020towards sommer2022athena\">(Sommer et al. 2020,\n","2022)</a>. Specifically, the setting begins with training a model\n","that has a mixture of clean and poison data items across all users. Some\n","of the users want their data deleted. If the users&#x2019; data are not\n","successfully deleted, the poison samples will be predicted as the target\n","class. Otherwise, the model will not predict the poison samples as the\n","target class. However, there is no absolute guarantee that this rule is\n","always correct, although one can increase the number of poison samples\n","to make this rule less likely to fail.</p>\n","<p><strong>Slow-down attacks.</strong> Some studies focus on the\n","theoretical guarantee of indistinguishability between an unlearned and a\n","retrained models. However, the practical bounds on computation costs are\n","largely neglected in these papers&#xA0;<a href=\"#ref-marchant2022hard\">(Marchant, Rubinstein, and Alfeld\n","2022)</a>. As a result, a new threat has been introduced to machine\n","unlearning where poisoning attacks are used to slow down the unlearning\n","process. Formally, let <span\n","class=\"math inline\"><em>h</em><sub>0</sub>&#x2004;=&#x2004;<em>A</em>(<em>D</em>)</span>\n","be an initial model trained by a learning algorithm <span\n","class=\"math inline\"><em>A</em></span> on a dataset <span\n","class=\"math inline\"><em>D</em></span>. The goal of the attacker is to\n","poison a subset <span\n","class=\"math inline\"><em>D</em><sub><em>p</em><em>o</em><em>i</em><em>s</em><em>o</em><em>n</em></sub>&#x2004;&#x2282;&#x2004;<em>D</em></span>\n","such as to maximize the computation cost of removing <span\n","class=\"math inline\"><em>D</em><sub><em>p</em><em>o</em><em>i</em><em>s</em><em>o</em><em>n</em></sub></span>\n","from <span class=\"math inline\"><em>h&#x302;</em></span> using an unlearning\n","algorithm <span class=\"math inline\"><em>U</em></span>. Marchant et al.\n","&#xA0;<a href=\"#ref-marchant2022hard\">(Marchant,\n","Rubinstein, and Alfeld 2022)</a> defined and estimated an efficient\n","computation cost for certifying removal methods. However, generalizing\n","this computation cost for different unlearning methods is still an open\n","research direction.</p>\n","<p><strong>Interclass Confusion Test.</strong> The idea of this test is\n","to investigate whether information from the data to be forgotten can\n","still be inferred from an unlearned model&#xA0;<a href=\"#ref-goel2022evaluating\">(Goel, Prabhu, and Kumaraguru\n","2022)</a>. Different from traditional approximate unlearning\n","definitions that focus on the indistinguishability between unlearned and\n","retrained models in the parameter space, this test focuses on the output\n","space. More precisely, the test involves randomly selecting a set of\n","samples <span class=\"math inline\"><em>S</em>&#x2004;&#x2282;&#x2004;<em>D</em></span> from\n","two chosen classes in the training data <span\n","class=\"math inline\"><em>D</em></span> and then randomly swapping the\n","label assignment between the samples of different classes to result in a\n","confused set <span class=\"math inline\"><em>S</em>&#x2032;</span>. Together\n","<span class=\"math inline\"><em>S</em>&#x2032;</span> and <span\n","class=\"math inline\"><em>D</em>&#x2005;\\&#x2005;<em>S</em></span> form a new training\n","dataset <span class=\"math inline\"><em>D</em>&#x2032;</span>, resulting in a new\n","trained model. <span class=\"math inline\"><em>S</em>&#x2032;</span> is\n","considered to be the forgotten data. From this, Goet et al.&#xA0;<a href=\"#ref-goel2022evaluating\">(Goel, Prabhu, and\n","Kumaraguru 2022)</a> computes a forgetting score from a confusion\n","matrix generated by the unlearned model. A lower forgetting score means\n","a better unlearned model.</p>\n","<p><strong>Federated verification.</strong> Unlearning verification in\n","federated learning is uniquely challenging. First, the participation of\n","one or a few clients in the federation may subtly change the global\n","model&#x2019;s performance, making verification in the output space\n","challenging. Second, verification using adversarial attacks is not\n","applicable in the federated setting because it might introduce new\n","security threats to the infrastructure&#xA0;<a href=\"#ref-gao2022verifi\">(X. Gao et al. 2022)</a>. As a result, Gao\n","et al.&#xA0;<a href=\"#ref-gao2022verifi\">(X. Gao et al.\n","2022)</a> proposes a verification mechanism that uses a few\n","communication rounds for clients to verify their data in the global\n","model. This approach is compatible with federated settings because the\n","model is trained in the same way where the clients communicate with the\n","server over several rounds.</p>\n","<p><strong>Cryptographic proofs.</strong> Since most of existing\n","verification frameworks do not provide any theoretical guarantee,\n","Eisenhofer et al.&#xA0;<a href=\"#ref-eisenhofer2022verifiable\">(Eisenhofer et al. 2022)</a>\n","proposed a cryptography-informed protocol to compute two proofs,\n","i.e.&#xA0;proof of update (the model was trained on a particular dataset\n","<span class=\"math inline\"><em>D</em></span>) and proof of unlearning\n","(the forget item <span class=\"math inline\"><em>d</em></span> is not a\n","member of <span class=\"math inline\"><em>D</em></span>). The core idea of\n","the proof of update is using SNARK&#xA0;<a href=\"#ref-bitansky2012extractable\">(Bitansky et al. 2012)</a> data\n","structure to commit a hash whenever the model is updated (learned or\n","unlearned) while ensuring that: (i) the model was obtained from the\n","remaining data, (ii) the remaining data does not contain any forget\n","items, (iii) the previous forget set is a subset of the current forget\n","set, and (iv) the forget items are never re-added into the training\n","data. The core idea of the proof of unlearning is using the Merkle tree\n","to maintain the order of data items in the training data so that an\n","unlearned item cannot be added to the training data again. While the\n","approach is demonstrated on SISA (efficient retraining)&#xA0;<a href=\"#ref-bourtoule2021machine\">(Bourtoule et al.\n","2021)</a>, it is applicable for any unlearning method.</p>\n","\n"]},{"cell_type":"markdown","id":"adf288f3","metadata":{"papermill":{"duration":0.006487,"end_time":"2023-07-06T00:16:39.547535","exception":false,"start_time":"2023-07-06T00:16:39.541048","status":"completed"},"tags":[]},"source":["<h1 id=\"sec:problem\">3. Unlearning Definition</h1>\n","<p>While the application of machine unlearning can originate from\n","security, usability, fidelity, and privacy reasons, it is often\n","formulated as a privacy preserving problem where users can ask for the\n","removal of their data from computer systems and machine learning\n","models&#xA0;<a href=\"#ref-sekhari2021remember ginart2019making bourtoule2021machine garg2020formalizing\">(Sekhari\n","et al. 2021; Ginart et al. 2019; Bourtoule et al. 2021; Garg,\n","Goldwasser, and Vasudevan 2020)</a>. The forgetting request can be\n","motivated by security and usability reasons as well. For example, the\n","models can be attacked by adversarial data and produce wrong outputs.\n","Once these types of attacks are detected, the corresponding adversarial\n","data has to be removed as well without harming the model&#x2019;s predictive\n","performance.</p>\n","<p>When fulfilling a removal request, the computer system needs to\n","remove all user&#x2019;s data and &#x2018;forget&#x2019; any influence on the models that\n","were trained on those data. As removing data from a database is\n","considered trivial, the literature mostly concerns how to unlearn data\n","from a model&#xA0;<a href=\"#ref-GuoGHM20 izzo2021approximate neel2021descent ullah2021machine\">(C.\n","Guo et al. 2020; Izzo et al. 2021; Neel, Roth, and Sharifi-Malvajerdi\n","2021; Ullah et al. 2021)</a>.</p>\n","<p>To properly formulate an unlearning problem, we need to introduce a\n","few concepts. First, let us denote <span class=\"math inline\">&#x1D4B5;</span> as\n","an example space, i.e., a space of data items or examples (called\n","samples). Then, the set of all possible training datasets is denoted as\n","<span class=\"math inline\">&#x1D4B5;<sup>*</sup></span>. One can argue that <span\n","class=\"math inline\">&#x1D4B5;<sup>*</sup>&#x2004;=&#x2004;2<sup>&#x1D4B5;</sup></span> but that is not\n","important, as a particular training dataset <span\n","class=\"math inline\"><em>D</em>&#x2004;&#x2208;&#x2004;<em>Z</em><sup>*</sup></span> is often\n","given as input. Given <span class=\"math inline\"><em>D</em></span>, we\n","want to get a machine learning model from a hypothesis space <span\n","class=\"math inline\">&#x210B;</span>. In general, the hypothesis space <span\n","class=\"math inline\">&#x210B;</span> covers the parameters and the meta-data of\n","the models. Sometimes, it is modeled as <span\n","class=\"math inline\">&#x1D4B2;&#x2005;&#xD7;&#x2005;<em>&#x398;</em></span>, where <span\n","class=\"math inline\">&#x1D4B2;</span> is the parameter space and <span\n","class=\"math inline\"><em>&#x398;</em></span> is the metadata/state space. The\n","process of training a model on <span\n","class=\"math inline\"><em>D</em></span> in the given computer system is\n","enabled by a learning algorithm, denoted by a function <span\n","class=\"math inline\"><em>A</em>&#x2004;:&#x2004;&#x1D4B5;<sup>*</sup>&#x2004;&#x2192;&#x2004;&#x210B;</span>, with the\n","trained model denoted as <span\n","class=\"math inline\"><em>A</em>(<em>D</em>)</span>.</p>\n","<p>To support forgetting requests, the computer system needs to have an\n","unlearning mechanism, denoted by a function <span\n","class=\"math inline\"><em>U</em></span>, that takes as input a training\n","dataset <span\n","class=\"math inline\"><em>D</em>&#x2004;&#x2208;&#x2004;<em>Z</em><sup>*</sup></span>, a forget\n","set <span\n","class=\"math inline\"><em>D</em><sub><em>f</em></sub>&#x2004;&#x2282;&#x2004;<em>D</em></span>\n","(data to forget) and a model <span\n","class=\"math inline\"><em>A</em>(<em>D</em>)</span>. It returns a\n","sanitized (or unlearned) model <span\n","class=\"math inline\"><em>U</em>(<em>D</em>,<em>D</em><sub><em>f</em></sub>,<em>A</em>(<em>D</em>))&#x2004;&#x2208;&#x2004;&#x210B;</span>.\n","The unlearned model is expected to be the same or similar to a retrained\n","model <span\n","class=\"math inline\"><em>A</em>(<em>D</em>\\<em>D</em><sub><em>f</em></sub>)</span>\n","(i.e., a model as if it had been trained on the remaining data). Note\n","that <span class=\"math inline\"><em>A</em></span> and <span\n","class=\"math inline\"><em>U</em></span> are assumed to be randomized\n","algorithms, i.e., the output is non-deterministic and can be modelled as\n","a conditional probability distribution over the hypothesis space given\n","the input data&#xA0;<a href=\"#ref-marchant2022hard\">(Marchant, Rubinstein, and Alfeld\n","2022)</a>. This assumption is reasonable as many learning algorithms\n","are inherently stochastic (e.g., SGD) and some floating-point operations\n","involve randomness in computer implementations&#xA0;<a href=\"#ref-bourtoule2021machine\">(Bourtoule et al. 2021)</a>.\n","Another note is that we do not define the function <span\n","class=\"math inline\"><em>U</em></span> precisely before-hand as its\n","definition varies with different settings.</p>\n","\n"]},{"cell_type":"markdown","id":"d8478deb","metadata":{"papermill":{"duration":0.006691,"end_time":"2023-07-06T00:16:39.561437","exception":false,"start_time":"2023-07-06T00:16:39.554746","status":"completed"},"tags":[]},"source":["<h1 id=\"sec:algorithms\">4. Unlearning Algorithms</h1>\n","<p>As mentioned in the Section&#xA0;<a href=\"#sec:intro\"\n","data-reference-type=\"ref\" data-reference=\"sec:intro\">1</a>, machine\n","unlearning can remove data and data linkages without retraining the\n","machine learning model from scratch, saving time and computational\n","resources&#xA0;<a href=\"#ref-wang2022federated chen2021machinegan\">(J. Wang, Guo, et al.\n","2022; K. Chen, Huang, et al. 2021)</a>. The specific approaches of\n","machine unlearning can be categorized into model-agnostic,\n","model-intrinsic, and data-driven approaches.</p>\n","<div class=\"table*\">\n","<div class=\"adjustbox\">\n","<p>max width=0.9</p>\n","<div class=\"threeparttable\">\n","<table>\n","<tbody>\n","<tr class=\"odd\">\n","<td style=\"text-align: left;\">\n","<strong>Unlearning Methods</strong>\n","</td>\n","<td colspan=\"5\" style=\"text-align: center;\">\n","<strong>Unlearning Scenarios</strong>\n","</td>\n","<td colspan=\"6\" style=\"text-align: center;\">\n","<strong>Design Requirements</strong>\n","</td>\n","<td colspan=\"5\" style=\"text-align: center;\">\n","<strong>Unlearning Requests</strong>\n","</td>\n","</tr>\n","<tr class=\"even\">\n","<td style=\"text-align: left;\">\n","</td>\n","<td style=\"text-align: center;\">\n","<div class=\"sideways\">\n","\n","</div>\n","</td>\n","<td style=\"text-align: center;\">\n","<div class=\"sideways\">\n","\n","</div>\n","</td>\n","<td style=\"text-align: center;\">\n","<div class=\"sideways\">\n","\n","</div>\n","</td>\n","<td style=\"text-align: center;\">\n","<div class=\"sideways\">\n","\n","</div>\n","</td>\n","<td style=\"text-align: center;\">\n","<div class=\"sideways\">\n","\n","</div>\n","</td>\n","<td style=\"text-align: center;\">\n","<div class=\"sideways\">\n","\n","</div>\n","</td>\n","<td style=\"text-align: center;\">\n","<div class=\"sideways\">\n","\n","</div>\n","</td>\n","<td style=\"text-align: center;\">\n","<div class=\"sideways\">\n","\n","</div>\n","</td>\n","<td style=\"text-align: center;\">\n","<div class=\"sideways\">\n","\n","</div>\n","</td>\n","<td style=\"text-align: center;\">\n","<div class=\"sideways\">\n","\n","</div>\n","</td>\n","<td style=\"text-align: center;\">\n","<div class=\"sideways\">\n","\n","</div>\n","</td>\n","<td style=\"text-align: center;\">\n","<div class=\"sideways\">\n","\n","</div>\n","</td>\n","<td style=\"text-align: center;\">\n","<div class=\"sideways\">\n","\n","</div>\n","</td>\n","<td style=\"text-align: center;\">\n","<div class=\"sideways\">\n","\n","</div>\n","</td>\n","<td style=\"text-align: center;\">\n","<div class=\"sideways\">\n","\n","</div>\n","</td>\n","<td style=\"text-align: center;\">\n","<div class=\"sideways\">\n","\n","</div>\n","</td>\n","</tr>\n","<tr class=\"odd\">\n","<td style=\"text-align: left;\">\n","<span><strong>Model-agnostic</strong></span>\n","</td>\n","<td colspan=\"9\" style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","</tr>\n","<tr class=\"even\">\n","<td style=\"text-align: left;\">\n","Differential privacy&#xA0;<span class=\"citation\"\n","data-cites=\"gupta2021adaptive\"></span>\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","</tr>\n","<tr class=\"odd\">\n","<td style=\"text-align: left;\">\n","Certified removal&#xA0;<span class=\"citation\"\n","data-cites=\"GuoGHM20 golatkar2020eternal neel2021descent ullah2021machine\"></span>\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","</tr>\n","<tr class=\"even\">\n","<td style=\"text-align: left;\">\n","Statistical query learning&#xA0;<span class=\"citation\"\n","data-cites=\"cao2015towards\"></span>\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","</tr>\n","<tr class=\"odd\">\n","<td style=\"text-align: left;\">\n","Decremental learning&#xA0;<span class=\"citation\"\n","data-cites=\"ginart2019making chen2019novel\"></span>\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","</tr>\n","<tr class=\"even\">\n","<td style=\"text-align: left;\">\n","Knowledge adaptation&#xA0;<span class=\"citation\"\n","data-cites=\"chundawat2022can\"></span>\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","</tr>\n","<tr class=\"odd\">\n","<td style=\"text-align: left;\">\n","Parameter sampling&#xA0;<span class=\"citation\"\n","data-cites=\"nguyen2022markov\"></span>\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","</tr>\n","<tr class=\"even\">\n","<td style=\"text-align: left;\">\n","<span><strong>Model-intrinsic</strong></span>\n","</td>\n","<td colspan=\"9\" style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","</tr>\n","<tr class=\"odd\">\n","<td style=\"text-align: left;\">\n","Softmax classifiers&#xA0;<span class=\"citation\"\n","data-cites=\"baumhauer2020machine\"></span>\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","</tr>\n","<tr class=\"even\">\n","<td style=\"text-align: left;\">\n","Linear models&#xA0;<span class=\"citation\"\n","data-cites=\"izzo2021approximate li2020online\"></span>\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","</tr>\n","<tr class=\"odd\">\n","<td style=\"text-align: left;\">\n","Tree-based models&#xA0;<span class=\"citation\"\n","data-cites=\"schelter2021hedgecut\"></span>\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","</tr>\n","<tr class=\"even\">\n","<td style=\"text-align: left;\">\n","Bayesian models&#xA0;<span class=\"citation\"\n","data-cites=\"nguyen2020variational\"></span>\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","</tr>\n","<tr class=\"odd\">\n","<td style=\"text-align: left;\">\n","DNN-based models&#xA0;<span class=\"citation\"\n","data-cites=\"he2021deepobliviate goyal2021revisiting mehta2022deep golatkar2021mixed golatkar2020forgetting basu2021influence zhang2022machine\"></span>\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","-\n","</td>\n","<td style=\"text-align: center;\">\n","-\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","-\n","</td>\n","<td style=\"text-align: center;\">\n","-\n","</td>\n","<td style=\"text-align: center;\">\n","-\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","</tr>\n","<tr class=\"even\">\n","<td style=\"text-align: left;\">\n","<span><strong>Data-driven</strong></span>\n","</td>\n","<td colspan=\"9\" style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","</tr>\n","<tr class=\"odd\">\n","<td style=\"text-align: left;\">\n","Data partition&#xA0;<span class=\"citation\"\n","data-cites=\"bourtoule2021machine aldaghri2021coded\"></span>\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","</tr>\n","<tr class=\"even\">\n","<td style=\"text-align: left;\">\n","Data augmentation&#xA0;<span class=\"citation\"\n","data-cites=\"huang2021unlearnable shan2020protecting tarun2021fast yu2021does\"></span>\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","</tr>\n","<tr class=\"odd\">\n","<td style=\"text-align: left;\">\n","Data influence&#xA0;<span class=\"citation\"\n","data-cites=\"peste2021ssse zeng2021learning cao2022machine\"></span>\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","&#x2013;\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","<td style=\"text-align: center;\">\n","</td>\n","</tr>\n","</tbody>\n","</table>\n","<div class=\"tablenotes\">\n","<dl>\n","<dt>: fully support</dt>\n","<dd>\n","<p>no support</p>\n","</dd>\n","<dt>&#x2013;: partially or indirectly support</dt>\n","<dd>\n","<p>representative citations</p>\n","</dd>\n","</dl>\n","</div>\n","</div>\n","</div>\n","</div>"]},{"cell_type":"markdown","id":"0594d35b","metadata":{"papermill":{"duration":0.006692,"end_time":"2023-07-06T00:16:39.575455","exception":false,"start_time":"2023-07-06T00:16:39.568763","status":"completed"},"tags":[]},"source":[]},{"cell_type":"markdown","id":"594c6c74","metadata":{"papermill":{"duration":0.006675,"end_time":"2023-07-06T00:16:39.58923","exception":false,"start_time":"2023-07-06T00:16:39.582555","status":"completed"},"tags":[]},"source":["<h2 id=\"sec:model-agnostic\">4.1. Model-Agnostic Approaches</h2>\n","\n","\n","\n","<p>Model-agnostic machine unlearning methodologies include unlearning\n","processes or frameworks that are applicable to different models.\n","However, in some cases, theoretical guarantees are only provided for a\n","class of models (e.g., linear models). Nonetheless, they are still\n","considered to be model-agnostic as their core ideas are applicable to\n","complex models (e.g.&#xA0;deep neural networks) with practical results.</p>\n","\n","<div class=\"figure*\">\n","<figure>\n","<img src=\"https://raw.githubusercontent.com/tamlhp/awesome-machine-unlearning/main/figs/model-agnostic.png\" alt=\"https://arxiv.org/abs/2209.02299\" style=\"max-width: 70%;\"/>\n","</figure>\n","</div>\n","\n","<p><strong>Differential Privacy.</strong> Differential privacy was first\n","proposed to bound a data sample&#x2019;s influence on a machine learning\n","model&#xA0;<a href=\"#ref-dwork2008differential\">(Dwork\n","2008)</a>. <span class=\"math inline\"><em>&#x3F5;</em></span>-differential\n","privacy unlearns a data sample by setting <span\n","class=\"math inline\"><em>&#x3F5;</em>&#x2004;=&#x2004;0</span>, where <span\n","class=\"math inline\"><em>&#x3F5;</em></span> bounds the level of change in any\n","model parameters affected by that data sample&#xA0;<a href=\"#ref-bourtoule2021machine thudi2022unrolling\">(Bourtoule et al.\n","2021; Thudi, Deza, et al. 2022)</a>. However, Bourtoule et al.&#xA0;<a href=\"#ref-bourtoule2021machine\">(Bourtoule et al.\n","2021)</a> notes that the algorithm cannot learn from the training\n","data in such a case. Gupta et el.&#xA0;<a href=\"#ref-gupta2021adaptive\">(Gupta et al. 2021)</a> proposed a\n","differentially private unlearning mechanism for streaming data removal\n","requests. These requests are adaptive as well, meaning the data to be\n","removed depends on the current unlearned model. The idea, which is based\n","on differential privacy, can be roughly formulated as: <span\n","class=\"math display\">Pr&#x2006;(<em>U</em>(<em>D</em>,<em>s</em>,<em>A</em>(<em>D</em>))&#x2208;&#x1D4AF;)&#x2004;&#x2264;&#x2004;<em>e</em><sup><em>&#x3F5;</em></sup><em>P</em><em>r</em>(<em>A</em>(<em>D</em>\\<em>s</em>)&#x2208;&#x1D4AF;)&#x2005;+&#x2005;<em>&#x3B2;</em></span>\n","for all adaptive removal sequences <span\n","class=\"math inline\"><em>s</em>&#x2004;=&#x2004;(<em>z</em><sub>1</sub>,&#x2026;,<em>z</em><sub><em>k</em></sub>)</span>.\n","One weakness of this condition is that it only guarantees the upper\n","bound of the unlearning scheme compared to full retraining. However, its\n","strength is that it supports a user&#x2019;s belief that the system has engaged\n","in full retraining. Finally, an unlearning process is developed by a\n","notion of differentially private publishing functions and a theoretical\n","reduction from adaptive to non-adaptive sequences. Differentially\n","private publishing functions guarantee that the model before and after\n","an unlearning request do not differ too much.</p>\n","<p><strong>Certified Removal Mechanisms.</strong> Unlearning algorithms\n","falling into this category are the ones following the original\n","approximate definition of machine unlearning&#xA0;<a href=\"#ref-GuoGHM20 golatkar2020eternal\">(C. Guo et al. 2020; Golatkar,\n","Achille, and Soatto 2020a)</a>. While Guo et al.&#xA0;<a href=\"#ref-GuoGHM20\">(C. Guo et al. 2020)</a> focus\n","on theoretical guarantees for linear models and convex losses, Golatkar\n","et al.&#xA0;<a href=\"#ref-golatkar2020eternal\">(Golatkar, Achille, and Soatto\n","2020a)</a> introduce a computable upper bound for SGD-based learning\n","algorithms, especially deep neural networks. The core idea is based on\n","the notion of perturbation (noise) to mask the small residue incurred by\n","the gradient-based update (e.g., a one-step Newton update&#xA0;<a href=\"#ref-koh2017understanding\">(Koh et al.\n","2017)</a>). The idea is applicable to other cases, although no\n","theoretical guarantees are provided&#xA0;<a href=\"#ref-bourtoule2021machine\">(Bourtoule et al. 2021)</a>.</p>\n","<p>More precisely, certified removal mechanisms mainly accommodate those\n","linear models that minimize a standardized empirical risk, which is the\n","total value of a convex loss function that measures the distance of the\n","actual value from the expected one&#xA0;<a href=\"#ref-marchant2022hard\">(Marchant, Rubinstein, and Alfeld\n","2022)</a>. However, one has to rely on a customized learning\n","algorithm that optimizes a perturbed version of the regularized\n","empirical risk, where the added noise is drawn from a standard normal\n","distribution. This normalized noise allows conventional convex\n","optimization techniques to solve the learning problem with perturbation.\n","As a result, the unlearning request can be done by computing the model\n","perturbation towards the regularized empirical risk on the remaining\n","data. The final trick is that this perturbation can be approximated by\n","the influence function&#xA0;<a href=\"#ref-koh2017understanding\">(Koh et al. 2017)</a>, which is\n","computed by inverting the Hessian on training data and the gradient of\n","the data to be forgotten&#xA0;<a href=\"#ref-marchant2022hard\">(Marchant, Rubinstein, and Alfeld\n","2022)</a>. However, the error of model parameters in such a\n","computation can be so large that the added noise cannot mask it.\n","Therefore, if the provided theoretical upper bound exceeds a certain\n","threshold, the unlearning algorithm resorts to retraining from\n","scratch&#xA0;<a href=\"#ref-marchant2022hard\">(Marchant,\n","Rubinstein, and Alfeld 2022)</a>.</p>\n","<p>Following this idea, Neel et al.&#xA0;<a href=\"#ref-neel2021descent\">(Neel, Roth, and Sharifi-Malvajerdi\n","2021)</a> provided further extensions, namely regularized perturbed\n","gradient descent and distributed perturbed gradient descent, to support\n","weak convex losses and provide theoretical guarantees on\n","indistinguishability, accuracy, and unlearning times.</p>\n","<p>Ullah et al.&#xA0;<a href=\"#ref-ullah2021machine\">(Ullah et al. 2021)</a> continued\n","studying machine unlearning in the context of SGD and streaming removal\n","requests. They define the notation of total variation stability for a\n","learning algorithm: <span\n","class=\"math display\">sup<sub><em>D</em>,&#x2006;<em>D</em>&#x2032;&#x2004;:&#x2004;|<em>D</em>\\<em>D</em>&#x2032;|&#x2005;+&#x2005;|<em>D</em>&#x2032;\\<em>D</em>|</sub><em>&#x394;</em>(<em>A</em>(<em>D</em>),<em>A</em>(<em>D</em>&#x2032;))&#x2004;&#x2264;&#x2004;<em>&#x3C1;</em></span>\n","where <span class=\"math inline\"><em>&#x394;</em>(.)</span> is the largest\n","possible difference between the two probabilities such that they can\n","assign to the same event, aka total variance distance&#xA0;<a href=\"#ref-verdu2014total\">(Verd&#xFA; 2014)</a>. This\n","is also a special case of the optimal transportation cost between two\n","probability distributions&#xA0; <a href=\"#ref-lei2019geometric\">(Lei et al. 2019)</a>. In other words,\n","a learning algorithm <span class=\"math inline\"><em>A</em>(.)</span> is\n","said to be <span class=\"math inline\"><em>&#x3C1;</em></span>-TV-stable if\n","given any two training datasets <span\n","class=\"math inline\"><em>D</em></span> and <span\n","class=\"math inline\"><em>D</em>&#x2032;</span>, as long as they have 1 common\n","data item, the cost of transporting from the model distribution <span\n","class=\"math inline\"><em>A</em>(<em>D</em>)</span> to <span\n","class=\"math inline\"><em>A</em>(<em>D</em>&#x2032;)</span> is bounded by <span\n","class=\"math inline\"><em>&#x3C1;</em></span>. For any <span\n","class=\"math inline\">1/<em>n</em>&#x2004;&#x2264;&#x2004;<em>&#x3C1;</em>&#x2004;&lt;&#x2004;&#x221E;</span>, Ullah et\n","al.&#xA0;<a href=\"#ref-ullah2021machine\">(Ullah et al.\n","2021)</a> proved that there exists an unlearning process that\n","satisfies exact unlearning at any time in the streaming removal request\n","while the model accuracy and the unlearning time are bounded w.r.t.\n","<span class=\"math inline\"><em>&#x3C1;</em></span>.</p>\n","<p><strong>Statistical Query Learning.</strong> Statistical query\n","learning is a form of machine learning that trains models by querying\n","statistics on the training data rather than itself&#xA0;<a href=\"#ref-cao2015towards\">(Y. Cao and Yang\n","2015)</a>. In this form, a data sample can be forgotten efficiently\n","by recomputing the statistics over the remaining data&#xA0;<a href=\"#ref-bourtoule2021machine\">(Bourtoule et al.\n","2021)</a>. More precisely, statistical query learning assumes that\n","most of the learning algorithms can be represented as a sum of some\n","efficiently computable transformations, called statistical queries&#xA0;<a href=\"#ref-kearns1998efficient\">(Kearns 1998)</a>.\n","These statistical queries are basically requests to an oracle (e.g., a\n","ground truth) to estimate a statistical function over all training data.\n","Cao et al.&#xA0;<a href=\"#ref-cao2015towards\">(Y. Cao\n","and Yang 2015)</a> showed that this formulation can generalize many\n","algorithms for machine learning, such as the Chi-square test, naive\n","Bayes, and linear regression. For example, in naive Bayes, these\n","statistical queries are indicator functions that return 1 when the\n","output is a target label and zero otherwise&#xA0;<a href=\"#ref-cao2015towards\">(Y. Cao and Yang 2015)</a>. In the\n","unlearning process, these queries are simply recomputed over the\n","remaining data. The approach is efficient as these statistical functions\n","are computationally efficient in the first place. Moreover, statistical\n","query learning also supports adaptive statistical queries, which are\n","computed based on the prior state of the learning models, including\n","k-means, SVM, and gradient descent. Although this time, the unlearning\n","update makes the model not convergent any more, only a few learning\n","iterations (adaptive statistical queries) are needed since the model\n","starts from an almost-converged state. Moreover, if the old results of\n","the summations are cached, say, via dynamic programming, then the\n","speedup might be even higher.</p>\n","<p>The limitation of this approach is that it does not scale with\n","complex models such as deep neural networks. Indeed, in complex models,\n","the number of statistical queries could become exponentially large&#xA0;<a href=\"#ref-bourtoule2021machine\">(Bourtoule et al.\n","2021)</a>, making both the unlearning and relearning steps less\n","efficient.</p>\n","<p>In general, statistical query learning supports item removal and can\n","be partially applied to stream removal&#xA0;<a href=\"#ref-gupta2021adaptive\">(Gupta et al. 2021)</a> as well,\n","although the streaming updates to the summations could be unbounded. It\n","supports exact unlearning, but only partially when the statistical\n","queries are non-adaptive. It also partially supports zero-shot\n","unlearning, because only the statistics over the data need to be\n","accessed, not the individual training data items.</p>\n","<p><strong>Decremental Learning.</strong> Decremental learning\n","algorithms were originally designed to remove redundant samples and\n","reduce the training load on the processor for support vector machines\n","(SVM)&#xA0;<a href=\"#ref-chen2019novel cauwenberghs2000incremental tveit2003multicategory tveit2003incremental romero2007incremental duan2007decremental\">(Y.\n","Chen et al. 2019; Cauwenberghs et al. 2000; Tveit et al. 2003; Tveit,\n","Hetland, and Engum 2003; Romero, Barrio, and Belanche 2007; Duan et al.\n","2007)</a> and linear classification&#xA0;<a href=\"#ref-karasuyama2009multiple karasuyama2010multiple tsai2014incremental\">(Karasuyama\n","and Takeuchi 2009, 2010; Tsai, Lin, and Lin 2014)</a>. As such, they\n","focus on accuracy rather than the completeness of the machine\n","unlearning.</p>\n","<p>Ginart et al.&#xA0;<a href=\"#ref-ginart2019making\">(Ginart et al. 2019)</a> developed\n","decremental learning solutions for <span\n","class=\"math inline\"><em>k</em></span>-means clustering based on\n","quantization and data partition. The idea of quantization is to ensure\n","that small changes in the data do not change the model. Quantization\n","helps to avoid unnecessary unlearning so that accuracy is not\n","catastrophically degraded. However, it is only applicable when there are\n","few model parameters compared to the size of the dataset. The idea\n","behind the data partitioning is to restrict the data&#x2019;s influence on the\n","model parameters to only a few specific data partitions. This process\n","helps to pinpoint the effects of unlearning to a few data features. But,\n","again, the approach is only effective with a small number of features\n","compared to the size of the dataset. Notably, data privacy and data\n","deletion are not completely correlative&#xA0;<a href=\"#ref-ginart2019making\">(Ginart et al. 2019)</a>. Data privacy\n","does not have to ensure data deletion (e.g., differential privacy), and\n","data deletion does not have to ensure data privacy.</p>\n","<p><strong>Knowledge Adaptation.</strong> Knowledge adaptation\n","selectively removes to-be-forgotten data samples&#xA0;<a href=\"#ref-chundawat2022can\">(Chundawat et al. 2022a)</a>. In this\n","approach&#xA0;<a href=\"#ref-chundawat2022can\">(Chundawat\n","et al. 2022a)</a>, one trains two neural networks as teachers\n","(competent and incompetent) and one neural network as a student. The\n","competent teacher is trained on the complete dataset, while the\n","incompetent teacher is randomly initialised. The student is initialised\n","with the competent teacher&#x2019;s model parameters. The student is trained to\n","mimic both competent teacher and incompetent teacher by a loss function\n","with KL-divergence evaluation values between the student and each of the\n","two teachers. Notably, the competent teacher processes the retained data\n","and the incompetent teacher deals with the forgotten data.</p>\n","<p>Beyond Chundwat et al.&#xA0;<a href=\"#ref-chundawat2022can\">(Chundawat et al. 2022a)</a>, machine\n","learning models have been quickly and accurately adapted by\n","reconstructing the past gradients of knowledge-adaptation priors\n","in&#xA0;<a href=\"#ref-khan2021knowledge\">(Khan et al.\n","2021)</a>. Ideas similar to knowledge-adaptation priors were also\n","investigated in&#xA0;<a href=\"#ref-ginart2019making wu2020priu\">(Ginart et al. 2019; Y. Wu,\n","Tannen, and Davidson 2020)</a>. In general, knowledge adaptation is\n","applicable to a wide range of unlearning requests and scenarios.\n","However, it is difficult to provide a theoretical guarantee for this\n","approach.</p>\n","<p><strong>MCMC Unlearning (Parameter Sampling).</strong> Sampling-based\n","machine unlearning has also been suggested as a way to train a standard\n","machine learning model to forget data samples from the training\n","data&#xA0;<a href=\"#ref-nguyen2022markov\">(Q. P. Nguyen\n","et al. 2022)</a>. The idea is to sample the distribution of model\n","parameters using Markov chain Monte Carlo (MCMC). It is assumed that the\n","forgetting set is often significantly smaller than the training data\n","(otherwise retraining might be a better solution). Thus, the parameter\n","distribution <span\n","class=\"math inline\"><em>P</em><em>r</em>(<em>w</em><sub><em>r</em></sub>)</span>\n","of the retrained models should not differ much from that of the original\n","model <span class=\"math inline\"><em>P</em><em>r</em>(<em>w</em>)</span>.\n","In other words, the posterior density <span\n","class=\"math inline\"><em>P</em><em>r</em>(<em>w</em><sub><em>r</em></sub>|<em>D</em>)</span>\n","should be sufficient large for sampling&#xA0;<a href=\"#ref-nguyen2022markov\">(Q. P. Nguyen et al. 2022)</a>. More\n","precisely, the posterior distribution from the retrained parameters can\n","be defined as: <span\n","class=\"math display\"><em>P</em><em>r</em>(<em>w</em><sub><em>r</em></sub>|<em>D</em>)&#x2004;&#x2248;&#x2004;<em>P</em><em>r</em>(<em>w</em>|<em>D</em>)&#x2004;&#x221D;&#x2004;<em>P</em><em>r</em>(<em>D</em>|<em>w</em>)<em>P</em><em>r</em>(<em>w</em>)</span>\n","Here, the prior distribution <span\n","class=\"math inline\"><em>P</em><em>r</em>(<em>w</em>)</span> is often\n","available from the learning algorithm, which means the stochasticity of\n","learning via sampling can be estimated. The likelihood <span\n","class=\"math inline\"><em>P</em><em>r</em>(<em>D</em>|<em>w</em>)</span>\n","is the prediction output of the model itself, which is also available\n","after training. From <a href=\"#eq:mcmc_unlearning\"\n","data-reference-type=\"autoref\"\n","data-reference=\"eq:mcmc_unlearning\">[eq:mcmc_unlearning]</a>, we only\n","know that the density function of <span\n","class=\"math inline\"><em>P</em><em>r</em>(<em>w</em>|<em>D</em>)</span>\n","is proportional to a function <span\n","class=\"math inline\"><em>f</em>(<em>w</em>)&#x2004;=&#x2004;<em>P</em><em>r</em>(<em>D</em>|<em>w</em>)<em>P</em><em>r</em>(<em>w</em>)</span>,\n","which means <span\n","class=\"math inline\"><em>P</em><em>r</em>(<em>w</em>|<em>D</em>)</span>\n","cannot be directly sampled. This is where MCMC comes into play, as it\n","can still generate the next samples using a proposal density <span\n","class=\"math inline\"><em>g</em>(<em>w</em>&#x2032;|<em>w</em>)</span>&#xA0;<a href=\"#ref-nguyen2022markov\">(Q. P. Nguyen et al.\n","2022)</a>. However, <span\n","class=\"math inline\"><em>g</em>(<em>w</em>&#x2032;|<em>w</em>)</span> is assumed\n","to be a Gaussian distribution centered on the current sample (the\n","sampling process can be initialized with the original model).</p>\n","<p>As a result, a candidate set of model parameters <span\n","class=\"math inline\"><em>P</em><em>r</em>(<em>w</em><sub><em>r</em></sub>|<em>D</em>)</span>\n","is constructed from the sampling, and the unlearning output is\n","calculated by simply maximizing the posterior probability <span\n","class=\"math inline\"><em>P</em><em>r</em>(<em>w</em>|<em>D</em><sub><em>r</em></sub>)</span>,\n","i.e.: <span\n","class=\"math display\"><em>w</em><sub><em>r</em></sub>&#x2004;=&#x2004;arg&#x2006;max<sub><em>w</em></sub><em>P</em><em>r</em>(<em>w</em>|<em>D</em><sub><em>r</em></sub>)</span>\n","The benefit of such sampling-based unlearning is that no access to the\n","forgetting set is required.</p>\n","\n","| **Paper Title** | **Year** | **Author** | **Venue** | **Model** | **Code** |\n","| --------------- | :----: | ---- | :----: | :----: | :----: |\n","| [Towards Adversarial Evaluations for Inexact Machine Unlearning](https://arxiv.org/abs/2201.06640) | 2023 | Goel et al. | _arXiv_ | EU-k, CF-k | [[Code]](https://github.com/shash42/Evaluating-Inexact-Unlearning) |\n","| [On the Trade-Off between Actionable Explanations and the Right to be Forgotten](https://openreview.net/pdf?id=HWt4BBZjVW) | 2023 | Pawelczyk et al. | _arXiv_ | - | - |  |\n","| [Towards Unbounded Machine Unlearning](https://arxiv.org/pdf/2302.09880) | 2023 | Kurmanji et al. | _arXiv_ | SCRUB | [[Code]](https://github.com/Meghdad92/SCRUB) | approximate unlearning |\n","| [Netflix and Forget: Efficient and Exact Machine Unlearning from Bi-linear Recommendations](https://arxiv.org/abs/2302.06676) | 2023 | Xu et al. | _arXiv_ | Unlearn-ALS | - | Exact Unlearning |\n","| [To Be Forgotten or To Be Fair: Unveiling Fairness Implications of Machine Unlearning Methods](https://arxiv.org/abs/2302.03350) | 2023 | Zhang et al. | _arXiv_ | - | [[Code]](https://github.com/cleverhans-lab/machine-unlearning) | |\n","| [Sequential Informed Federated Unlearning: Efficient and Provable Client Unlearning in Federated Optimization](https://arxiv.org/abs/2211.11656) | 2022 | Fraboni et al. | _arXiv_ | SIFU | - | |\n","| [Certified Data Removal in Sum-Product Networks](https://arxiv.org/abs/2210.01451) | 2022 | Becker and Liebig | _ICKG_ | UNLEARNSPN | [[Code]](https://github.com/ROYALBEFF/UnlearnSPN) | Certified Removal Mechanisms |\n","| [Learning with Recoverable Forgetting](https://arxiv.org/abs/2207.08224) | 2022 | Ye et al.  | _ECCV_ | LIRF | - |  |\n","| [Continual Learning and Private Unlearning](https://arxiv.org/abs/2203.12817) | 2022 | Liu et al. | _CoLLAs_ | CLPU | [[Code]](https://github.com/Cranial-XIX/Continual-Learning-Private-Unlearning) | |\n","| [Verifiable and Provably Secure Machine Unlearning](https://arxiv.org/abs/2210.09126) | 2022 | Eisenhofer et al. | _arXiv_ | - | [[Code]](https://github.com/cleverhans-lab/verifiable-unlearning) |  Certified Removal Mechanisms |\n","| [VeriFi: Towards Verifiable Federated Unlearning](https://arxiv.org/abs/2205.12709) | 2022 | Gao et al. | _arXiv_ | VERIFI | - | Certified Removal Mechanisms |\n","| [FedRecover: Recovering from Poisoning Attacks in Federated Learning using Historical Information](https://arxiv.org/abs/2210.10936) | 2022 | Cao et al. | _S&P_ | FedRecover | - | recovery method |\n","| [Fast Yet Effective Machine Unlearning](https://arxiv.org/abs/2111.08947) | 2022 | Tarun et al. | _arXiv_ | UNSIR | - |  |\n","| [Membership Inference via Backdooring](https://arxiv.org/abs/2206.04823) | 2022 | Hu et al.  | _IJCAI_ | MIB | [[Code]](https://github.com/HongshengHu/membership-inference-via-backdooring) | Membership Inferencing |\n","| [Forget Unlearning: Towards True Data-Deletion in Machine Learning](https://arxiv.org/abs/2210.08911) | 2022 | Chourasia et al. | _ICLR_ | - | - | noisy gradient descent |\n","| [Zero-Shot Machine Unlearning](https://arxiv.org/abs/2201.05629) | 2022 | Chundawat et al. | _arXiv_ | - | - |  |\n","| [Efficient Attribute Unlearning: Towards Selective Removal of Input Attributes from Feature Representations](https://arxiv.org/abs/2202.13295) | 2022 | Guo et al. | _arXiv_ | attribute unlearning | - |  |\n","| [Few-Shot Unlearning](https://download.huan-zhang.com/events/srml2022/accepted/yoon22fewshot.pdf) | 2022 | Yoon et al.   | _ICLR_ | - | - |  |\n","| [Federated Unlearning: How to Efficiently Erase a Client in FL?](https://arxiv.org/abs/2207.05521) | 2022 | Halimi et al. | _UpML Workshop_ | - | - | federated learning |\n","| [Machine Unlearning Method Based On Projection Residual](https://arxiv.org/abs/2209.15276) | 2022 | Cao et al. | _DSAA_ | - | - |  Projection Residual Method |\n","| [Hard to Forget: Poisoning Attacks on Certified Machine Unlearning](https://ojs.aaai.org/index.php/AAAI/article/view/20736) | 2022 | Marchant et al. | _AAAI_ | - | [[Code]](https://github.com/ngmarchant/attack-unlearning) | Certified Removal Mechanisms |\n","| [Athena: Probabilistic Verification of Machine Unlearning](https://web.archive.org/web/20220721061150id_/https://petsymposium.org/popets/2022/popets-2022-0072.pdf) | 2022 | Sommer et al. | _PoPETs_ | ATHENA | - | |\n","| [FP2-MIA: A Membership Inference Attack Free of Posterior Probability in Machine Unlearning](https://link.springer.com/chapter/10.1007/978-3-031-20917-8_12) | 2022 | Lu et al. | _ProvSec_ | FP2-MIA | - | inference attack |\n","| [Deletion Inference, Reconstruction, and Compliance in Machine (Un)Learning](https://arxiv.org/abs/2202.03460) | 2022 | Gao et al. | _PETS_ | - | - |  |\n","| [Prompt Certified Machine Unlearning with Randomized Gradient Smoothing and Quantization](https://openreview.net/pdf?id=ue4gP8ZKiWb) | 2022 | Zhang et al.   | _NeurIPS_ | PCMU | - | Certified Removal Mechanisms |\n","| [The Right to be Forgotten in Federated Learning: An Efficient Realization with Rapid Retraining](https://arxiv.org/abs/2203.07320) | 2022 | Liu et al. | _INFOCOM_ | - | [[Code]](https://github.com/yiliucs/federated-unlearning) |  |\n","| [Backdoor Defense with Machine Unlearning](https://arxiv.org/abs/2201.09538) | 2022 | Liu et al. | _INFOCOM_ | BAERASER | - | Backdoor defense |\n","| [Markov Chain Monte Carlo-Based Machine Unlearning: Unlearning What Needs to be Forgotten](https://dl.acm.org/doi/abs/10.1145/3488932.3517406) | 2022 | Nguyen et al. | _ASIA CCS_ | MCU | - | MCMC Unlearning  |\n","| [Federated Unlearning for On-Device Recommendation](https://arxiv.org/abs/2210.10958) | 2022 | Yuan et al. | _arXiv_ | - | - |  |\n","| [Can Bad Teaching Induce Forgetting? Unlearning in Deep Networks using an Incompetent Teacher](https://arxiv.org/abs/2205.08096) | 2022 | Chundawat et al. | _arXiv_ | - | - | Knowledge Adaptation |\n","| [ Efficient Two-Stage Model Retraining for Machine Unlearning](https://openaccess.thecvf.com/content/CVPR2022W/HCIS/html/Kim_Efficient_Two-Stage_Model_Retraining_for_Machine_Unlearning_CVPRW_2022_paper.html) | 2022 | Kim and Woo | _CVPR Workshop_ | - | - |  |\n","| [Learn to Forget: Machine Unlearning Via Neuron Masking](https://ieeexplore.ieee.org/abstract/document/9844865?casa_token=_eowH3BTt1sAAAAA:X0uCpLxOwcFRNJHoo3AtA0ay4t075_cSptgTMznsjusnvgySq-rJe8GC285YhWG4Q0fUmP9Sodw0) | 2021 | Ma et al. | _IEEE_ | Forsaken | - | Mask Gradients |\n","| [Adaptive Machine Unlearning](https://proceedings.neurips.cc/paper/2021/hash/87f7ee4fdb57bdfd52179947211b7ebb-Abstract.html) | 2021 | Gupta et al. | _NeurIPS_ | - | [[Code]](https://github.com/ChrisWaites/adaptive-machine-unlearning) | Differential Privacy |\n","| [Descent-to-Delete: Gradient-Based Methods for Machine Unlearning](https://proceedings.mlr.press/v132/neel21a.html) | 2021 | Neel et al. | _ALT_ | - | - | Certified Removal Mechanisms |\n","| [Remember What You Want to Forget: Algorithms for Machine Unlearning](https://arxiv.org/abs/2103.03279) | 2021 | Sekhari et al. | _NeurIPS_ | - | - |  |\n","| [FedEraser: Enabling Efficient Client-Level Data Removal from Federated Learning Models](https://ieeexplore.ieee.org/abstract/document/9521274) | 2021 | Liu et al. | _IWQoS_ | FedEraser | - |  |\n","| [Federated Unlearning](https://arxiv.org/abs/2012.13891) | 2021 | Liu et al. | _IWQoS_ | FedEraser | [[Code]](https://www.dropbox.com/s/1lhx962axovbbom/FedEraser-Code.zip?dl=0) |  |\n","| [Machine Unlearning via Algorithmic Stability](https://proceedings.mlr.press/v134/ullah21a.html) | 2021 | Ullah et al. | _COLT_ | TV | - | Certified Removal Mechanisms |\n","| [EMA: Auditing Data Removal from Trained Models](https://link.springer.com/chapter/10.1007/978-3-030-87240-3_76) | 2021 | Huang et al. | _MICCAI_ | EMA | [[Code]](https://github.com/Hazelsuko07/EMA) | Certified Removal Mechanisms |\n","| [Knowledge-Adaptation Priors](https://proceedings.neurips.cc/paper/2021/hash/a4380923dd651c195b1631af7c829187-Abstract.html) | 2021 | Khan and Swaroop | _NeurIPS_ | K-prior | [[Code]](https://github.com/team-approx-bayes/kpriors) | Knowledge Adaptation |\n","| [PrIU: A Provenance-Based Approach for Incrementally Updating Regression Models](https://dl.acm.org/doi/abs/10.1145/3318464.3380571) | 2020 | Wu et al. | _NeurIPS_ | PrIU | - | Knowledge Adaptation |\n","| [Eternal Sunshine of the Spotless Net: Selective Forgetting in Deep Networks](https://arxiv.org/abs/1911.04933) | 2020 | Golatkar et al. | _CVPR_ | - | - | Certified Removal Mechanisms |\n","| [Learn to Forget: User-Level Memorization Elimination in Federated Learning](https://www.researchgate.net/profile/Ximeng-Liu-5/publication/340134612_Learn_to_Forget_User-Level_Memorization_Elimination_in_Federated_Learning/links/5e849e64a6fdcca789e5f955/Learn-to-Forget-User-Level-Memorization-Elimination-in-Federated-Learning.pdf) | 2020 | Liu et al. | _arXiv_ | Forsaken | - |  |\n","| [Certified Data Removal from Machine Learning Models](https://proceedings.mlr.press/v119/guo20c.html) | 2020 | Guo et al. | _ICML_ | - | - | Certified Removal Mechanisms |\n","| [Class Clown: Data Redaction in Machine Unlearning at Enterprise Scale](https://arxiv.org/abs/2012.04699) | 2020 | Felps et al. | _arXiv_ | - | - | Decremental Learning |\n","| [A Novel Online Incremental and Decremental Learning Algorithm Based on Variable Support Vector Machine](https://link.springer.com/article/10.1007/s10586-018-1772-4) | 2019 | Chen et al. | _Cluster Computing_ | - | - | Decremental Learning  |\n","| [Making AI Forget You: Data Deletion in Machine Learning](https://papers.nips.cc/paper/2019/hash/cb79f8fa58b91d3af6c9c991f63962d3-Abstract.html) | 2019 | Ginart et al. | _NeurIPS_ | - | - | Decremental Learning  |\n","| [Lifelong Anomaly Detection Through Unlearning](https://dl.acm.org/doi/abs/10.1145/3319535.3363226) | 2019 | Du et al. | _CCS_ | - | - |  |\n","| [Learning Not to Learn: Training Deep Neural Networks With Biased Data](https://openaccess.thecvf.com/content_CVPR_2019/html/Kim_Learning_Not_to_Learn_Training_Deep_Neural_Networks_With_Biased_CVPR_2019_paper.html) | 2019 | Kim et al. | _CVPR_ | - | - |  |\n","| [Efficient Repair of Polluted Machine Learning Systems via Causal Unlearning](https://dl.acm.org/citation.cfm?id=3196517) | 2018 | Cao et al. | _ASIACCS_ | KARMA | [[Code]](https://github.com/CausalUnlearning/KARMA) |  |\n","| [Understanding Black-box Predictions via Influence Functions](https://proceedings.mlr.press/v70/koh17a.html) | 2017 | Koh et al. | _ICML_ | - | [[Code]](https://github.com/kohpangwei/influence-release) | Certified Removal Mechanisms |\n","| [Towards Making Systems Forget with Machine Unlearning](https://ieeexplore.ieee.org/abstract/document/7163042) | 2015 | Cao and Yang | _S&P_ | - |  |\n","| [Towards Making Systems Forget with Machine Unlearning](https://dl.acm.org/doi/10.1109/SP.2015.35) | 2015 | Cao et al. | _S&P_ | - | - | Statistical Query Learning  |\n","| [Incremental and decremental training for linear classification](https://dl.acm.org/doi/10.1145/2623330.2623661) | 2014 | Tsai et al. | _KDD_ | - | [[Code]](https://www.csie.ntu.edu.tw/~cjlin/papers/ws/) | Decremental Learning  |\n","| [Multiple Incremental Decremental Learning of Support Vector Machines](https://dl.acm.org/doi/10.5555/2984093.2984196) | 2009 | Karasuyama et al. | _NIPS_ | - | - | Decremental Learning  |\n","| [Incremental and Decremental Learning for Linear Support Vector Machines](https://dl.acm.org/doi/10.5555/1776814.1776838) | 2007 | Romero et al. | _ICANN_ | - | - | Decremental Learning  |\n","| [Decremental Learning Algorithms for Nonlinear Langrangian and Least Squares Support Vector Machines](https://www.semanticscholar.org/paper/Decremental-Learning-Algorithms-for-Nonlinear-and-Duan-Li/312c677f0882d0dfd60bfd77346588f52aefd10f) | 2007 | Duan et al. | _OSB_ | - | - | Decremental Learning  |\n","| [Multicategory Incremental Proximal Support Vector Classifiers](https://link.springer.com/chapter/10.1007/978-3-540-45224-9_54) | 2003 | Tveit et al. | _KES_ | - | - | Decremental Learning  |\n","| [Incremental and Decremental Proximal Support Vector Classification using Decay Coefficients](https://link.springer.com/chapter/10.1007/978-3-540-45228-7_42) | 2003 | Tveit et al. | _DaWak_ | - | - | Decremental Learning  |\n","| [Incremental and Decremental Support Vector Machine Learning](https://dl.acm.org/doi/10.5555/3008751.3008808) | 2000 | Cauwenberg et al. | _NeurIPS_ | - | - | Decremental Learning  |\n","----------"]},{"cell_type":"markdown","id":"a778b7e3","metadata":{"papermill":{"duration":0.006988,"end_time":"2023-07-06T00:16:39.603355","exception":false,"start_time":"2023-07-06T00:16:39.596367","status":"completed"},"tags":[]},"source":["<h2 id=\"model-intrinsic-approaches\">4.2. Model-Intrinsic Approaches</h2>\n","\n","<p>The model-intrinsic approaches include unlearning methods designed\n","for a specific type of models. Although they are model-intrinsic, their\n","applications are not necessarily narrow, as many machine learning models\n","can share the same type.</p>\n","\n","\n","<div class=\"figure*\">\n","<figure>\n","<img src=\"https://raw.githubusercontent.com/tamlhp/awesome-machine-unlearning/main/figs/model-intrinsic.png\" alt=\"https://arxiv.org/abs/2209.02299\" style=\"max-width: 70%;\"/>\n","</figure>\n","</div>\n","\n","\n","<p><strong>Unlearning for softmax classifiers (logit-based\n","classifiers).</strong> Softmax (or logit-based) classifiers are\n","classification models <span\n","class=\"math inline\"><em>M</em>&#x2004;:&#x2004;&#x1D4B5;&#x2004;&#x2192;&#x2004;&#x211D;<sup><em>K</em></sup></span> that\n","output a vector of logits <span\n","class=\"math inline\"><em>l</em>&#x2004;&#x2208;&#x2004;&#x211D;<sup><em>k</em></sup></span>, where\n","<span class=\"math inline\"><em>K</em></span> is the number of classes,\n","for each data sample <span class=\"math inline\"><em>x</em>&#x2004;&#x2208;&#x2004;&#x1D4B5;</span>.\n","The core task of <span class=\"math inline\"><em>M</em>(<em>x</em>)</span>\n","is to estimate the probability distribution <span\n","class=\"math inline\"><em>P</em><em>r</em>(<em>X</em>,<em>Y</em>)</span>,\n","where <span class=\"math inline\"><em>X</em></span> is the random variable\n","in <span class=\"math inline\">&#x1D4B3;</span>, and <span\n","class=\"math inline\"><em>Y</em></span> is the random variable in <span\n","class=\"math inline\">1,&#x2006;&#x2026;,&#x2006;<em>K</em></span>, such that: <span\n","class=\"math display\"><em>P</em><em>r</em>(<em>Y</em>=<em>i</em>|<em>X</em>=<em>x</em>)&#x2004;&#x2248;&#x2004;<em>&#x3C3;</em>(<em>l</em><sub><em>i</em></sub>)</span>\n","Here, <span class=\"math inline\">$\\sigma(l_i) =\n","\\frac{\\exp(l_i)}{\\sum_{j=1..K} \\exp l_j}$</span> is the softmax\n","function. This formulation applies to logistic regression and deep\n","neural networks with a densely connected output layer using softmax\n","activations&#xA0;<a href=\"#ref-baumhauer2020machine\">(Baumhauer, Sch&#xF6;ttle, and Zeppelzauer\n","2020)</a>. Baumhauer et al.&#xA0;<a href=\"#ref-baumhauer2020machine\">(Baumhauer, Sch&#xF6;ttle, and Zeppelzauer\n","2020)</a> proposed an unlearning method for softmax classifiers based\n","on a linear filtration operator to proportionally shift the\n","classification of the to-be-forgetten class samples to other classes.\n","However, this approach is only works for class removal.</p>\n","<p><strong>Unlearning for linear models.</strong> Izzo et al.&#xA0;<a href=\"#ref-izzo2021approximate\">(Izzo et al.\n","2021)</a> proposed an approximate unlearning method for linear and\n","logistic models based on influence functions. They approximated a\n","Hessian matrix computation with a project residual update&#xA0;<a href=\"#ref-izzo2021approximate cao2022machine\">(Izzo\n","et al. 2021; Z. Cao et al. 2022)</a> that combines gradient methods\n","with synthetic data. It is suitable for forgetting small groups of\n","points out of a learned model. Some other studies consider an online\n","setting for machine unlearning (aka online data deletion)&#xA0;<a href=\"#ref-ginart2019making li2020online\">(Ginart et\n","al. 2019; Li, Wang, and Cheng 2021)</a>, in which the removal request\n","is a sequence of entries that indicates which data item is to be\n","unlearned. In general, this setting is more challenging than normal\n","setting because indistinguishability must hold for any entry and for the\n","end of the deletion sequence. The goal is to achieve a lower bound on\n","amortized computation time&#xA0;<a href=\"#ref-ginart2019making li2020online\">(Ginart et al. 2019; Li,\n","Wang, and Cheng 2021)</a>.</p>\n","<p>Li et al.&#xA0;<a href=\"#ref-li2020online\">(Li, Wang,\n","and Cheng 2021)</a> formulated a special case of the online setting\n","where data is only accessible for a limited time so there is no full\n","training process in the first place. More precisely, the system is\n","allowed a constant memory to store historical data or a data sketch, and\n","it has to make predictions within a bounded period of time. Although the\n","data to be forgotten can be unlearned from a model on-the-fly using a\n","regret scheme on the memory, this particular unlearning process is only\n","applicable to ordinary linear regression&#xA0;<a href=\"#ref-li2020online\">(Li, Wang, and Cheng 2021)</a>.</p>\n","<p><strong>Unlearning for Tree-based Models.</strong> Tree-based models\n","are classification techniques that partition the feature space\n","recursively, where the features and cut-off thresholds to split the data\n","are determined by some criterion, such as information gain&#xA0;<a href=\"#ref-schelter2021hedgecut\">(Schelter,\n","Grafberger, and Dunning 2021)</a>. There is a class of tree-based\n","models, called extremely randomized trees&#xA0;<a href=\"#ref-geurts2006extremely\">(Geurts, Ernst, et al. 2006)</a>,\n","that are built by an ensemble of decision trees. These are very\n","efficient because the candidate set of split features and cut-off\n","thresholds are randomly generated. The best candidate is selected by a\n","reduction in Gini impurity, which avoids the heavy computation of\n","logarithms.</p>\n","<p>Schelter et al.&#xA0;<a href=\"#ref-schelter2021hedgecut\">(Schelter, Grafberger, and Dunning\n","2021)</a> proposed an unlearning solution for extremely randomized\n","trees by measuring the robustness of the split decisions. A split\n","decision is robust if removing <span\n","class=\"math inline\"><em>k</em></span> data items does not reverse that\n","split. Note that <span class=\"math inline\"><em>k</em></span> can be\n","bounded, and it is often small as only one in ten-thousand users who\n","wants to remove their data at a time&#xA0;<a href=\"#ref-schelter2021hedgecut\">(Schelter, Grafberger, and Dunning\n","2021)</a>). The learning algorithm is redesigned such that most of\n","splits, especially the high-level ones, are robust. For the non-robust\n","splits, all subtree variants are grown from all split candidates and\n","maintained until a removal request would revise that split. When that\n","happens, the split is switched to its variant with higher Gini gain. As\n","a result, the unlearning process involves recalculating the Gini gains\n","and updating the splits if necessary.</p>\n","<p>One limitation of this approach is that if the set to be forgotten is\n","too large, there might be many non-robust splits. This would lead to\n","high storage costs for the subtree variants. However, it does give a\n","parameterized choice between unlearning and retraining. If there are\n","many removal requests, retraining might be the best asymptotically.\n","Alternatively, one might limit the maximum number of removal requests to\n","be processed at a time. Moreover, tree-based models have a highly\n","competitive performance for many predictive applications&#xA0;<a href=\"#ref-schelter2021hedgecut\">(Schelter,\n","Grafberger, and Dunning 2021)</a>.</p>\n","<p><strong>Unlearning for Bayesian Models.</strong> Bayesian models are\n","probabilistic models that approximate a posterior likelihood&#xA0;<a href=\"#ref-fu2022knowledge fu2021bayesian jose2021unified nguyen2020variational\">(Fu,\n","He, et al. 2022; Fu et al. 2021; Jose and Simeone 2021; Q. P. Nguyen,\n","Low, and Jaillet 2020)</a>. Also known as Bayesian inference, this\n","process is particularly useful when a loss function is not well-defined\n","or does not even exist. Bayesian models cover a wide range of machine\n","learning algorithms, such as Bayesian neural networks, probabilistic\n","graphical models, generative models, topic modeling, and probabilistic\n","matrix factorization&#xA0;<a href=\"#ref-zhang2020deep roth2018bayesian pearce2020uncertainty\">(H.\n","Zhang et al. 2020; Roth and Pernkopf 2018; Pearce, Leibfried, and\n","Brintrup 2020)</a>.</p>\n","<p>Unlearning for Bayesian models requires a special treatment, as the\n","training already involves optimizing the posterior distribution of the\n","model&#x2019;s parameters. It also often involves optimizing the\n","Kullback-Leibler divergence between a prior belief and the posterior\n","distribution&#xA0;<a href=\"#ref-nguyen2020variational\">(Q. P. Nguyen, Low, and Jaillet\n","2020)</a>. Nguyen et al.&#xA0;<a href=\"#ref-nguyen2020variational\">(Q. P. Nguyen, Low, and Jaillet\n","2020)</a> proposed the notion of <em>exact Bayesian learning</em>:\n","<span\n","class=\"math display\"><em>P</em><em>r</em>(<em>w</em>|<em>D</em><sub><em>r</em></sub>)&#x2004;=&#x2004;<em>P</em><em>r</em>(<em>w</em>|<em>D</em>)<em>P</em><em>r</em>(<em>D</em><sub><em>f</em></sub>|<em>D</em><sub><em>r</em></sub>)/<em>P</em><em>r</em>(<em>D</em><sub><em>f</em></sub>|<em>w</em>)&#x2004;&#x221D;&#x2004;<em>P</em><em>r</em>(<em>w</em>|<em>D</em>)/<em>P</em><em>r</em>(<em>D</em><sub><em>f</em></sub>|<em>w</em>)</span>\n","where <span\n","class=\"math inline\"><em>P</em><em>r</em>(<em>w</em>|<em>D</em><sub><em>r</em></sub>)</span>\n","is the distribution of a retrained model (as if it were trained only on\n","<span class=\"math inline\"><em>D</em><sub><em>r</em></sub></span>).\n","However, the posterior distribution <span\n","class=\"math inline\"><em>P</em><em>r</em>(<em>w</em>|<em>D</em><sub><em>r</em></sub>)</span>\n","can only be sampled directly when the model parameters are\n","discrete-valued (quantized) or the prior is conjugate&#xA0;<a href=\"#ref-nguyen2020variational\">(Q. P. Nguyen, Low,\n","and Jaillet 2020)</a>. For non-conjugate priors, Nguyen et al.&#xA0;<a href=\"#ref-nguyen2020variational\">(Q. P. Nguyen, Low,\n","and Jaillet 2020)</a> proved that we can approximate <span\n","class=\"math inline\"><em>P</em><em>r</em>(<em>w</em>|<em>D</em><sub><em>r</em></sub>)</span>\n","by minimizing the KL divergence between <span\n","class=\"math inline\"><em>P</em><em>r</em>(<em>w</em>|<em>D</em>)</span>\n","and <span\n","class=\"math inline\"><em>P</em><em>r</em>(<em>w</em>|<em>D</em><sub><em>r</em></sub>)</span>.\n","Since <span\n","class=\"math inline\"><em>P</em><em>r</em>(<em>w</em>|<em>D</em>)</span>\n","is the original model&#x2019;s parameter distribution, this approximation\n","prevents catastrophic unlearning. As such, the retained model performs\n","significantly better than the unlearned model in terms of accuracy.</p>\n","<p>A notion of certified Bayesian unlearning has also been studied,\n","where the KL divergence between the unlearned model and the retrained\n","model is bounded&#xA0;<a href=\"#ref-fu2022knowledge fu2021bayesian jose2021unified\">(Fu, He, et\n","al. 2022; Fu et al. 2021; Jose and Simeone 2021)</a>: <span\n","class=\"math display\"><em>K</em><em>L</em>(<em>P</em><em>r</em>(<em>A</em>(<em>D</em><sub><em>r</em></sub>)),&#x1D53C;<sub><em>A</em>(<em>D</em>)</sub><em>P</em><em>r</em>(<em>U</em>(<em>D</em>,<em>D</em><sub><em>f</em></sub>,<em>A</em>(<em>D</em>))))&#x2004;&#x2264;&#x2004;<em>&#x3F5;</em></span>\n","Here, the result of the unlearning process is an expectation over the\n","parameter distribution of the original model <span\n","class=\"math inline\"><em>A</em>(<em>D</em>)&#x2004;&#x223C;&#x2004;<em>P</em><em>r</em>(<em>w</em>|<em>D</em>)</span>.\n","This certification can be achieved for some energy functions when\n","formulating the evidence lower bound (ELBO) in Bayesian models&#xA0;<a href=\"#ref-fu2022knowledge fu2021bayesian jose2021unified\">(Fu, He, et\n","al. 2022; Fu et al. 2021; Jose and Simeone 2021)</a>.</p>\n","<p><strong>Unlearning for DNN-based Models.</strong> Deep neural\n","networks are advanced models that automatically learn features from\n","data. As a result, it is very difficult to pinpoint the exact model\n","update for each data item&#xA0;<a href=\"#ref-golatkar2020forgetting golatkar2020eternal mehta2022deep he2021deepobliviate goyal2021revisiting\">(Golatkar,\n","Achille, and Soatto 2020b, 2020a; Mehta et al. 2022; He et al. 2021;\n","Goyal, Hassija, and Albuquerque 2021)</a>. Fortunately, deep neural\n","networks consist of multiple layers. For layers with convex activation\n","functions, existing unlearning methods such as certified removal\n","mechanisms can be applied&#xA0;<a href=\"#ref-GuoGHM20 neel2021descent sekhari2021remember cao2022machine\">(C.\n","Guo et al. 2020; Neel, Roth, and Sharifi-Malvajerdi 2021; Sekhari et al.\n","2021; Z. Cao et al. 2022)</a>. For non-convex layers, Golatkar et\n","al.&#xA0;<a href=\"#ref-golatkar2021mixed golatkar2020forgetting\">(Golatkar et al.\n","2021; Golatkar, Achille, and Soatto 2020b)</a> proposed a caching\n","approach that trains the model on data that are known a priori to be\n","permanent. Then the model is fine-tuned on user data using some convex\n","optimization.</p>\n","<p>Sophisticated unlearning methods for DNNs rely primarily on influence\n","functions&#xA0;<a href=\"#ref-koh2017understanding zhang2022machine\">(Koh et al. 2017;\n","P.-F. Zhang et al. 2022)</a>. Here, Taylor expansions are used to\n","approximate the impact of a data item on the parameters of black-box\n","models&#xA0;<a href=\"#ref-zeng2021learning\">(Zeng et al.\n","2021)</a>. Some variants include DeltaGrad&#xA0;<a href=\"#ref-wu2020deltagrad\">(Y. Wu et al. 2020)</a>, which stores\n","the historical updates for each data item, and Fisher-based\n","unlearning&#xA0;<a href=\"#ref-golatkar2020eternal\">(Golatkar, Achille, and Soatto\n","2020a)</a>, which we discussed under <a href=\"#sec:model-agnostic\"\n","data-reference-type=\"autoref\"\n","data-reference=\"sec:model-agnostic\">[sec:model-agnostic]</a>). However,\n","influence functions in deep neural networks are not stable with a large\n","forget set&#xA0;<a href=\"#ref-basu2021influence mahadevan2021certifiable mahadevan2022certifiable\">(Basu,\n","Pope, and Feizi 2021; Mahadevan and Mathioudakis 2021, 2022)</a>.</p>\n","<p>More precisely, after the data to be forgotten has been deleted from\n","database, Fisher-based unlearning&#xA0;<a href=\"#ref-golatkar2020eternal\">(Golatkar, Achille, and Soatto\n","2020a)</a> works on the remaining training data with the Newton&#x2019;s\n","method, which uses a second-order gradient. To mitigate potential\n","information leaks, noise is injected into the model&#x2019;s parameters&#xA0;<a href=\"#ref-conggrapheditor\">(Cong and Mahdavi\n","2022a)</a>. As the Fisher-based method aims to approximate the model\n","without the deleted data, there can be no guarantee that all the\n","influence of the deleted data has been removed. Although injecting noise\n","can help mitigate information leaks, the model&#x2019;s performance may be\n","affected by the noise&#xA0;<a href=\"#ref-conggrapheditor\">(Cong and Mahdavi 2022a)</a>.</p>\n","<p>Golatkar et al.&#xA0;<a href=\"#ref-golatkar2020eternal\">(Golatkar, Achille, and Soatto\n","2020a)</a> point out that the Hessian computation in certified\n","removal mechanisms is too expensive for complex models like deep neural\n","networks. Hence, they resorted to an approximation of Hessian via\n","Levenberg-Marquardt semi-positive-definite approximation, which turns\n","out to correspond with the Fisher Information Matrix&#xA0;<a href=\"#ref-martens2020new\">(Martens 2020)</a>.\n","Although it does not provide a concrete theoretical guarantee,\n","Fisher-based unlearning could lead to further information-theoretic\n","approaches to machine unlearning&#xA0;<a href=\"#ref-guo2022efficient golatkar2020forgetting\">(T. Guo et al.\n","2022; Golatkar, Achille, and Soatto 2020b)</a>.</p>\n","\n","| **Paper Title** | **Year** | **Author** | **Venue** | **Model** | **Code** |\n","| --------------- | :----: | ---- | :----: | :----: | :----: |\n","| [Towards Adversarial Evaluations for Inexact Machine Unlearning](https://arxiv.org/abs/2201.06640) | 2023 | Goel et al. | _arXiv_ | EU-k, CF-k | [[Code]](https://github.com/shash42/Evaluating-Inexact-Unlearning) |\n","| [On the Trade-Off between Actionable Explanations and the Right to be Forgotten](https://openreview.net/pdf?id=HWt4BBZjVW) | 2023 | Pawelczyk et al. | _arXiv_ | - | - |  |\n","| [Towards Unbounded Machine Unlearning](https://arxiv.org/pdf/2302.09880) | 2023 | Kurmanji et al. | _arXiv_ | SCRUB | [[Code]](https://github.com/Meghdad92/SCRUB) | approximate unlearning |\n","| [Netflix and Forget: Efficient and Exact Machine Unlearning from Bi-linear Recommendations](https://arxiv.org/abs/2302.06676) | 2023 | Xu et al. | _arXiv_ | Unlearn-ALS | - | Exact Unlearning |\n","| [To Be Forgotten or To Be Fair: Unveiling Fairness Implications of Machine Unlearning Methods](https://arxiv.org/abs/2302.03350) | 2023 | Zhang et al. | _arXiv_ | - | [[Code]](https://github.com/cleverhans-lab/machine-unlearning) | |\n","| [Sequential Informed Federated Unlearning: Efficient and Provable Client Unlearning in Federated Optimization](https://arxiv.org/abs/2211.11656) | 2022 | Fraboni et al. | _arXiv_ | SIFU | - | |\n","| [Certified Data Removal in Sum-Product Networks](https://arxiv.org/abs/2210.01451) | 2022 | Becker and Liebig | _ICKG_ | UNLEARNSPN | [[Code]](https://github.com/ROYALBEFF/UnlearnSPN) | Certified Removal Mechanisms |\n","| [Learning with Recoverable Forgetting](https://arxiv.org/abs/2207.08224) | 2022 | Ye et al.  | _ECCV_ | LIRF | - |  |\n","| [Continual Learning and Private Unlearning](https://arxiv.org/abs/2203.12817) | 2022 | Liu et al. | _CoLLAs_ | CLPU | [[Code]](https://github.com/Cranial-XIX/Continual-Learning-Private-Unlearning) | |\n","| [Verifiable and Provably Secure Machine Unlearning](https://arxiv.org/abs/2210.09126) | 2022 | Eisenhofer et al. | _arXiv_ | - | [[Code]](https://github.com/cleverhans-lab/verifiable-unlearning) |  Certified Removal Mechanisms |\n","| [VeriFi: Towards Verifiable Federated Unlearning](https://arxiv.org/abs/2205.12709) | 2022 | Gao et al. | _arXiv_ | VERIFI | - | Certified Removal Mechanisms |\n","| [FedRecover: Recovering from Poisoning Attacks in Federated Learning using Historical Information](https://arxiv.org/abs/2210.10936) | 2022 | Cao et al. | _S&P_ | FedRecover | - | recovery method |\n","| [Fast Yet Effective Machine Unlearning](https://arxiv.org/abs/2111.08947) | 2022 | Tarun et al. | _arXiv_ | UNSIR | - |  |\n","| [Membership Inference via Backdooring](https://arxiv.org/abs/2206.04823) | 2022 | Hu et al.  | _IJCAI_ | MIB | [[Code]](https://github.com/HongshengHu/membership-inference-via-backdooring) | Membership Inferencing |\n","| [Forget Unlearning: Towards True Data-Deletion in Machine Learning](https://arxiv.org/abs/2210.08911) | 2022 | Chourasia et al. | _ICLR_ | - | - | noisy gradient descent |\n","| [Zero-Shot Machine Unlearning](https://arxiv.org/abs/2201.05629) | 2022 | Chundawat et al. | _arXiv_ | - | - |  |\n","| [Efficient Attribute Unlearning: Towards Selective Removal of Input Attributes from Feature Representations](https://arxiv.org/abs/2202.13295) | 2022 | Guo et al. | _arXiv_ | attribute unlearning | - |  |\n","| [Few-Shot Unlearning](https://download.huan-zhang.com/events/srml2022/accepted/yoon22fewshot.pdf) | 2022 | Yoon et al.   | _ICLR_ | - | - |  |\n","| [Federated Unlearning: How to Efficiently Erase a Client in FL?](https://arxiv.org/abs/2207.05521) | 2022 | Halimi et al. | _UpML Workshop_ | - | - | federated learning |\n","| [Machine Unlearning Method Based On Projection Residual](https://arxiv.org/abs/2209.15276) | 2022 | Cao et al. | _DSAA_ | - | - |  Projection Residual Method |\n","| [Hard to Forget: Poisoning Attacks on Certified Machine Unlearning](https://ojs.aaai.org/index.php/AAAI/article/view/20736) | 2022 | Marchant et al. | _AAAI_ | - | [[Code]](https://github.com/ngmarchant/attack-unlearning) | Certified Removal Mechanisms |\n","| [Athena: Probabilistic Verification of Machine Unlearning](https://web.archive.org/web/20220721061150id_/https://petsymposium.org/popets/2022/popets-2022-0072.pdf) | 2022 | Sommer et al. | _PoPETs_ | ATHENA | - | |\n","| [FP2-MIA: A Membership Inference Attack Free of Posterior Probability in Machine Unlearning](https://link.springer.com/chapter/10.1007/978-3-031-20917-8_12) | 2022 | Lu et al. | _ProvSec_ | FP2-MIA | - | inference attack |\n","| [Deletion Inference, Reconstruction, and Compliance in Machine (Un)Learning](https://arxiv.org/abs/2202.03460) | 2022 | Gao et al. | _PETS_ | - | - |  |\n","| [Prompt Certified Machine Unlearning with Randomized Gradient Smoothing and Quantization](https://openreview.net/pdf?id=ue4gP8ZKiWb) | 2022 | Zhang et al.   | _NeurIPS_ | PCMU | - | Certified Removal Mechanisms |\n","| [The Right to be Forgotten in Federated Learning: An Efficient Realization with Rapid Retraining](https://arxiv.org/abs/2203.07320) | 2022 | Liu et al. | _INFOCOM_ | - | [[Code]](https://github.com/yiliucs/federated-unlearning) |  |\n","| [Backdoor Defense with Machine Unlearning](https://arxiv.org/abs/2201.09538) | 2022 | Liu et al. | _INFOCOM_ | BAERASER | - | Backdoor defense |\n","| [Markov Chain Monte Carlo-Based Machine Unlearning: Unlearning What Needs to be Forgotten](https://dl.acm.org/doi/abs/10.1145/3488932.3517406) | 2022 | Nguyen et al. | _ASIA CCS_ | MCU | - | MCMC Unlearning  |\n","| [Federated Unlearning for On-Device Recommendation](https://arxiv.org/abs/2210.10958) | 2022 | Yuan et al. | _arXiv_ | - | - |  |\n","| [Can Bad Teaching Induce Forgetting? Unlearning in Deep Networks using an Incompetent Teacher](https://arxiv.org/abs/2205.08096) | 2022 | Chundawat et al. | _arXiv_ | - | - | Knowledge Adaptation |\n","| [ Efficient Two-Stage Model Retraining for Machine Unlearning](https://openaccess.thecvf.com/content/CVPR2022W/HCIS/html/Kim_Efficient_Two-Stage_Model_Retraining_for_Machine_Unlearning_CVPRW_2022_paper.html) | 2022 | Kim and Woo | _CVPR Workshop_ | - | - |  |\n","| [Learn to Forget: Machine Unlearning Via Neuron Masking](https://ieeexplore.ieee.org/abstract/document/9844865?casa_token=_eowH3BTt1sAAAAA:X0uCpLxOwcFRNJHoo3AtA0ay4t075_cSptgTMznsjusnvgySq-rJe8GC285YhWG4Q0fUmP9Sodw0) | 2021 | Ma et al. | _IEEE_ | Forsaken | - | Mask Gradients |\n","| [Adaptive Machine Unlearning](https://proceedings.neurips.cc/paper/2021/hash/87f7ee4fdb57bdfd52179947211b7ebb-Abstract.html) | 2021 | Gupta et al. | _NeurIPS_ | - | [[Code]](https://github.com/ChrisWaites/adaptive-machine-unlearning) | Differential Privacy |\n","| [Descent-to-Delete: Gradient-Based Methods for Machine Unlearning](https://proceedings.mlr.press/v132/neel21a.html) | 2021 | Neel et al. | _ALT_ | - | - | Certified Removal Mechanisms |\n","| [Remember What You Want to Forget: Algorithms for Machine Unlearning](https://arxiv.org/abs/2103.03279) | 2021 | Sekhari et al. | _NeurIPS_ | - | - |  |\n","| [FedEraser: Enabling Efficient Client-Level Data Removal from Federated Learning Models](https://ieeexplore.ieee.org/abstract/document/9521274) | 2021 | Liu et al. | _IWQoS_ | FedEraser | - |  |\n","| [Federated Unlearning](https://arxiv.org/abs/2012.13891) | 2021 | Liu et al. | _IWQoS_ | FedEraser | [[Code]](https://www.dropbox.com/s/1lhx962axovbbom/FedEraser-Code.zip?dl=0) |  |\n","| [Machine Unlearning via Algorithmic Stability](https://proceedings.mlr.press/v134/ullah21a.html) | 2021 | Ullah et al. | _COLT_ | TV | - | Certified Removal Mechanisms |\n","| [EMA: Auditing Data Removal from Trained Models](https://link.springer.com/chapter/10.1007/978-3-030-87240-3_76) | 2021 | Huang et al. | _MICCAI_ | EMA | [[Code]](https://github.com/Hazelsuko07/EMA) | Certified Removal Mechanisms |\n","| [Knowledge-Adaptation Priors](https://proceedings.neurips.cc/paper/2021/hash/a4380923dd651c195b1631af7c829187-Abstract.html) | 2021 | Khan and Swaroop | _NeurIPS_ | K-prior | [[Code]](https://github.com/team-approx-bayes/kpriors) | Knowledge Adaptation |\n","| [PrIU: A Provenance-Based Approach for Incrementally Updating Regression Models](https://dl.acm.org/doi/abs/10.1145/3318464.3380571) | 2020 | Wu et al. | _NeurIPS_ | PrIU | - | Knowledge Adaptation |\n","| [Eternal Sunshine of the Spotless Net: Selective Forgetting in Deep Networks](https://arxiv.org/abs/1911.04933) | 2020 | Golatkar et al. | _CVPR_ | - | - | Certified Removal Mechanisms |\n","| [Learn to Forget: User-Level Memorization Elimination in Federated Learning](https://www.researchgate.net/profile/Ximeng-Liu-5/publication/340134612_Learn_to_Forget_User-Level_Memorization_Elimination_in_Federated_Learning/links/5e849e64a6fdcca789e5f955/Learn-to-Forget-User-Level-Memorization-Elimination-in-Federated-Learning.pdf) | 2020 | Liu et al. | _arXiv_ | Forsaken | - |  |\n","| [Certified Data Removal from Machine Learning Models](https://proceedings.mlr.press/v119/guo20c.html) | 2020 | Guo et al. | _ICML_ | - | - | Certified Removal Mechanisms |\n","| [Class Clown: Data Redaction in Machine Unlearning at Enterprise Scale](https://arxiv.org/abs/2012.04699) | 2020 | Felps et al. | _arXiv_ | - | - | Decremental Learning |\n","| [A Novel Online Incremental and Decremental Learning Algorithm Based on Variable Support Vector Machine](https://link.springer.com/article/10.1007/s10586-018-1772-4) | 2019 | Chen et al. | _Cluster Computing_ | - | - | Decremental Learning  |\n","| [Making AI Forget You: Data Deletion in Machine Learning](https://papers.nips.cc/paper/2019/hash/cb79f8fa58b91d3af6c9c991f63962d3-Abstract.html) | 2019 | Ginart et al. | _NeurIPS_ | - | - | Decremental Learning  |\n","| [Lifelong Anomaly Detection Through Unlearning](https://dl.acm.org/doi/abs/10.1145/3319535.3363226) | 2019 | Du et al. | _CCS_ | - | - |  |\n","| [Learning Not to Learn: Training Deep Neural Networks With Biased Data](https://openaccess.thecvf.com/content_CVPR_2019/html/Kim_Learning_Not_to_Learn_Training_Deep_Neural_Networks_With_Biased_CVPR_2019_paper.html) | 2019 | Kim et al. | _CVPR_ | - | - |  |\n","| [Efficient Repair of Polluted Machine Learning Systems via Causal Unlearning](https://dl.acm.org/citation.cfm?id=3196517) | 2018 | Cao et al. | _ASIACCS_ | KARMA | [[Code]](https://github.com/CausalUnlearning/KARMA) |  |\n","| [Understanding Black-box Predictions via Influence Functions](https://proceedings.mlr.press/v70/koh17a.html) | 2017 | Koh et al. | _ICML_ | - | [[Code]](https://github.com/kohpangwei/influence-release) | Certified Removal Mechanisms |\n","| [Towards Making Systems Forget with Machine Unlearning](https://ieeexplore.ieee.org/abstract/document/7163042) | 2015 | Cao and Yang | _S&P_ | - |  |\n","| [Towards Making Systems Forget with Machine Unlearning](https://dl.acm.org/doi/10.1109/SP.2015.35) | 2015 | Cao et al. | _S&P_ | - | - | Statistical Query Learning  |\n","| [Incremental and decremental training for linear classification](https://dl.acm.org/doi/10.1145/2623330.2623661) | 2014 | Tsai et al. | _KDD_ | - | [[Code]](https://www.csie.ntu.edu.tw/~cjlin/papers/ws/) | Decremental Learning  |\n","| [Multiple Incremental Decremental Learning of Support Vector Machines](https://dl.acm.org/doi/10.5555/2984093.2984196) | 2009 | Karasuyama et al. | _NIPS_ | - | - | Decremental Learning  |\n","| [Incremental and Decremental Learning for Linear Support Vector Machines](https://dl.acm.org/doi/10.5555/1776814.1776838) | 2007 | Romero et al. | _ICANN_ | - | - | Decremental Learning  |\n","| [Decremental Learning Algorithms for Nonlinear Langrangian and Least Squares Support Vector Machines](https://www.semanticscholar.org/paper/Decremental-Learning-Algorithms-for-Nonlinear-and-Duan-Li/312c677f0882d0dfd60bfd77346588f52aefd10f) | 2007 | Duan et al. | _OSB_ | - | - | Decremental Learning  |\n","| [Multicategory Incremental Proximal Support Vector Classifiers](https://link.springer.com/chapter/10.1007/978-3-540-45224-9_54) | 2003 | Tveit et al. | _KES_ | - | - | Decremental Learning  |\n","| [Incremental and Decremental Proximal Support Vector Classification using Decay Coefficients](https://link.springer.com/chapter/10.1007/978-3-540-45228-7_42) | 2003 | Tveit et al. | _DaWak_ | - | - | Decremental Learning  |\n","| [Incremental and Decremental Support Vector Machine Learning](https://dl.acm.org/doi/10.5555/3008751.3008808) | 2000 | Cauwenberg et al. | _NeurIPS_ | - | - | Decremental Learning  |\n","----------"]},{"cell_type":"markdown","id":"a73ff151","metadata":{"papermill":{"duration":0.006637,"end_time":"2023-07-06T00:16:39.617248","exception":false,"start_time":"2023-07-06T00:16:39.610611","status":"completed"},"tags":[]},"source":["<h2 id=\"data-driven-approaches\">4.3. Data-Driven Approaches</h2>\n","\n","The approaches fallen into this category use data partition, data augmentation and data influence to speed up the retraining process. Methods of attack by data manipulation (e.g. data poisoning) are also included for reference.\n","\n","<div class=\"figure*\">\n","<figure>\n","<img src=\"https://raw.githubusercontent.com/tamlhp/awesome-machine-unlearning/main/figs/data-driven.png\" alt=\"https://arxiv.org/abs/2209.02299\" style=\"max-width: 60%;\"/>\n","</figure>\n","</div>\n","\n","<p><strong>Data Partitioning (Efficient Retraining).</strong> The\n","approaches falling into this category uses data partitioning mechanisms\n","to speed up the retraining process. Alternatively, they partially\n","retrain the model with some bounds on accuracy. Bourtoule et al.&#xA0;<a href=\"#ref-bourtoule2021machine\">(Bourtoule et al.\n","2021)</a> proposed the well-known SISA framework (<a\n","href=\"#fig:partition\" data-reference-type=\"autoref\"\n","data-reference=\"fig:partition\">[fig:partition]</a>) that partitions the\n","data into shards and slices. Each shard has a single model, and the\n","final output is an aggregation of multiple models over these shards. For\n","each slice of a shard, a model checkpoint is stored during training so\n","that a new model can be retrained from an intermediate state&#xA0;<a href=\"#ref-bourtoule2021machine aldaghri2021coded\">(Bourtoule et al.\n","2021; Aldaghri, Mahdavifar, et al. 2021)</a>.</p>\n","\n","<figure id=\"fig:partition\">\n","<img src=\"https://raw.githubusercontent.com/tamlhp/awesome-machine-unlearning/main/kaggle/partition.png\"\n","alt=\"Efficient retraining for machine unlearning using data partition\" style=\"max-width: 80%;\" />\n","<figcaption aria-hidden=\"true\">Efficient retraining for machine\n","unlearning using data partition</figcaption>\n","</figure>\n","\n","<p><strong>Data Augmentation (Error-manipulation noise).</strong> Data\n","augmentation is the process of enriching or adding more data to support\n","a model&#x2019;s training&#xA0;<a href=\"#ref-yu2021does\">(D. Yu\n","et al. 2021)</a>. Such mechanisms can be used to support machine\n","unlearning as well. Huang et al.&#xA0;<a href=\"#ref-huang2021unlearnable\">(H. Huang et al. 2021)</a> proposed\n","the idea of error-minimizing noise, which tricks a model into thinking\n","that there is nothing to be learned from a given set of data (i.e., the\n","loss does not change). However, it can only be used to protect a\n","particular data item before the model is trained. A similar setting was\n","also studied by Fawkes&#xA0;<a href=\"#ref-shan2020protecting\">(Shan et al. 2020)</a>, in which a\n","targeted adversarial attack is used to ensure the model does not learn\n","anything from a targeted data item.</p>\n","<p>Conversely, Tarun et al.&#xA0;<a href=\"#ref-tarun2021fast\">(Tarun et al. 2021)</a> proposed\n","error-maximizing noise to impair the model on a target class of data (to\n","be forgotten). However, this tactic does not work on specific data items\n","as it is easier to interfere with a model&#x2019;s prediction on a whole class\n","as opposed to a specific data item of that class&#xA0;<a href=\"#ref-tarun2021fast\">(Tarun et al. 2021)</a>.</p>\n","<p><strong>Data influence.</strong> This group of unlearning approaches\n","studies how a change in training data impacts a model&#x2019;s parameters&#xA0;<a href=\"#ref-wu2022puma conggrapheditor cao2022machine\">(G. Wu, Hashemi,\n","and Srinivasa 2022; Cong and Mahdavi 2022a; Z. Cao et al. 2022)</a>,\n","where impact is computed using influence functions&#xA0;<a href=\"#ref-mahadevan2022certifiable chundawat2022zero\">(Mahadevan and\n","Mathioudakis 2022; Chundawat et al. 2022b)</a>. However, influence\n","functions depend on the current state of a learning algorithm&#xA0;<a href=\"#ref-wu2022puma\">(G. Wu, Hashemi, and Srinivasa\n","2022)</a>. To mitigate this issue, several works store a training\n","history of intermediate quantities (e.g., model parameters or gradients)\n","generated by each step of model training&#xA0;<a href=\"#ref-graves2021amnesiac neel2021descent wu2020deltagrad wu2020priu\">(Graves,\n","Nagisetty, and Ganesh 2021; Neel, Roth, and Sharifi-Malvajerdi 2021; Y.\n","Wu et al. 2020; Y. Wu, Tannen, and Davidson 2020)</a>. Then, the\n","unlearning process becomes one of subtracting these historical updates.\n","However, the model&#x2019;s accuracy might degrade significantly due to\n","catastrophic unlearning&#xA0;<a href=\"#ref-nguyen2020variational\">(Q. P. Nguyen, Low, and Jaillet\n","2020)</a> since the order in which the training data is fed matters\n","to the learning model. Moreover, the influence itself does not verify\n","whether the data to be forgotten is still included in the unlearned\n","model&#xA0;<a href=\"#ref-thudi2021necessity thudi2022unrolling\">(Thudi, Jia, et al.\n","2022; Thudi, Deza, et al. 2022)</a>.</p>\n","<p>Zeng et al.&#xA0;<a href=\"#ref-zeng2021learning\">(Zeng et al. 2021)</a> suggested a new\n","method of modeling data influence by adding regularization terms into\n","the learning algorithm. Although this method is model-agnostic, it\n","requires intervening in the original training process of the original\n","model. Moreover, it is only applicable to convex learning problems and\n","deep neural networks.</p>\n","<p>Peste et al.&#xA0;<a href=\"#ref-peste2021ssse\">(Peste, Alistarh, and Lampert 2021)</a>\n","closed this gap by introducing a new Fisher-based unlearning method,\n","which can approximate the Hessian matrix. This method works for both\n","shallow and deep models, and also convex and non-convex problems. The\n","idea is to efficiently compute the matrix inversion of a Fisher\n","Information Matrix using rank-one updates. However, as the whole process\n","is approximate, there is no concrete guarantee on the unlearned\n","model.</p>\n","\n","| **Paper Title** | **Year** | **Author** | **Venue** | **Model** | **Code** | \n","| --------------- | :----: | ---- | :----: | :----: | :----: |\n","| [Hidden Poison: Machine Unlearning Enables Camouflaged Poisoning Attacks](https://arxiv.org/abs/2212.10717) | 2022 | Di et al. | _NeurIPS_ | - | [[Code]](https://github.com/Jimmy-di/camouflage-poisoning) | Data Poisoning |\n","| [Forget Unlearning: Towards True Data Deletion in Machine Learning](https://arxiv.org/pdf/2210.08911.pdf) | 2022 | Chourasia et al. | _ICLR_ | - | - | Data Influence |\n","| [ARCANE: An Efficient Architecture for Exact Machine Unlearning](https://www.ijcai.org/proceedings/2022/0556.pdf) | 2022 | Yan et al.  | _IJCAI_ | ARCANE | - | Data Partition |\n","| [PUMA: Performance Unchanged Model Augmentation for Training Data Removal](https://ojs.aaai.org/index.php/AAAI/article/view/20846) | 2022 | Wu et al. | _AAAI_ | PUMA | - | Data Influence |\n","| [Certifiable Unlearning Pipelines for Logistic Regression: An Experimental Study](https://www.mdpi.com/2504-4990/4/3/28) | 2022 | Mahadevan and Mathioudakis | _MAKE_ | - | [[Code]](https://version.helsinki.fi/mahadeva/unlearning-experiments) | Data Influence |\n","| [Zero-Shot Machine Unlearning](https://arxiv.org/abs/2201.05629) | 2022 | Chundawat et al. | _arXiv_ | - | - | Data Influence |\n","| [GRAPHEDITOR: An Efficient Graph Representation Learning and Unlearning Approach](https://congweilin.github.io/CongWeilin.io/files/GraphEditor.pdf) | 2022 | Cong and Mahdavi | - | GRAPHEDITOR | [[Code]](https://anonymous.4open.science/r/GraphEditor-NeurIPS22-856E/README.md) | Data Influence |\n","| [Fast Model Update for IoT Traffic Anomaly Detection with Machine Unlearning](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9927728) | 2022 | Fan et al. | _IEEE IoT-J_ | ViFLa | - | Data Partition |\n","| [Learning to Refit for Convex Learning Problems](https://arxiv.org/abs/2111.12545) | 2021 | Zeng et al. | _arXiv_ | OPTLEARN | - | Data Influence |\n","| [Fast Yet Effective Machine Unlearning](https://arxiv.org/abs/2111.08947) | 2021 | Ayush et al. | _arXiv_ | - | - | Data Augmentation |\n","| [Learning with Selective Forgetting](https://www.ijcai.org/proceedings/2021/0137.pdf) | 2021 | Shibata et al. | _IJCAI_ | - | - | Data Augmentation |\n","| [SSSE: Efficiently Erasing Samples from Trained Machine Learning Models](https://openreview.net/forum?id=GRMKEx3kEo) | 2021 | Peste et al. | _NeurIPS_ | SSSE | - | Data Influence |\n","| [How Does Data Augmentation Affect Privacy in Machine Learning?](https://arxiv.org/abs/2007.10567) | 2021 | Yu et al. | _AAAI_ | - | [[Code]](https://github.com/dayu11/MI_with_DA) | Data Augmentation |\n","| [Coded Machine Unlearning](https://ieeexplore.ieee.org/document/9458237) | 2021 | Aldaghri et al. | _IEEE_ | - | - | Data Partitioning |\n","| [Machine Unlearning](https://ieeexplore.ieee.org/document/9519428) | 2021 | Bourtoule et al. | _IEEE_ | SISA | [[Code]](https://github.com/cleverhans-lab/machine-unlearning) | Data Partitioning |\n","| [How Does Data Augmentation Affect Privacy in Machine Learning?](https://ojs.aaai.org/index.php/AAAI/article/view/17284/) | 2021 | Yu et al. | _AAAI_ | - | [[Code]](https://github.com/dayu11/MI_with_DA) | Data Augmentation |\n","| [Amnesiac Machine Learning](https://ojs.aaai.org/index.php/AAAI/article/view/17371) | 2021 | Graves et al. | _AAAI_ | AmnesiacML | [[Code]](https://github.com/lmgraves/AmnesiacML) | Data Influence |\n","| [Unlearnable Examples: Making Personal Data Unexploitable](https://arxiv.org/abs/2101.04898) | 2021 | Huang et al. | _ICLR_ | - | [[Code]](https://github.com/HanxunH/Unlearnable-Examples) | Data Augmentation |\n","| [Descent-to-Delete: Gradient-Based Methods for Machine Unlearning](https://proceedings.mlr.press/v132/neel21a.html) | 2021 | Neel et al. | _ALT_ | - | - | Data Influence |\n","| [Fawkes: Protecting Privacy against Unauthorized Deep Learning Models](https://dl.acm.org/doi/abs/10.5555/3489212.3489302) | 2020 | Shan et al. | _USENIX Sec. Sym._ | Fawkes | [[Code]](https://github.com/Shawn-Shan/fawkes) | Data Augmentation |\n","| [PrIU: A Provenance-Based Approach for Incrementally Updating Regression Models](https://dl.acm.org/doi/abs/10.1145/3318464.3380571) | 2020 | Wu et al. | _SIGMOD_ | PrIU/PrIU-opt | - | Data Influence |\n","| [DeltaGrad: Rapid retraining of machine learning models](https://proceedings.mlr.press/v119/wu20b.html) | 2020 | Wu et al. | _ICML_ | DeltaGrad | [[Code]](https://github.com/thuwuyinjun/DeltaGrad) | Data Influence |\n","\n","\n","----------"]},{"cell_type":"markdown","id":"45c90f4a","metadata":{"papermill":{"duration":0.006833,"end_time":"2023-07-06T00:16:39.631139","exception":false,"start_time":"2023-07-06T00:16:39.624306","status":"completed"},"tags":[]},"source":["<h2 id=\"sec:metrics\">5. Evaluation Metrics</h2>\n","\n","| Metrics | Formula/Description | Usage |\n","| ---- | ---- | ---- |\n","| Accuracy | Accuracy on unlearned model on forget set and retrain set | Evaluating the predictive performance of unlearned model |\n","| Completeness | The overlapping (e.g. Jaccard distance) of output space between the retrained and the unlearned model | Evaluating the indistinguishability between model outputs |\n","| Unlearn time | The amount of time of unlearning request | Evaluating the unlearning efficiency |\n","| Relearn Time | The epochs number required for the unlearned model to reach the accuracy of source model | Evaluating the unlearning efficiency (relearn with some data sample) |\n","| Layer-wise Distance | The weight difference between original model and retrain model | Evaluate the indistinguishability between model parameters |\n","| Activation Distance | An average of the L2-distance between the unlearned model and retrained models predicted probabilities on the forget set | Evaluating the indistinguishability between model outputs | \n","| JS-Divergence | Jensen-Shannon divergence between the predictions of the unlearned and retrained model | Evaluating the indistinguishability between model outputs |\n","| Membership Inference Attack | Recall (#detected items / #forget items) | Verify the influence of forget data on the unlearned model |\n","| ZRF score | $\\mathcal{ZRF} = 1 - \\frac{1}{nf}\\sum\\limits_{i=0}^{n_f} \\mathcal{JS}(M(x_i), T_d(x_i))$ | The unlearned model should not intentionally give wrong output $\\(\\mathcal{ZRF} = 0\\)$ or random output $\\(\\mathcal{ZRF} = 1\\)$ on the forget item |\n","| Anamnesis Index (AIN) | $AIN = \\frac{r_t (M_u, M_{orig}, \\alpha)}{r_t (M_s, M_{orig}, \\alpha)}$ | Zero-shot machine unlearning | \n","| Epistemic Uncertainty | if $\\mbox{i(w;D) > 0}$, then $\\mbox{efficacy}(w;D) = \\frac{1}{i(w; D)}$;<br />otherwise $\\mbox{efficacy}(w;D) = \\infty$ | How much information the model exposes |\n","| Model Inversion Attack | Visualization | Qualitative verifications and evaluations |\n","\n","<p>The most often used metrics for measuring anomaly detection\n","performance include accuracy, completeness, unlearn time, distance, and\n","forgetting scores. Their formulas and common usage are summarized in <a\n","href=\"#tab:metrics\" data-reference-type=\"autoref\"\n","data-reference=\"tab:metrics\">[tab:metrics]</a>. More detailed\n","descriptions are given below.</p>\n","<p><strong>Accuracy.</strong> In machine unlearning, a model&#x2019;s accuracy\n","needs to be compared on three different datasets: (1) The set to be\n","forgotten. Since the expected behaviour of an unlearned model after\n","unlearning should mirror that of a retrained model, the accuracy on the\n","remaining data should be similar to the retrained model. (2) The\n","retained set. The retained set&#x2019;s accuracy should be close to that of the\n","original model. (3) The test set. The unlearned model should still\n","perform well on a separate test dataset compared to the retrained\n","model.</p>\n","<p><strong>Completeness.</strong> The influence of the to-be-removed\n","samples on the unlearned model must be completely eliminated.\n","Completeness, hence, measures the degree to which an unlearned model is\n","compatible with a retrained model&#xA0;<a href=\"#ref-cao2015towards\">(Y. Cao and Yang 2015)</a>. If the\n","unlearned model gives similar predictions to a retrained model for all\n","samples, the operation of feeding samples or observing the model&#x2019;s\n","information is impractical to achieve the forgotten data and its\n","lineage. The final metric is often calculated as the overlap of output\n","space (e.g., the Jaccard distance) between the unlearned model and the\n","retraining. However, computing this metric is often exhaustive.</p>\n","<p><strong>Unlearning time and Retraining time.</strong> Timeliness\n","quantifies the time saved when using unlearning instead of retraining\n","for model update. The quicker the system restores privacy, security, and\n","usefulness, the more timely the unlearning process. In particular,\n","retraining uses the whole training set to execute the learning\n","algorithm, whereas unlearning executes the learning algorithm on a\n","limited amount of summations; hence, the speed of unlearning is quicker\n","due to the reduced size of the training data.</p>\n","<p><strong>Relearn time.</strong> Relearning time is an excellent proxy\n","for measuring the amount of unlearned data information left in the\n","model. If a model recovers its performance on unlearned data with just a\n","few steps of retraining, it is extremely probable that the model has\n","retained some knowledge of the unlearned data.</p>\n","<p><strong>The layer-wise distance.</strong> The layer-wise distance\n","between the original and unlearned models helps when trying to\n","understand the impact of the unlearning on each layer. The weight\n","difference should be comparable to a retrained model given that a\n","shorter distance indicates ineffective unlearning. Likewise, a much\n","longer distance may point to a Streisand effect and possible information\n","leaks.</p>\n","<p><strong>Activation Distance.</strong> The activation distance is the\n","separation between the final activation of the scrubbed weights and the\n","retrained model. A shorter activation distance indicates superior\n","unlearning.</p>\n","<p><strong>JS-Divergence.</strong> When paired with the activation\n","distance, the JS-Divergence between the predictions of the unlearned and\n","retrained model provides a more full picture of unlearning. Less\n","divergence results in better unlearning. The formula of JS-Divergence is\n","<span\n","class=\"math inline\">&#x1D4A5;&#x1D4AE;(<em>M</em>(<em>x</em>),<em>T</em><sub><em>d</em></sub>(<em>x</em>))&#x2004;=&#x2004;0.5&#x2005;*&#x2005;&#x1D4A6;&#x2112;(<em>M</em>(<em>x</em>)||<em>m</em>)&#x2005;+&#x2005;0.5&#x2005;*&#x2005;&#x1D4A6;&#x2112;(<em>T</em><sub><em>d</em></sub>(<em>x</em>)||<em>m</em>)</span>,\n","where <span class=\"math inline\"><em>M</em></span> is unlearned model,\n","<span class=\"math inline\"><em>T</em><sub><em>d</em></sub></span> is a\n","competent teacher, and <span class=\"math inline\">&#x1D4A6;&#x2112;</span> is The\n","Kullback-Leibler divergence <a href=\"#ref-KLFormula\">(Kullback et al. 1951)</a>, <span\n","class=\"math inline\">$m= \\frac{M(x)+T_d(x)}{2}$</span>.</p>\n","<p><strong>Membership Inference.</strong> The membership inference\n","metric leverages a membership inference attack to determine whether or\n","not any information about the forgotten samples remains in the\n","model&#xA0;<a href=\"#ref-chen2021machine\">(M. Chen et\n","al. 2021b)</a>. The set to be forgotten should have reduced the\n","attack probability in the unlearned model. The chance of an inference\n","attack should be reduced in the unlearned model compared to the original\n","model for the forgotten class data.</p>\n","<p><strong>ZRF score.</strong> Zero Retrain Forgetting (ZRF) makes it\n","possible to evaluate unlearning approaches independent of\n","retraining&#xA0;<a href=\"#ref-chundawat2022can\">(Chundawat et al. 2022a)</a>. The\n","unpredictability of the model&#x2019;s predictions is measured by comparing\n","them to an unskilled instructor. ZRF compares the set to be forgotten&#x2019;s\n","output distribution to the output of a randomly initialised model, which\n","in most situations is our lousy instructor. The ZRF score ranges between\n","0 and 1; it will be near to 1 if the model&#x2019;s behaviour with the\n","forgotten samples is entirely random, and close to 0 if it exhibits a\n","certain pattern. The formula of ZRF score is <span\n","class=\"math inline\">$\\mathcal{ZRF} = 1 -\n","\\frac{1}{nf}\\sum\\limits_{i=0}^{n_f} \\mathcal{JS}(M(x_i),\n","T_d(x_i))$</span>, where <span\n","class=\"math inline\"><em>x</em><sub><em>i</em></sub></span> is the <span\n","class=\"math inline\"><em>i</em><sub><em>t</em><em>h</em></sub></span>\n","sample from the set to be forgotten with a total number of samples <span\n","class=\"math inline\"><em>n</em><sub><em>f</em></sub></span></p>\n","<p><strong>Anamnesis Index (AIN).</strong> AIN values range between 0\n","and 1. The better the unlearning, the closer to 1. Instances where\n","information from the classes to be forgotten are still preserved in the\n","model correlate to AIN levels well below 1. A score closer to 0 also\n","suggests that the unlearned model will rapidly relearn to generate\n","correct predictions. This may be due to the fact that the last layers\n","contain limited reversible modifications, which degrades the performance\n","of the model on the forgotten classes. If an AIN score is much greater\n","than 1, it may suggest that the approach causes parameter changes that\n","are so severe that the unlearning itself may be detected (Streisand\n","effect). This might be due to the fact that the model was pushed away\n","from the original point and, as a result, is unable to retrieve\n","previously learned knowledge about the forgotten class(es). The formula\n","for calculating an AIN value is <span class=\"math inline\">$AIN =\n","\\frac{r_t (M_u, M_{orig}, \\alpha)}{r_t (M_s, M_{orig}, \\alpha)}$</span>,\n","where <span class=\"math inline\"><em>&#x3B1;</em>%</span> is a margin around\n","the initial precision used to determine relearn time. <span\n","class=\"math inline\"><em>r</em><sub><em>t</em></sub>(<em>M</em>,<em>M</em><sub><em>o</em><em>r</em><em>i</em><em>g</em></sub>,<em>&#x3B1;</em>)</span>\n","are mini-batches (or steps) to be achieved by the model <span\n","class=\"math inline\"><em>M</em></span> on the classes to be forgotten\n","within <span class=\"math inline\"><em>&#x3B1;</em>%</span> of the precision\n","compared to the original model <span\n","class=\"math inline\"><em>M</em><sub><em>o</em><em>r</em><em>i</em><em>g</em></sub></span>.\n","<span class=\"math inline\"><em>M</em><sub><em>u</em></sub></span> and\n","<span class=\"math inline\"><em>M</em><sub><em>s</em></sub></span>\n","respectively represent the unlearned model and a model trained from\n","scratch.</p>\n","\n"," \n","\n","\n","----------"]},{"cell_type":"markdown","id":"85008f23","metadata":{"papermill":{"duration":0.008117,"end_time":"2023-07-06T00:16:39.646136","exception":false,"start_time":"2023-07-06T00:16:39.638019","status":"completed"},"tags":[]},"source":["<h1 id=\"sec:reference\">References</h1>\n","\n","<div id=\"refs\" class=\"references csl-bib-body hanging-indent\"\n","role=\"list\">\n","<div id=\"ref-abadi2016deep\" class=\"csl-entry\" role=\"listitem\">\n","Abadi, Martin, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya\n","Mironov, Kunal Talwar, and Li Zhang. 2016. <span>&#x201C;Deep Learning with\n","Differential Privacy.&#x201D;</span> In <em>SIGSAC</em>, 308&#x2013;18.\n","</div>\n","<div id=\"ref-aldaghri2021coded\" class=\"csl-entry\" role=\"listitem\">\n","Aldaghri, Nasser, Hessam Mahdavifar, et al. 2021. <span>&#x201C;Coded Machine\n","Unlearning.&#x201D;</span> <em>IEEE Access</em> 9: 88137&#x2013;50.\n","</div>\n","<div id=\"ref-basu2021influence\" class=\"csl-entry\" role=\"listitem\">\n","Basu, Samyadeep, Phil Pope, and Soheil Feizi. 2021. <span>&#x201C;Influence\n","Functions in Deep Learning Are Fragile.&#x201D;</span> In <em>ICLR</em>.\n","</div>\n","<div id=\"ref-baumhauer2020machine\" class=\"csl-entry\" role=\"listitem\">\n","Baumhauer, Thomas, Pascal Sch&#xF6;ttle, and Matthias Zeppelzauer. 2020.\n","<span>&#x201C;Machine Unlearning: Linear Filtration for Logit-Based\n","Classifiers.&#x201D;</span> <em>arXiv Preprint arXiv:2002.02730</em>.\n","</div>\n","<div id=\"ref-becker2022epistemic\" class=\"csl-entry\" role=\"listitem\">\n","Becker, Alexander, and Thomas Liebig. 2022. <span>&#x201C;Evaluating Machine\n","Unlearning via Epistemic Uncertainty.&#x201D;</span>\n","</div>\n","<div id=\"ref-berahas2016multi\" class=\"csl-entry\" role=\"listitem\">\n","Berahas, Albert S, Jorge Nocedal, et al. 2016. <span>&#x201C;A Multi-Batch\n","l-BFGS Method for Machine Learning.&#x201D;</span> <em>NIPS</em> 29.\n","</div>\n","<div id=\"ref-bitansky2012extractable\" class=\"csl-entry\" role=\"listitem\">\n","Bitansky, Nir, Ran Canetti, Alessandro Chiesa, and Eran Tromer. 2012.\n","<span>&#x201C;From Extractable Collision Resistance to Succinct Non-Interactive\n","Arguments of Knowledge, and Back Again.&#x201D;</span> In <em>ITCS</em>,\n","326&#x2013;49.\n","</div>\n","<div id=\"ref-bollapragada2018progressive\" class=\"csl-entry\"\n","role=\"listitem\">\n","Bollapragada, Raghu, Jorge Nocedal, Dheevatsa Mudigere, Hao-Jun Shi, and\n","Ping Tak Peter Tang. 2018. <span>&#x201C;A Progressive Batching l-BFGS Method\n","for Machine Learning.&#x201D;</span> In <em>ICML</em>, 620&#x2013;29.\n","</div>\n","<div id=\"ref-bourtoule2021machine\" class=\"csl-entry\" role=\"listitem\">\n","Bourtoule, Lucas, Varun Chandrasekaran, Christopher A Choquette-Choo,\n","Hengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas\n","Papernot. 2021. <span>&#x201C;Machine Unlearning.&#x201D;</span> In <em>SP</em>,\n","141&#x2013;59.\n","</div>\n","<div id=\"ref-brophy2021machine\" class=\"csl-entry\" role=\"listitem\">\n","Brophy, Jonathan, and Daniel Lowd. 2021. <span>&#x201C;Machine Unlearning for\n","Random Forests.&#x201D;</span> In <em>ICML</em>, 1092&#x2013;1104.\n","</div>\n","<div id=\"ref-cao2015towards\" class=\"csl-entry\" role=\"listitem\">\n","Cao, Yinzhi, and Junfeng Yang. 2015. <span>&#x201C;Towards Making Systems\n","Forget with Machine Unlearning.&#x201D;</span> In <em>2015 IEEE Symposium on\n","Security and Privacy</em>, 463&#x2013;80.\n","</div>\n","<div id=\"ref-cao2018efficient\" class=\"csl-entry\" role=\"listitem\">\n","Cao, Yinzhi, Alexander Fangxiao Yu, Andrew Aday, Eric Stahl, Jon\n","Merwine, and Junfeng Yang. 2018. <span>&#x201C;Efficient Repair of Polluted\n","Machine Learning Systems via Causal Unlearning.&#x201D;</span> In\n","<em>ASIACCS</em>, 735&#x2013;47.\n","</div>\n","<div id=\"ref-cao2022machine\" class=\"csl-entry\" role=\"listitem\">\n","Cao, Zihao, Jianzong Wang, Shijing Si, Zhangcheng Huang, and Jing Xiao.\n","2022. <span>&#x201C;Machine Unlearning Method Based on Projection\n","Residual.&#x201D;</span> In <em>DSAA</em>, 1&#x2013;8.\n","</div>\n","<div id=\"ref-cauwenberghs2000incremental\" class=\"csl-entry\"\n","role=\"listitem\">\n","Cauwenberghs, Gert et al. 2000. <span>&#x201C;Incremental and Decremental\n","Support Vector Machine Learning.&#x201D;</span> <em>NIPS</em> 13.\n","</div>\n","<div id=\"ref-chang2022example\" class=\"csl-entry\" role=\"listitem\">\n","Chang, Yi, Zhao Ren, Thanh Tam Nguyen, Wolfgang Nejdl, and Bj&#xF6;rn W\n","Schuller. 2022. <span>&#x201C;Example-Based Explanations with Adversarial\n","Attacks for Respiratory Sound Analysis.&#x201D;</span> In <em>INTERSPEECH</em>.\n","</div>\n","<div id=\"ref-chaudhuri2011differentially\" class=\"csl-entry\"\n","role=\"listitem\">\n","Chaudhuri, Kamalika, Claire Monteleoni, and Anand D Sarwate. 2011.\n","<span>&#x201C;Differentially Private Empirical Risk Minimization.&#x201D;</span>\n","<em>JMLR</em> 12 (3).\n","</div>\n","<div id=\"ref-chen2022recommendation\" class=\"csl-entry\" role=\"listitem\">\n","Chen, Chong, Fei Sun, Min Zhang, and Bolin Ding. 2022.\n","<span>&#x201C;Recommendation Unlearning.&#x201D;</span> In <em>WWW</em>, 2768&#x2013;77.\n","</div>\n","<div id=\"ref-chen2020graph\" class=\"csl-entry\" role=\"listitem\">\n","Chen, Fenxiao, Yun-Cheng Wang, Bin Wang, et al. 2020. <span>&#x201C;Graph\n","Representation Learning: A Survey.&#x201D;</span> <em>ATSIP</em> 9.\n","</div>\n","<div id=\"ref-chen2021machinegan\" class=\"csl-entry\" role=\"listitem\">\n","Chen, Kongyang, Yao Huang, et al. 2021. <span>&#x201C;Machine Unlearning via\n","GAN.&#x201D;</span> <em>arXiv Preprint arXiv:2111.11869</em>.\n","</div>\n","<div id=\"ref-chen2021graph\" class=\"csl-entry\" role=\"listitem\">\n","Chen, Min, Zhikun Zhang, Tianhao Wang, Michael Backes, Mathias Humbert,\n","and Yang Zhang. 2021a. <span>&#x201C;Graph Unlearning.&#x201D;</span> <em>arXiv\n","Preprint arXiv:2103.14991</em>.\n","</div>\n","<div id=\"ref-chen2021machine\" class=\"csl-entry\" role=\"listitem\">\n","&#x2014;&#x2014;&#x2014;. 2021b. <span>&#x201C;When Machine Unlearning Jeopardizes Privacy.&#x201D;</span>\n","In <em>SIGSAC</em>, 896&#x2013;911.\n","</div>\n","<div id=\"ref-chen2019novel\" class=\"csl-entry\" role=\"listitem\">\n","Chen, Yuantao, Jie Xiong, Weihong Xu, and Jingwen Zuo. 2019. <span>&#x201C;A\n","Novel Online Incremental and Decremental Learning Algorithm Based on\n","Variable Support Vector Machine.&#x201D;</span> <em>Cluster Computing</em> 22\n","(3): 7435&#x2013;45.\n","</div>\n","<div id=\"ref-chien2022certified\" class=\"csl-entry\" role=\"listitem\">\n","Chien, Eli, Chao Pan, et al. 2022. <span>&#x201C;Certified Graph\n","Unlearning.&#x201D;</span> <em>arXiv Preprint arXiv:2206.09140</em>.\n","</div>\n","<div id=\"ref-chundawat2022can\" class=\"csl-entry\" role=\"listitem\">\n","Chundawat, Vikram S, Ayush K Tarun, Murari Mandal, and Mohan\n","Kankanhalli. 2022a. <span>&#x201C;Can Bad Teaching Induce Forgetting?\n","Unlearning in Deep Networks Using an Incompetent Teacher.&#x201D;</span>\n","<em>arXiv Preprint arXiv:2205.08096</em>.\n","</div>\n","<div id=\"ref-chundawat2022zero\" class=\"csl-entry\" role=\"listitem\">\n","&#x2014;&#x2014;&#x2014;. 2022b. <span>&#x201C;Zero-Shot Machine Unlearning.&#x201D;</span> <em>arXiv\n","Preprint arXiv:2201.05629</em>.\n","</div>\n","<div id=\"ref-conggrapheditor\" class=\"csl-entry\" role=\"listitem\">\n","Cong, Weilin, and Mehrdad Mahdavi. 2022a. <span>&#x201C;GRAPHEDITOR: An\n","Efficient Graph Representation Learning and Unlearning Approach.&#x201D;</span>\n","<em><a href=\"https://congweilin.github.io/CongWeilin.io/\"\n","class=\"uri\">Https://Congweilin.github.io/CongWeilin.io/</a></em>.\n","</div>\n","<div id=\"ref-congprivacy\" class=\"csl-entry\" role=\"listitem\">\n","&#x2014;&#x2014;&#x2014;. 2022b. <span>&#x201C;Privacy Matters! Efficient Graph Representation\n","Unlearning with Data Removal Guarantee.&#x201D;</span> <em><a\n","href=\"https://congweilin.github.io/CongWeilin.io/\"\n","class=\"uri\">Https://Congweilin.github.io/CongWeilin.io/</a></em>.\n","</div>\n","<div id=\"ref-DaiDHSCW22\" class=\"csl-entry\" role=\"listitem\">\n","Dai, Damai, Li Dong, et al. 2022. <span>&#x201C;Knowledge Neurons in Pretrained\n","Transformers.&#x201D;</span> In <em>ACL</em>, 8493&#x2013;8502.\n","</div>\n","<div id=\"ref-dang2021right\" class=\"csl-entry\" role=\"listitem\">\n","Dang, Quang-Vinh. 2021. <span>&#x201C;Right to Be Forgotten in the Age of\n","Machine Learning.&#x201D;</span> In <em>ICADS</em>, 403&#x2013;11.\n","</div>\n","<div id=\"ref-deng2009imagenet\" class=\"csl-entry\" role=\"listitem\">\n","Deng, Jia, Wei Dong, Richard Socher, et al. 2009. <span>&#x201C;Imagenet: A\n","Large-Scale Hierarchical Image Database.&#x201D;</span> In <em>CVPR</em>,\n","248&#x2013;55.\n","</div>\n","<div id=\"ref-dinsdale2020unlearning\" class=\"csl-entry\" role=\"listitem\">\n","Dinsdale, Nicola K, Mark Jenkinson, et al. 2020. <span>&#x201C;Unlearning\n","Scanner Bias for Mri Harmonisation.&#x201D;</span> In <em>MICCAI</em>, 369&#x2013;78.\n","</div>\n","<div id=\"ref-dinsdale2021deep\" class=\"csl-entry\" role=\"listitem\">\n","Dinsdale, Nicola K, Mark Jenkinson, and Ana IL Namburete. 2021.\n","<span>&#x201C;Deep Learning-Based Unlearning of Dataset Bias for MRI\n","Harmonisation and Confound Removal.&#x201D;</span> <em>NeuroImage</em> 228:\n","117689.\n","</div>\n","<div id=\"ref-du2019lifelong\" class=\"csl-entry\" role=\"listitem\">\n","Du, Min, Zhi Chen, et al. 2019. <span>&#x201C;Lifelong Anomaly Detection\n","Through Unlearning.&#x201D;</span> In <em>SIGSAC</em>, 1283&#x2013;97.\n","</div>\n","<div id=\"ref-duan2007decremental\" class=\"csl-entry\" role=\"listitem\">\n","Duan, Hua, Hua Li, Guoping He, and Qingtian Zeng. 2007.\n","<span>&#x201C;Decremental Learning Algorithms for Nonlinear Langrangian and\n","Least Squares Support Vector Machines.&#x201D;</span> In <em>OSB</em>, 358&#x2013;66.\n","</div>\n","<div id=\"ref-duda2020training\" class=\"csl-entry\" role=\"listitem\">\n","Duda, Piotr, Maciej Jaworski, Andrzej Cader, and Lipo Wang. 2020.\n","<span>&#x201C;On Training Deep Neural Networks Using a Streaming\n","Approach.&#x201D;</span> <em>JAISCR</em> 10.\n","</div>\n","<div id=\"ref-dwork2008differential\" class=\"csl-entry\" role=\"listitem\">\n","Dwork, Cynthia. 2008. <span>&#x201C;Differential Privacy: A Survey of\n","Results.&#x201D;</span> In <em>TAMC</em>, 1&#x2013;19.\n","</div>\n","<div id=\"ref-dwork2014algorithmic\" class=\"csl-entry\" role=\"listitem\">\n","Dwork, Cynthia, Aaron Roth, et al. 2014. <span>&#x201C;The Algorithmic\n","Foundations of Differential Privacy.&#x201D;</span> <em>Foundations and\n","Trends<span></span> in Theoretical Computer Science</em> 9 (3&#x2013;4):\n","211&#x2013;407.\n","</div>\n","<div id=\"ref-eisenhofer2022verifiable\" class=\"csl-entry\"\n","role=\"listitem\">\n","Eisenhofer, Thorsten, Doreen Riepel, Varun Chandrasekaran, Esha Ghosh,\n","Olga Ohrimenko, and Nicolas Papernot. 2022. <span>&#x201C;Verifiable and\n","Provably Secure Machine Unlearning.&#x201D;</span> <em>arXiv Preprint\n","arXiv:2210.09126</em>.\n","</div>\n","<div id=\"ref-felps2020class\" class=\"csl-entry\" role=\"listitem\">\n","Felps, Daniel L, Amelia D Schwickerath, Joyce D Williams, Trung N Vuong,\n","Alan Briggs, Matthew Hunt, Evan Sakmar, David D Saranchak, and Tyler\n","Shumaker. 2020. <span>&#x201C;Class Clown: Data Redaction in Machine Unlearning\n","at Enterprise Scale.&#x201D;</span> <em>arXiv Preprint arXiv:2012.04699</em>.\n","</div>\n","<div id=\"ref-feuerriegel2020fair\" class=\"csl-entry\" role=\"listitem\">\n","Feuerriegel, Stefan, Mateusz Dolata, and Gerhard Schwabe. 2020.\n","<span>&#x201C;Fair AI.&#x201D;</span> <em>Business &amp; Information Systems\n","Engineering</em> 62 (4): 379&#x2013;84.\n","</div>\n","<div id=\"ref-fredrikson2014privacy\" class=\"csl-entry\" role=\"listitem\">\n","Fredrikson, Matthew, Eric Lantz, Somesh Jha, Simon Lin, David Page, and\n","Thomas Ristenpart. 2014. <span>&#x201C;Privacy in Pharmacogenetics: An <span\n","class=\"math inline\">{</span>End-to-End<span class=\"math inline\">}</span>\n","Case Study of Personalized Warfarin Dosing.&#x201D;</span> In <em>USENIX\n","Security</em>, 17&#x2013;32.\n","</div>\n","<div id=\"ref-fu2022knowledge\" class=\"csl-entry\" role=\"listitem\">\n","Fu, Shaopeng, Fengxiang He, et al. 2022. <span>&#x201C;Knowledge Removal in\n","Sampling-Based Bayesian Inference.&#x201D;</span> In <em>ICLR</em>.\n","</div>\n","<div id=\"ref-fu2021bayesian\" class=\"csl-entry\" role=\"listitem\">\n","Fu, Shaopeng, Fengxiang He, Yue Xu, and Dacheng Tao. 2021.\n","<span>&#x201C;Bayesian Inference Forgetting.&#x201D;</span> <em>arXiv Preprint\n","arXiv:2101.06417</em>.\n","</div>\n","<div id=\"ref-gao2022deletion\" class=\"csl-entry\" role=\"listitem\">\n","Gao, Ji, Sanjam Garg, Mohammad Mahmoody, and Prashant Nalini Vasudevan.\n","2022. <span>&#x201C;Deletion Inference, Reconstruction, and Compliance in\n","Machine (Un) Learning.&#x201D;</span> <em>Proc. Priv. Enhancing Technol.</em>\n","2022 (3): 415&#x2013;36.\n","</div>\n","<div id=\"ref-gao2022verifi\" class=\"csl-entry\" role=\"listitem\">\n","Gao, Xiangshan, Xingjun Ma, Jingyi Wang, Youcheng Sun, Bo Li, Shouling\n","Ji, Peng Cheng, and Jiming Chen. 2022. <span>&#x201C;VeriFi: Towards Verifiable\n","Federated Unlearning.&#x201D;</span> <em>arXiv Preprint arXiv:2205.12709</em>.\n","</div>\n","<div id=\"ref-garg2020formalizing\" class=\"csl-entry\" role=\"listitem\">\n","Garg, Sanjam, Shafi Goldwasser, and Prashant Nalini Vasudevan. 2020.\n","<span>&#x201C;Formalizing Data Deletion in the Context of the Right to Be\n","Forgotten.&#x201D;</span> In <em>EUROCRYPT</em>, 373&#x2013;402.\n","</div>\n","<div id=\"ref-gasteiger2018combining\" class=\"csl-entry\" role=\"listitem\">\n","Gasteiger, Johannes, Aleksandar Bojchevski, and Stephan G&#xFC;nnemann. 2019.\n","<span>&#x201C;Combining Neural Networks with Personalized PageRank for\n","Classification on Graphs.&#x201D;</span> In <em>ICLR</em>.\n","</div>\n","<div id=\"ref-geurts2006extremely\" class=\"csl-entry\" role=\"listitem\">\n","Geurts, Pierre, Damien Ernst, et al. 2006. <span>&#x201C;Extremely Randomized\n","Trees.&#x201D;</span> <em>Machine Learning</em> 63 (1): 3&#x2013;42.\n","</div>\n","<div id=\"ref-ginart2019making\" class=\"csl-entry\" role=\"listitem\">\n","Ginart, Antonio, Melody Guan, Gregory Valiant, and James Y Zou. 2019.\n","<span>&#x201C;Making Ai Forget You: Data Deletion in Machine Learning.&#x201D;</span>\n","<em>NIPS</em> 32.\n","</div>\n","<div id=\"ref-goel2022evaluating\" class=\"csl-entry\" role=\"listitem\">\n","Goel, Shashwat, Ameya Prabhu, and Ponnurangam Kumaraguru. 2022.\n","<span>&#x201C;Evaluating Inexact Unlearning Requires Revisiting\n","Forgetting.&#x201D;</span> <em>arXiv Preprint arXiv:2201.06640</em>.\n","</div>\n","<div id=\"ref-golatkar2021mixed\" class=\"csl-entry\" role=\"listitem\">\n","Golatkar, Aditya, Alessandro Achille, Avinash Ravichandran, Marzia\n","Polito, and Stefano Soatto. 2021. <span>&#x201C;Mixed-Privacy Forgetting in\n","Deep Networks.&#x201D;</span> In <em>CVPR</em>, 792&#x2013;801.\n","</div>\n","<div id=\"ref-golatkar2020eternal\" class=\"csl-entry\" role=\"listitem\">\n","Golatkar, Aditya, Alessandro Achille, and Stefano Soatto. 2020a.\n","<span>&#x201C;Eternal Sunshine of the Spotless Net: Selective Forgetting in\n","Deep Networks.&#x201D;</span> In <em>CVPR</em>, 9304&#x2013;12.\n","</div>\n","<div id=\"ref-golatkar2020forgetting\" class=\"csl-entry\" role=\"listitem\">\n","&#x2014;&#x2014;&#x2014;. 2020b. <span>&#x201C;Forgetting Outside the Box: Scrubbing Deep Networks\n","of Information Accessible from Input-Output Observations.&#x201D;</span> In\n","<em>ECCV</em>, 383&#x2013;98.\n","</div>\n","<div id=\"ref-goyal2021revisiting\" class=\"csl-entry\" role=\"listitem\">\n","Goyal, Adit, Vikas Hassija, and Victor Hugo C de Albuquerque. 2021.\n","<span>&#x201C;Revisiting Machine Learning Training Process for Enhanced Data\n","Privacy.&#x201D;</span> In <em>IC3</em>, 247&#x2013;51.\n","</div>\n","<div id=\"ref-graves2021amnesiac\" class=\"csl-entry\" role=\"listitem\">\n","Graves, Laura, Vineel Nagisetty, and Vijay Ganesh. 2021. <span>&#x201C;Amnesiac\n","Machine Learning.&#x201D;</span> In <em>AAAI</em>, 35:11516&#x2013;24. 13.\n","</div>\n","<div id=\"ref-GuoGHM20\" class=\"csl-entry\" role=\"listitem\">\n","Guo, Chuan, Tom Goldstein, Awni Y. Hannun, and Laurens van der Maaten.\n","2020. <span>&#x201C;Certified Data Removal from Machine Learning\n","Models.&#x201D;</span> In <em>ICML</em>, 119:3832&#x2013;42.\n","</div>\n","<div id=\"ref-guo2020survey\" class=\"csl-entry\" role=\"listitem\">\n","Guo, Ruocheng, Lu Cheng, Jundong Li, P Richard Hahn, and Huan Liu. 2020.\n","<span>&#x201C;A Survey of Learning Causality with Data: Problems and\n","Methods.&#x201D;</span> <em>CSUR</em> 53 (4): 1&#x2013;37.\n","</div>\n","<div id=\"ref-guo2022efficient\" class=\"csl-entry\" role=\"listitem\">\n","Guo, Tao, Song Guo, Jiewei Zhang, Wenchao Xu, and Junxiao Wang. 2022.\n","<span>&#x201C;Efficient Attribute Unlearning: Towards Selective Removal of\n","Input Attributes from Feature Representations.&#x201D;</span> <em>arXiv\n","Preprint arXiv:2202.13295</em>.\n","</div>\n","<div id=\"ref-gupta2021adaptive\" class=\"csl-entry\" role=\"listitem\">\n","Gupta, Varun, Christopher Jung, Seth Neel, Aaron Roth, Saeed\n","Sharifi-Malvajerdi, and Chris Waites. 2021. <span>&#x201C;Adaptive Machine\n","Unlearning.&#x201D;</span> <em>NIPS</em> 34: 16319&#x2013;30.\n","</div>\n","<div id=\"ref-halimi2022federated\" class=\"csl-entry\" role=\"listitem\">\n","Halimi, Anisa, Swanand Kadhe, Ambrish Rawat, and Nathalie Baracaldo.\n","2022. <span>&#x201C;Federated Unlearning: How to Efficiently Erase a Client in\n","FL?&#x201D;</span> <em>arXiv Preprint arXiv:2207.05521</em>.\n","</div>\n","<div id=\"ref-hamilton2020graph\" class=\"csl-entry\" role=\"listitem\">\n","Hamilton, William L. 2020. <span>&#x201C;Graph Representation Learning.&#x201D;</span>\n","<em>Synthesis Lectures on Artifical Intelligence and Machine\n","Learning</em> 14 (3): 1&#x2013;159.\n","</div>\n","<div id=\"ref-haug2021learning\" class=\"csl-entry\" role=\"listitem\">\n","Haug, Johannes, and Gjergji Kasneci. 2021. <span>&#x201C;Learning Parameter\n","Distributions to Detect Concept Drift in Data Streams.&#x201D;</span> In\n","<em>ICPR</em>, 9452&#x2013;59.\n","</div>\n","<div id=\"ref-he2021deepobliviate\" class=\"csl-entry\" role=\"listitem\">\n","He, Yingzhe, Guozhu Meng, Kai Chen, Jinwen He, and Xingbo Hu. 2021.\n","<span>&#x201C;Deepobliviate: A Powerful Charm for Erasing Data Residual Memory\n","in Deep Neural Networks.&#x201D;</span> <em>arXiv Preprint\n","arXiv:2105.06209</em>.\n","</div>\n","<div id=\"ref-hu2021distilling\" class=\"csl-entry\" role=\"listitem\">\n","Hu, Xinting, Kaihua Tang, Chunyan Miao, Xian-Sheng Hua, and Hanwang\n","Zhang. 2021. <span>&#x201C;Distilling Causal Effect of Data in\n","Class-Incremental Learning.&#x201D;</span> In <em>CVPR</em>, 3957&#x2013;66.\n","</div>\n","<div id=\"ref-huang2021unlearnable\" class=\"csl-entry\" role=\"listitem\">\n","Huang, Hanxun, Xingjun Ma, Sarah Monazam Erfani, James Bailey, and Yisen\n","Wang. 2021. <span>&#x201C;Unlearnable Examples: Making Personal Data\n","Unexploitable.&#x201D;</span> In <em>ICLR</em>.\n","</div>\n","<div id=\"ref-huang2021mathsf\" class=\"csl-entry\" role=\"listitem\">\n","Huang, Yangsibo, Xiaoxiao Li, et al. 2021. <span>&#x201C;EMA: Auditing Data\n","Removal from Trained Models.&#x201D;</span> In <em>MICCAI</em>, 793&#x2013;803.\n","</div>\n","<div id=\"ref-hullermeier2021aleatoric\" class=\"csl-entry\"\n","role=\"listitem\">\n","H&#xFC;llermeier, Eyke, and Willem Waegeman. 2021. <span>&#x201C;Aleatoric and\n","Epistemic Uncertainty in Machine Learning: An Introduction to Concepts\n","and Methods.&#x201D;</span> <em>Machine Learning</em> 110 (3): 457&#x2013;506.\n","</div>\n","<div id=\"ref-izzo2021approximate\" class=\"csl-entry\" role=\"listitem\">\n","Izzo, Zachary, Mary Anne Smart, Kamalika Chaudhuri, and James Zou. 2021.\n","<span>&#x201C;Approximate Data Deletion from Machine Learning Models.&#x201D;</span>\n","In <em>AISTAT</em>, 2008&#x2013;16.\n","</div>\n","<div id=\"ref-jagielski2022measuring\" class=\"csl-entry\" role=\"listitem\">\n","Jagielski, Matthew, Om Thakkar, Florian Tram&#xE8;r, Daphne Ippolito,\n","Katherine Lee, Nicholas Carlini, Eric Wallace, et al. 2022.\n","<span>&#x201C;Measuring Forgetting of Memorized Training Examples.&#x201D;</span>\n","<em>arXiv Preprint arXiv:2207.00099</em>.\n","</div>\n","<div id=\"ref-jia2021proof\" class=\"csl-entry\" role=\"listitem\">\n","Jia, Hengrui, Mohammad Yaghini, Christopher A Choquette-Choo, Natalie\n","Dullerud, Anvith Thudi, Varun Chandrasekaran, and Nicolas Papernot.\n","2021. <span>&#x201C;Proof-of-Learning: Definitions and Practice.&#x201D;</span> In\n","<em>SP</em>, 1039&#x2013;56.\n","</div>\n","<div id=\"ref-jose2021unified\" class=\"csl-entry\" role=\"listitem\">\n","Jose, Sharu Theresa, and Osvaldo Simeone. 2021. <span>&#x201C;A Unified\n","PAC-Bayesian Framework for Machine Unlearning via Information Risk\n","Minimization.&#x201D;</span> In <em>MLSP</em>, 1&#x2013;6.\n","</div>\n","<div id=\"ref-karasuyama2009multiple\" class=\"csl-entry\" role=\"listitem\">\n","Karasuyama, Masayuki, and Ichiro Takeuchi. 2009. <span>&#x201C;Multiple\n","Incremental Decremental Learning of Support Vector Machines.&#x201D;</span>\n","<em>NIPS</em> 22.\n","</div>\n","<div id=\"ref-karasuyama2010multiple\" class=\"csl-entry\" role=\"listitem\">\n","&#x2014;&#x2014;&#x2014;. 2010. <span>&#x201C;Multiple Incremental Decremental Learning of Support\n","Vector Machines.&#x201D;</span> <em>IEEE Transactions on Neural Networks</em>\n","21 (7): 1048&#x2013;59.\n","</div>\n","<div id=\"ref-kearns1998efficient\" class=\"csl-entry\" role=\"listitem\">\n","Kearns, Michael. 1998. <span>&#x201C;Efficient Noise-Tolerant Learning from\n","Statistical Queries.&#x201D;</span> <em>JACM</em> 45 (6): 983&#x2013;1006.\n","</div>\n","<div id=\"ref-khan2021knowledge\" class=\"csl-entry\" role=\"listitem\">\n","Khan, Mohammad Emtiyaz E et al. 2021. <span>&#x201C;Knowledge-Adaptation\n","Priors.&#x201D;</span> <em>NIPS</em> 34: 19757&#x2013;70.\n","</div>\n","<div id=\"ref-kirkpatrick2017overcoming\" class=\"csl-entry\"\n","role=\"listitem\">\n","Kirkpatrick, James, Razvan Pascanu, Neil Rabinowitz, Joel Veness,\n","Guillaume Desjardins, Andrei A Rusu, Kieran Milan, et al. 2017.\n","<span>&#x201C;Overcoming Catastrophic Forgetting in Neural Networks.&#x201D;</span>\n","<em>PNAS</em> 114 (13): 3521&#x2013;26.\n","</div>\n","<div id=\"ref-koh2017understanding\" class=\"csl-entry\" role=\"listitem\">\n","Koh, Pang Wei et al. 2017. <span>&#x201C;Understanding Black-Box Predictions\n","via Influence Functions.&#x201D;</span> In <em>ICML</em>, 1885&#x2013;94.\n","</div>\n","<div id=\"ref-KLFormula\" class=\"csl-entry\" role=\"listitem\">\n","Kullback, S. et al. 1951. <span>&#x201C;<span class=\"nocase\">On Information and\n","Sufficiency</span>.&#x201D;</span> <em>The Annals of Mathematical\n","Statistics</em> 22 (1): 79&#x2013;86.\n","</div>\n","<div id=\"ref-lei2019geometric\" class=\"csl-entry\" role=\"listitem\">\n","Lei, Na, Kehua Su, Li Cui, Shing-Tung Yau, and Xianfeng David Gu. 2019.\n","<span>&#x201C;A Geometric View of Optimal Transportation and Generative\n","Model.&#x201D;</span> <em>Computer Aided Geometric Design</em> 68: 1&#x2013;21.\n","</div>\n","<div id=\"ref-li2020online\" class=\"csl-entry\" role=\"listitem\">\n","Li, Yuantong, Chi-Hua Wang, and Guang Cheng. 2021. <span>&#x201C;Online\n","Forgetting Process for Linear Regression Models.&#x201D;</span> In\n","<em>AISTAT</em>, 130:217&#x2013;25.\n","</div>\n","<div id=\"ref-liu2022continual\" class=\"csl-entry\" role=\"listitem\">\n","Liu, Bo, Qiang Liu, et al. 2022. <span>&#x201C;Continual Learning and Private\n","Unlearning.&#x201D;</span> <em>arXiv Preprint arXiv:2203.12817</em>.\n","</div>\n","<div id=\"ref-liu2020federated\" class=\"csl-entry\" role=\"listitem\">\n","Liu, Gaoyang, Xiaoqiang Ma, Yang Yang, Chen Wang, and Jiangchuan Liu.\n","2020. <span>&#x201C;Federated Unlearning.&#x201D;</span> <em>arXiv Preprint\n","arXiv:2012.13891</em>.\n","</div>\n","<div id=\"ref-liu2021federaser\" class=\"csl-entry\" role=\"listitem\">\n","&#x2014;&#x2014;&#x2014;. 2021. <span>&#x201C;Federaser: Enabling Efficient Client-Level Data\n","Removal from Federated Learning Models.&#x201D;</span> In <em>IWQOS</em>, 1&#x2013;10.\n","</div>\n","<div id=\"ref-liu2020have\" class=\"csl-entry\" role=\"listitem\">\n","Liu, Xiao, and Sotirios A Tsaftaris. 2020. <span>&#x201C;Have You Forgotten? A\n","Method to Assess If Machine Learning Models Have Forgotten Data.&#x201D;</span>\n","In <em>MICCAI</em>, 95&#x2013;105.\n","</div>\n","<div id=\"ref-liu2022backdoor\" class=\"csl-entry\" role=\"listitem\">\n","Liu, Yang, Mingyuan Fan, Cen Chen, Ximeng Liu, Zhuo Ma, Li Wang, and\n","Jianfeng Ma. 2022. <span>&#x201C;Backdoor Defense with Machine\n","Unlearning.&#x201D;</span> <em>arXiv Preprint arXiv:2201.09538</em>.\n","</div>\n","<div id=\"ref-liu2020learn\" class=\"csl-entry\" role=\"listitem\">\n","Liu, Yang, Zhuo Ma, Ximeng Liu, Jian Liu, Zhongyuan Jiang, Jianfeng Ma,\n","Philip Yu, and Kui Ren. 2020. <span>&#x201C;Learn to Forget: Machine Unlearning\n","via Neuron Masking.&#x201D;</span> <em>arXiv Preprint arXiv:2003.10933</em>.\n","</div>\n","<div id=\"ref-liu2021revfrf\" class=\"csl-entry\" role=\"listitem\">\n","Liu, Yang, Zhuo Ma, Yilong Yang, Ximeng Liu, Jianfeng Ma, and Kui Ren.\n","2021. <span>&#x201C;Revfrf: Enabling Cross-Domain Random Forest Training with\n","Revocable Federated Learning.&#x201D;</span> <em>TDSC</em>.\n","</div>\n","<div id=\"ref-liu2022right\" class=\"csl-entry\" role=\"listitem\">\n","Liu, Yi, Lei Xu, Xingliang Yuan, Cong Wang, and Bo Li. 2022. <span>&#x201C;The\n","Right to Be Forgotten in Federated Learning: An Efficient Realization\n","with Rapid Retraining.&#x201D;</span> In <em>INFOCOM</em>, 1749&#x2013;58.\n","</div>\n","<div id=\"ref-mahadevan2021certifiable\" class=\"csl-entry\"\n","role=\"listitem\">\n","Mahadevan, Ananth, and Michael Mathioudakis. 2021. <span>&#x201C;Certifiable\n","Machine Unlearning for Linear Models.&#x201D;</span> <em>arXiv Preprint\n","arXiv:2106.15093</em>.\n","</div>\n","<div id=\"ref-mahadevan2022certifiable\" class=\"csl-entry\"\n","role=\"listitem\">\n","&#x2014;&#x2014;&#x2014;. 2022. <span>&#x201C;Certifiable Unlearning Pipelines for Logistic\n","Regression: An Experimental Study.&#x201D;</span> <em>Machine Learning and\n","Knowledge Extraction</em> 4 (3): 591&#x2013;620.\n","</div>\n","<div id=\"ref-magdziarczyk2019right\" class=\"csl-entry\" role=\"listitem\">\n","Mantelero, Alessandro. 2013. <span>&#x201C;The EU Proposal for a General Data\n","Protection Regulation and the Roots of the <span>&#x2018;Right to Be\n","Forgotten&#x2019;</span>.&#x201D;</span> <em>Computer Law &amp; Security Review</em>\n","29 (3): 229&#x2013;35.\n","</div>\n","<div id=\"ref-marchant2022hard\" class=\"csl-entry\" role=\"listitem\">\n","Marchant, Neil G, Benjamin IP Rubinstein, and Scott Alfeld. 2022.\n","<span>&#x201C;Hard to Forget: Poisoning Attacks on Certified Machine\n","Unlearning.&#x201D;</span> In <em>AAAI</em>, 36:7691&#x2013;7700. 7.\n","</div>\n","<div id=\"ref-martens2020new\" class=\"csl-entry\" role=\"listitem\">\n","Martens, James. 2020. <span>&#x201C;New Insights and Perspectives on the\n","Natural Gradient Method.&#x201D;</span> <em>JMLR</em> 21 (1): 5776&#x2013;5851.\n","</div>\n","<div id=\"ref-masi2018deep\" class=\"csl-entry\" role=\"listitem\">\n","Masi, Iacopo, Yue Wu, Tal Hassner, and Prem Natarajan. 2018. <span>&#x201C;Deep\n","Face Recognition: A Survey.&#x201D;</span> In <em>SIBGRAPI</em>, 471&#x2013;78.\n","</div>\n","<div id=\"ref-mcmahan2017communication\" class=\"csl-entry\"\n","role=\"listitem\">\n","McMahan, Brendan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise\n","Aguera y Arcas. 2017. <span>&#x201C;Communication-Efficient Learning of Deep\n","Networks from Decentralized Data.&#x201D;</span> In <em>AISTAT</em>, 1273&#x2013;82.\n","</div>\n","<div id=\"ref-mehrabi2021survey\" class=\"csl-entry\" role=\"listitem\">\n","Mehrabi, Ninareh, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and\n","Aram Galstyan. 2021. <span>&#x201C;A Survey on Bias and Fairness in Machine\n","Learning.&#x201D;</span> <em>CSUR</em> 54 (6): 1&#x2013;35.\n","</div>\n","<div id=\"ref-mehta2022deep\" class=\"csl-entry\" role=\"listitem\">\n","Mehta, Ronak, Sourav Pal, Vikas Singh, and Sathya N Ravi. 2022.\n","<span>&#x201C;Deep Unlearning via Randomized Conditionally Independent\n","Hessians.&#x201D;</span> In <em>CVPR</em>, 10422&#x2013;31.\n","</div>\n","<div id=\"ref-micaelli2019zero\" class=\"csl-entry\" role=\"listitem\">\n","Micaelli, Paul et al. 2019. <span>&#x201C;Zero-Shot Knowledge Transfer via\n","Adversarial Belief Matching.&#x201D;</span> <em>NIPS</em> 32.\n","</div>\n","<div id=\"ref-nam2020learning\" class=\"csl-entry\" role=\"listitem\">\n","Nam, Junhyun, Hyuntak Cha, Sungsoo Ahn, Jaeho Lee, and Jinwoo Shin.\n","2020. <span>&#x201C;Learning from Failure: De-Biasing Classifier from Biased\n","Classifier.&#x201D;</span> <em>NIPS</em> 33: 20673&#x2013;84.\n","</div>\n","<div id=\"ref-neel2021descent\" class=\"csl-entry\" role=\"listitem\">\n","Neel, Seth, Aaron Roth, and Saeed Sharifi-Malvajerdi. 2021.\n","<span>&#x201C;Descent-to-Delete: Gradient-Based Methods for Machine\n","Unlearning.&#x201D;</span> In <em>Algorithmic Learning Theory</em>, 931&#x2013;62.\n","</div>\n","<div id=\"ref-nguyen2020variational\" class=\"csl-entry\" role=\"listitem\">\n","Nguyen, Quoc Phong, Bryan Kian Hsiang Low, and Patrick Jaillet. 2020.\n","<span>&#x201C;Variational Bayesian Unlearning.&#x201D;</span> <em>NIPS</em> 33:\n","16025&#x2013;36.\n","</div>\n","<div id=\"ref-nguyen2022markov\" class=\"csl-entry\" role=\"listitem\">\n","Nguyen, Quoc Phong, Ryutaro Oikawa, Dinil Mon Divakaran, Mun Choon Chan,\n","et al. 2022. <span>&#x201C;Markov Chain Monte Carlo-Based Machine Unlearning:\n","Unlearning What Needs to Be Forgotten.&#x201D;</span> In <em>ASIACCS</em>,\n","351&#x2013;63.\n","</div>\n","<div id=\"ref-nguyen2019debunking\" class=\"csl-entry\" role=\"listitem\">\n","Nguyen, Thanh Tam. 2019. <span>&#x201C;Debunking Misinformation on the Web:\n","Detection, Validation, and Visualisation.&#x201D;</span> PhD thesis, EPFL,\n","Switzerland.\n","</div>\n","<div id=\"ref-nguyen2022survey\" class=\"csl-entry\" role=\"listitem\">\n","Nguyen, Thanh Tam, Thanh Trung Huynh, Phi Le Nguyen, Alan Wee-Chung\n","Liew, Hongzhi Yin, and Quoc Viet Hung Nguyen. 2022. <span>&#x201C;A Survey of\n","Machine Unlearning.&#x201D;</span> <em>arXiv Preprint arXiv:2209.02299</em>.\n","</div>\n","<div id=\"ref-nguyen2021judo\" class=\"csl-entry\" role=\"listitem\">\n","Nguyen, Thanh Toan, Thanh Tam Nguyen, Thanh Thi Nguyen, Bay Vo, Jun Jo,\n","and Quoc Viet Hung Nguyen. 2021. <span>&#x201C;Judo: Just-in-Time Rumour\n","Detection in Streaming Social Platforms.&#x201D;</span> <em>Information\n","Sciences</em> 570: 70&#x2013;93.\n","</div>\n","<div id=\"ref-pardau2018california\" class=\"csl-entry\" role=\"listitem\">\n","Pardau, Stuart L. 2018. <span>&#x201C;The California Consumer Privacy Act:\n","Towards a European-Style Privacy Regime in the United States.&#x201D;</span>\n","<em>J. Tech. L. &amp; Pol&#x2019;y</em> 23: 68.\n","</div>\n","<div id=\"ref-parisi2019continual\" class=\"csl-entry\" role=\"listitem\">\n","Parisi, German I, Ronald Kemker, Jose L Part, Christopher Kanan, and\n","Stefan Wermter. 2019. <span>&#x201C;Continual Lifelong Learning with Neural\n","Networks: A Review.&#x201D;</span> <em>Neural Networks</em> 113: 54&#x2013;71.\n","</div>\n","<div id=\"ref-parne2021machine\" class=\"csl-entry\" role=\"listitem\">\n","Parne, Nishchal, Kyathi Puppaala, Nithish Bhupathi, and Ripon Patgiri.\n","2021. <span>&#x201C;An Investigation on Learning, Polluting, and Unlearning the\n","Spam Emails for Lifelong Learning.&#x201D;</span> <em>arXiv Preprint\n","arXiv:2111.14609</em>.\n","</div>\n","<div id=\"ref-pearce2020uncertainty\" class=\"csl-entry\" role=\"listitem\">\n","Pearce, Tim, Felix Leibfried, and Alexandra Brintrup. 2020.\n","<span>&#x201C;Uncertainty in Neural Networks: Approximately Bayesian\n","Ensembling.&#x201D;</span> In <em>AISTATS</em>, 234&#x2013;44.\n","</div>\n","<div id=\"ref-peste2021ssse\" class=\"csl-entry\" role=\"listitem\">\n","Peste, Alexandra, Dan Alistarh, and Christoph H Lampert. 2021.\n","<span>&#x201C;<span>SSSE</span>: Efficiently Erasing Samples from Trained\n","Machine Learning Models.&#x201D;</span> In <em>NeurIPS 2021 Workshop Privacy in\n","Machine Learning</em>.\n","</div>\n","<div id=\"ref-ramaswamy2021fair\" class=\"csl-entry\" role=\"listitem\">\n","Ramaswamy, Vikram V, Sunnie SY Kim, and Olga Russakovsky. 2021.\n","<span>&#x201C;Fair Attribute Classification Through Latent Space\n","de-Biasing.&#x201D;</span> In <em>CVPR</em>, 9301&#x2013;10.\n","</div>\n","<div id=\"ref-ren2020adversarial\" class=\"csl-entry\" role=\"listitem\">\n","Ren, Kui, Tianhang Zheng, Zhan Qin, and Xue Liu. 2020.\n","<span>&#x201C;Adversarial Attacks and Defenses in Deep Learning.&#x201D;</span>\n","<em>Engineering</em> 6 (3): 346&#x2013;60.\n","</div>\n","<div id=\"ref-ren2020generating\" class=\"csl-entry\" role=\"listitem\">\n","Ren, Zhao, Alice Baird, Jing Han, Zixing Zhang, and Bj&#xF6;rn Schuller.\n","2020. <span>&#x201C;Generating and Protecting Against Adversarial Attacks for\n","Deep Speech-Based Emotion Recognition Models.&#x201D;</span> In\n","<em>ICASSP</em>, 7184&#x2013;88.\n","</div>\n","<div id=\"ref-ren2020enhancing\" class=\"csl-entry\" role=\"listitem\">\n","Ren, Zhao, Jing Han, Nicholas Cummins, and Bj&#xF6;rn W Schuller. 2020.\n","<span>&#x201C;Enhancing Transferability of Black-Box Adversarial Attacks via\n","Lifelong Learning for Speech Emotion Recognition Models.&#x201D;</span> In\n","<em>INTERSPEECH</em>, 496&#x2013;500.\n","</div>\n","<div id=\"ref-ren2022prototype\" class=\"csl-entry\" role=\"listitem\">\n","Ren, Zhao, Thanh Tam Nguyen, and Wolfgang Nejdl. 2022. <span>&#x201C;Prototype\n","Learning for Interpretable Respiratory Sound Analysis.&#x201D;</span> In\n","<em>ICASSP</em>, 9087&#x2013;91.\n","</div>\n","<div id=\"ref-romero2007incremental\" class=\"csl-entry\" role=\"listitem\">\n","Romero, Enrique, Ignacio Barrio, and Llu&#x131;&#x301;s Belanche. 2007.\n","<span>&#x201C;Incremental and Decremental Learning for Linear Support Vector\n","Machines.&#x201D;</span> In <em>ICANN</em>, 209&#x2013;18.\n","</div>\n","<div id=\"ref-roth2018bayesian\" class=\"csl-entry\" role=\"listitem\">\n","Roth, Wolfgang, and Franz Pernkopf. 2018. <span>&#x201C;Bayesian Neural\n","Networks with Weight Sharing Using Dirichlet Processes.&#x201D;</span>\n","<em>TPAMI</em> 42 (1): 246&#x2013;52.\n","</div>\n","<div id=\"ref-salem2020updates\" class=\"csl-entry\" role=\"listitem\">\n","Salem, Ahmed, Apratim Bhattacharya, Michael Backes, Mario Fritz, and\n","Yang Zhang. 2020. <span>&#x201C;<span\n","class=\"math inline\">{</span>Updates-Leak<span\n","class=\"math inline\">}</span>: Data Set Inference and Reconstruction\n","Attacks in Online Learning.&#x201D;</span> In <em>USENIX Security</em>,\n","1291&#x2013;1308.\n","</div>\n","<div id=\"ref-Salem0HBF019\" class=\"csl-entry\" role=\"listitem\">\n","Salem, Ahmed, Yang Zhang, Mathias Humbert, Pascal Berrang, Mario Fritz,\n","and Michael Backes. 2019. <span>&#x201C;ML-Leaks: Model and Data Independent\n","Membership Inference Attacks and Defenses on Machine Learning\n","Models.&#x201D;</span> In <em>NDSS</em>.\n","</div>\n","<div id=\"ref-sari2020learning\" class=\"csl-entry\" role=\"listitem\">\n","Sari, WN, BS Samosir, N Sahara, L Agustina, and Y Anita. 2020.\n","<span>&#x201C;Learning Mathematics <span>&#x2018;Asyik&#x2019;</span> with Youtube Educative\n","Media.&#x201D;</span> In <em>Journal of Physics: Conference Series</em>,\n","1477:022012. 2.\n","</div>\n","<div id=\"ref-sattler2021fedaux\" class=\"csl-entry\" role=\"listitem\">\n","Sattler, Felix, Tim Korjakow, Roman Rischke, and Wojciech Samek. 2021.\n","<span>&#x201C;Fedaux: Leveraging Unlabeled Auxiliary Data in Federated\n","Learning.&#x201D;</span> <em>TNNLS</em>.\n","</div>\n","<div id=\"ref-schelter2020amnesia\" class=\"csl-entry\" role=\"listitem\">\n","Schelter, Sebastian. 2020. <span>&#x201C;<span>&#x2018;Amnesia&#x2019;</span> - a Selection\n","of Machine Learning Models That Can Forget User Data Very Fast.&#x201D;</span>\n","In <em>CIDR</em>.\n","</div>\n","<div id=\"ref-schelter2021hedgecut\" class=\"csl-entry\" role=\"listitem\">\n","Schelter, Sebastian, Stefan Grafberger, and Ted Dunning. 2021.\n","<span>&#x201C;Hedgecut: Maintaining Randomised Trees for Low-Latency Machine\n","Unlearning.&#x201D;</span> In <em>SIGMOD</em>, 1545&#x2013;57.\n","</div>\n","<div id=\"ref-sekhari2021remember\" class=\"csl-entry\" role=\"listitem\">\n","Sekhari, Ayush, Jayadev Acharya, Gautam Kamath, and Ananda Theertha\n","Suresh. 2021. <span>&#x201C;Remember What You Want to Forget: Algorithms for\n","Machine Unlearning.&#x201D;</span> <em>NIPS</em> 34: 18075&#x2013;86.\n","</div>\n","<div id=\"ref-shan2020protecting\" class=\"csl-entry\" role=\"listitem\">\n","Shan, S, E Wenger, J Zhang, H Li, H Zheng, and BY Zhao. 2020.\n","<span>&#x201C;Protecting Personal Privacy Against Una Uthorized Deep Learning\n","Models.&#x201D;</span> In <em>USENIX Security</em>, 1&#x2013;16.\n","</div>\n","<div id=\"ref-shibata2021learning\" class=\"csl-entry\" role=\"listitem\">\n","Shibata, Takashi, Go Irie, Daiki Ikami, and Yu Mitsuzumi. 2021.\n","<span>&#x201C;Learning with Selective Forgetting.&#x201D;</span> In <em>IJCAI</em>,\n","2:6. 4.\n","</div>\n","<div id=\"ref-shintre2019making\" class=\"csl-entry\" role=\"listitem\">\n","Shintre, Saurabh et al. 2019. <span>&#x201C;Making Machine Learning\n","Forget.&#x201D;</span> In <em>Annual Privacy Forum</em>, 72&#x2013;83.\n","</div>\n","<div id=\"ref-shokri2017membership\" class=\"csl-entry\" role=\"listitem\">\n","Shokri, Reza, Marco Stronati, Congzheng Song, and Vitaly Shmatikov.\n","2017. <span>&#x201C;Membership Inference Attacks Against Machine Learning\n","Models.&#x201D;</span> In <em>SP</em>, 3&#x2013;18.\n","</div>\n","<div id=\"ref-shwartz2017opening\" class=\"csl-entry\" role=\"listitem\">\n","Shwartz-Ziv, Ravid, and Naftali Tishby. 2017. <span>&#x201C;Opening the Black\n","Box of Deep Neural Networks via Information.&#x201D;</span> <em>arXiv Preprint\n","arXiv:1703.00810</em>.\n","</div>\n","<div id=\"ref-singh2017data\" class=\"csl-entry\" role=\"listitem\">\n","Singh, Abhijeet, and Abhineet Anand. 2017. <span>&#x201C;Data Leakage Detection\n","Using Cloud Computing.&#x201D;</span> <em>IJECS</em> 6 (4).\n","</div>\n","<div id=\"ref-singh2022anatomizing\" class=\"csl-entry\" role=\"listitem\">\n","Singh, Richa, Puspita Majumdar, Surbhi Mittal, and Mayank Vatsa. 2022.\n","<span>&#x201C;Anatomizing Bias in Facial Analysis.&#x201D;</span> In <em>AAAI</em>,\n","36:12351&#x2013;58. 11.\n","</div>\n","<div id=\"ref-sommer2020towards\" class=\"csl-entry\" role=\"listitem\">\n","Sommer, David Marco, Liwei Song, Sameer Wagh, and Prateek Mittal. 2020.\n","<span>&#x201C;Towards Probabilistic Verification of Machine Unlearning.&#x201D;</span>\n","<em>arXiv Preprint arXiv:2003.04247</em>.\n","</div>\n","<div id=\"ref-sommer2022athena\" class=\"csl-entry\" role=\"listitem\">\n","&#x2014;&#x2014;&#x2014;. 2022. <span>&#x201C;Athena: Probabilistic Verification of Machine\n","Unlearning.&#x201D;</span> <em>Proc. Priv. Enhancing Technol.</em> 2022 (3):\n","268&#x2013;90.\n","</div>\n","<div id=\"ref-tahiliani2021machine\" class=\"csl-entry\" role=\"listitem\">\n","Tahiliani, Aman, Vikas Hassija, Vinay Chamola, and Mohsen Guizani. 2021.\n","<span>&#x201C;Machine Unlearning: Its Need and Implementation\n","Strategies.&#x201D;</span> In <em>IC3</em>, 241&#x2013;46.\n","</div>\n","<div id=\"ref-nguyen2017retaining\" class=\"csl-entry\" role=\"listitem\">\n","Tam, Nguyen Thanh, Matthias Weidlich, Duong Chi Thang, Hongzhi Yin, and\n","Nguyen Quoc Viet Hung. 2017. <span>&#x201C;Retaining Data from Streams of\n","Social Platforms with Minimal Regret.&#x201D;</span> In <em>IJCAI</em>,\n","2850&#x2013;56.\n","</div>\n","<div id=\"ref-tanha2020boosting\" class=\"csl-entry\" role=\"listitem\">\n","Tanha, Jafar, Yousef Abdi, Negin Samadi, Nazila Razzaghi, and Mohammad\n","Asadpour. 2020. <span>&#x201C;Boosting Methods for Multi-Class Imbalanced Data\n","Classification: An Experimental Review.&#x201D;</span> <em>Journal of Big\n","Data</em> 7 (1): 1&#x2013;47.\n","</div>\n","<div id=\"ref-tarun2021fast\" class=\"csl-entry\" role=\"listitem\">\n","Tarun, Ayush K, Vikram S Chundawat, Murari Mandal, and Mohan\n","Kankanhalli. 2021. <span>&#x201C;Fast yet Effective Machine Unlearning.&#x201D;</span>\n","<em>arXiv Preprint arXiv:2111.08947</em>.\n","</div>\n","<div id=\"ref-thudi2022unrolling\" class=\"csl-entry\" role=\"listitem\">\n","Thudi, Anvith, Gabriel Deza, Varun Chandrasekaran, and Nicolas Papernot.\n","2022. <span>&#x201C;Unrolling Sgd: Understanding Factors Influencing Machine\n","Unlearning.&#x201D;</span> In <em>EuroS&amp;p</em>, 303&#x2013;19.\n","</div>\n","<div id=\"ref-thudi2021necessity\" class=\"csl-entry\" role=\"listitem\">\n","Thudi, Anvith, Hengrui Jia, Ilia Shumailov, and Nicolas Papernot. 2022.\n","<span>&#x201C;On the Necessity of Auditable Algorithmic Definitions for Machine\n","Unlearning.&#x201D;</span> In <em>USENIX Security</em>, 4007&#x2013;22.\n","</div>\n","<div id=\"ref-thudi2022bounding\" class=\"csl-entry\" role=\"listitem\">\n","Thudi, Anvith, Ilia Shumailov, Franziska Boenisch, and Nicolas Papernot.\n","2022. <span>&#x201C;Bounding Membership Inference.&#x201D;</span> <em>arXiv Preprint\n","arXiv:2202.12232</em>.\n","</div>\n","<div id=\"ref-tishby2000information\" class=\"csl-entry\" role=\"listitem\">\n","Tishby, Naftali et al. 2000. <span>&#x201C;The Information Bottleneck\n","Method.&#x201D;</span> <em>arXiv Preprint Physics/0004057</em>.\n","</div>\n","<div id=\"ref-tishby2015deep\" class=\"csl-entry\" role=\"listitem\">\n","Tishby, Naftali, and Noga Zaslavsky. 2015. <span>&#x201C;Deep Learning and the\n","Information Bottleneck Principle.&#x201D;</span> In <em>ITW</em>, 1&#x2013;5.\n","</div>\n","<div id=\"ref-tsai2014incremental\" class=\"csl-entry\" role=\"listitem\">\n","Tsai, Cheng-Hao, Chieh-Yen Lin, and Chih-Jen Lin. 2014.\n","<span>&#x201C;Incremental and Decremental Training for Linear\n","Classification.&#x201D;</span> In <em>KDD</em>, 343&#x2013;52.\n","</div>\n","<div id=\"ref-tveit2003multicategory\" class=\"csl-entry\" role=\"listitem\">\n","Tveit, Amund et al. 2003. <span>&#x201C;Multicategory Incremental Proximal\n","Support Vector Classifiers.&#x201D;</span> In <em>KES</em>, 386&#x2013;92.\n","</div>\n","<div id=\"ref-tveit2003incremental\" class=\"csl-entry\" role=\"listitem\">\n","Tveit, Amund, Magnus Lie Hetland, and H&#xE5;avard Engum. 2003.\n","<span>&#x201C;Incremental and Decremental Proximal Support Vector\n","Classification Using Decay Coefficients.&#x201D;</span> In <em>DaWaK</em>,\n","422&#x2013;29.\n","</div>\n","<div id=\"ref-ullah2021machine\" class=\"csl-entry\" role=\"listitem\">\n","Ullah, Enayat, Tung Mai, Anup Rao, Ryan A Rossi, and Raman Arora. 2021.\n","<span>&#x201C;Machine Unlearning via Algorithmic Stability.&#x201D;</span> In\n","<em>Conference on Learning Theory</em>, 4126&#x2013;42.\n","</div>\n","<div id=\"ref-veale2018algorithms\" class=\"csl-entry\" role=\"listitem\">\n","Veale, Michael, Reuben Binns, and Lilian Edwards. 2018.\n","<span>&#x201C;Algorithms That Remember: Model Inversion Attacks and Data\n","Protection Law.&#x201D;</span> <em>Philos. Trans. R. Soc. A</em> 376 (2133):\n","20180083.\n","</div>\n","<div id=\"ref-verdu2014total\" class=\"csl-entry\" role=\"listitem\">\n","Verd&#xFA;, Sergio. 2014. <span>&#x201C;Total Variation Distance and the\n","Distribution of Relative Information.&#x201D;</span> In <em>ITA</em>, 1&#x2013;3.\n","</div>\n","<div id=\"ref-villaronga2018humans\" class=\"csl-entry\" role=\"listitem\">\n","Villaronga, Eduard Fosch, Peter Kieseberg, and Tiffany Li. 2018.\n","<span>&#x201C;Humans Forget, Machines Remember: Artificial Intelligence and the\n","Right to Be Forgotten.&#x201D;</span> <em>Computer Law &amp; Security\n","Review</em> 34 (2): 304&#x2013;13.\n","</div>\n","<div id=\"ref-voigt2017eu\" class=\"csl-entry\" role=\"listitem\">\n","Voigt, Paul, and Axel Von dem Bussche. 2017. <span>&#x201C;The Eu General Data\n","Protection Regulation (Gdpr).&#x201D;</span> <em>A Practical Guide, 1st Ed.,\n","Cham: Springer International Publishing</em> 10 (3152676): 10&#x2013;5555.\n","</div>\n","<div id=\"ref-wang2022efficiently\" class=\"csl-entry\" role=\"listitem\">\n","Wang, Benjamin Longxiang, and Sebastian Schelter. 2022.\n","<span>&#x201C;Efficiently Maintaining Next Basket Recommendations Under\n","Additions and Deletions of Baskets and Items.&#x201D;</span> <em>arXiv Preprint\n","arXiv:2201.13313</em>.\n","</div>\n","<div id=\"ref-wang2019neural\" class=\"csl-entry\" role=\"listitem\">\n","Wang, Bolun, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath,\n","Haitao Zheng, and Ben Y Zhao. 2019. <span>&#x201C;Neural Cleanse: Identifying\n","and Mitigating Backdoor Attacks in Neural Networks.&#x201D;</span> In\n","<em>SP</em>, 707&#x2013;23.\n","</div>\n","<div id=\"ref-wang2022federated\" class=\"csl-entry\" role=\"listitem\">\n","Wang, Junxiao, Song Guo, et al. 2022. <span>&#x201C;Federated Unlearning via\n","Class-Discriminative Pruning.&#x201D;</span> In <em>WWW</em>, 622&#x2013;32.\n","</div>\n","<div id=\"ref-wang2009learning\" class=\"csl-entry\" role=\"listitem\">\n","Wang, Rui, Yong Fuga Li, XiaoFeng Wang, Haixu Tang, and Xiaoyong Zhou.\n","2009. <span>&#x201C;Learning Your Identity and Disease from Research Papers:\n","Information Leaks in Genome Wide Association Study.&#x201D;</span> In\n","<em>CCS</em>, 534&#x2013;44.\n","</div>\n","<div id=\"ref-wang2020towards\" class=\"csl-entry\" role=\"listitem\">\n","Wang, Zeyu, Klint Qinami, Ioannis Christos Karakozis, Kyle Genova, Prem\n","Nair, Kenji Hata, and Olga Russakovsky. 2020. <span>&#x201C;Towards Fairness in\n","Visual Recognition: Effective Strategies for Bias Mitigation.&#x201D;</span> In\n","<em>CVPR</em>, 8919&#x2013;28.\n","</div>\n","<div id=\"ref-warnecke2021machine\" class=\"csl-entry\" role=\"listitem\">\n","Warnecke, Alexander, Lukas Pirch, Christian Wressnegger, and Konrad\n","Rieck. 2021. <span>&#x201C;Machine Unlearning of Features and Labels.&#x201D;</span>\n","<em>arXiv Preprint arXiv:2108.11577</em>.\n","</div>\n","<div id=\"ref-wu2022federated\" class=\"csl-entry\" role=\"listitem\">\n","Wu, Chen et al. 2022. <span>&#x201C;Federated Unlearning with Knowledge\n","Distillation.&#x201D;</span> <em>arXiv Preprint arXiv:2201.09441</em>.\n","</div>\n","<div id=\"ref-wu2019simplifying\" class=\"csl-entry\" role=\"listitem\">\n","Wu, Felix, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and\n","Kilian Weinberger. 2019. <span>&#x201C;Simplifying Graph Convolutional\n","Networks.&#x201D;</span> In <em>ICML</em>, 6861&#x2013;71.\n","</div>\n","<div id=\"ref-wu2022puma\" class=\"csl-entry\" role=\"listitem\">\n","Wu, Ga, Masoud Hashemi, and Christopher Srinivasa. 2022. <span>&#x201C;PUMA:\n","Performance Unchanged Model Augmentation for Training Data\n","Removal.&#x201D;</span> In <em>AAAI</em>.\n","</div>\n","<div id=\"ref-wu2020deltagrad\" class=\"csl-entry\" role=\"listitem\">\n","Wu, Yinjun et al. 2020. <span>&#x201C;Deltagrad: Rapid Retraining of Machine\n","Learning Models.&#x201D;</span> In <em>ICML</em>, 10355&#x2013;66.\n","</div>\n","<div id=\"ref-wu2020priu\" class=\"csl-entry\" role=\"listitem\">\n","Wu, Yinjun, Val Tannen, and Susan B Davidson. 2020. <span>&#x201C;Priu: A\n","Provenance-Based Approach for Incrementally Updating Regression\n","Models.&#x201D;</span> In <em>SIGMOD</em>, 447&#x2013;62.\n","</div>\n","<div id=\"ref-yoon2022few\" class=\"csl-entry\" role=\"listitem\">\n","Yoon, Youngsik, Jinhwan Nam, Hyojeong Yun, Dongwoo Kim, and Jungseul Ok.\n","2022. <span>&#x201C;Few-Shot Unlearning by Model Inversion.&#x201D;</span> <em>arXiv\n","Preprint arXiv:2205.15567</em>.\n","</div>\n","<div id=\"ref-yu2021does\" class=\"csl-entry\" role=\"listitem\">\n","Yu, Da, Huishuai Zhang, Wei Chen, Jian Yin, and Tie-Yan Liu. 2021.\n","<span>&#x201C;How Does Data Augmentation Affect Privacy in Machine\n","Learning?&#x201D;</span> In <em>AAAI</em>, 35:10746&#x2013;53. 12.\n","</div>\n","<div id=\"ref-yu2015lsun\" class=\"csl-entry\" role=\"listitem\">\n","Yu, Fisher, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and\n","Jianxiong Xiao. 2015. <span>&#x201C;Lsun: Construction of a Large-Scale Image\n","Dataset Using Deep Learning with Humans in the Loop.&#x201D;</span> <em>arXiv\n","Preprint arXiv:1506.03365</em>.\n","</div>\n","<div id=\"ref-zanella2020analyzing\" class=\"csl-entry\" role=\"listitem\">\n","Zanella-B&#xE9;guelin, Santiago, Lukas Wutschitz, Shruti Tople, Victor R&#xFC;hle,\n","Andrew Paverd, Olga Ohrimenko, et al. 2020. <span>&#x201C;Analyzing Information\n","Leakage of Updates to Natural Language Models.&#x201D;</span> In\n","<em>SIGSAC</em>, 363&#x2013;75.\n","</div>\n","<div id=\"ref-zeng2021learning\" class=\"csl-entry\" role=\"listitem\">\n","Zeng, Yingyan, Tianhao Wang, Si Chen, Hoang Anh Just, Ran Jin, and Ruoxi\n","Jia. 2021. <span>&#x201C;Learning to Refit for Convex Learning\n","Problems.&#x201D;</span> <em>arXiv Preprint arXiv:2111.12545</em>.\n","</div>\n","<div id=\"ref-zhang2020deep\" class=\"csl-entry\" role=\"listitem\">\n","Zhang, Hao, Bo Chen, Yulai Cong, Dandan Guo, Hongwei Liu, and Mingyuan\n","Zhou. 2020. <span>&#x201C;Deep Autoencoding Topic Model with Scalable Hybrid\n","Bayesian Inference.&#x201D;</span> <em>TPAMI</em> 43 (12): 4306&#x2013;22.\n","</div>\n","<div id=\"ref-zhang2022machine\" class=\"csl-entry\" role=\"listitem\">\n","Zhang, Peng-Fei, Guangdong Bai, Zi Huang, and Xin-Shun Xu. 2022.\n","<span>&#x201C;Machine Unlearning for Image Retrieval: A Generative Scrubbing\n","Approach.&#x201D;</span> In <em>MM</em>, 237&#x2013;45.\n","</div>\n","<div id=\"ref-zou2018ai\" class=\"csl-entry\" role=\"listitem\">\n","Zou, James, and Londa Schiebinger. 2018. <span>&#x201C;AI Can Be Sexist and\n","Racist &#x2013; It&#x2019;s Time to Make It Fair.&#x201D;</span> Nature Publishing Group.\n","</div>\n","</div>\n","<aside id=\"footnotes\" class=\"footnotes footnotes-end-of-document\"\n","role=\"doc-endnotes\">\n","<hr />\n","<ol>\n","<li id=\"fn1\"><p><a\n","href=\"https://github.com/tamlhp/awesome-machine-unlearning\"\n","class=\"uri\">https://github.com/tamlhp/awesome-machine-unlearning</a><a\n","href=\"#fnref1\" class=\"footnote-back\" role=\"doc-backlink\">&#x21A9;&#xFE0E;</a></p></li>\n","<li id=\"fn2\"><p><a\n","href=\"https://insights.daffodilsw.com/blog/machine-unlearning-what-it-is-all-about\"\n","class=\"uri\">https://insights.daffodilsw.com/blog/machine-unlearning-what-it-is-all-about</a><a\n","href=\"#fnref2\" class=\"footnote-back\" role=\"doc-backlink\">&#x21A9;&#xFE0E;</a></p></li>\n","<li id=\"fn3\"><p><a\n","href=\"https://github.com/ZIYU-DEEP/Awesome-Information-Bottleneck\"\n","class=\"uri\">https://github.com/ZIYU-DEEP/Awesome-Information-Bottleneck</a><a\n","href=\"#fnref3\" class=\"footnote-back\" role=\"doc-backlink\">&#x21A9;&#xFE0E;</a></p></li>\n","</ol>\n","</aside>"]},{"cell_type":"markdown","id":"deb8c05d","metadata":{"papermill":{"duration":0.006677,"end_time":"2023-07-06T00:16:39.659672","exception":false,"start_time":"2023-07-06T00:16:39.652995","status":"completed"},"tags":[]},"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"},"papermill":{"default_parameters":{},"duration":11.278694,"end_time":"2023-07-06T00:16:40.491388","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-07-06T00:16:29.212694","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}